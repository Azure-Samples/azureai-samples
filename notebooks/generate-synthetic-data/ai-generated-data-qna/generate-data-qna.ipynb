{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Questions and Answers from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Use the QADataGenerator to generate high-quality questions and answers from your data using LLMs.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Access to Azure OpenAI Service - you can apply for access [here](https://go.microsoft.com/fwlink/?linkid=2222006)\n",
    "- An Azure AI Studio project - go to [aka.ms/azureaistudio](https://aka.ms/azureaistudio) to create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "Large Language Models (LLMs) can help you create question and answer datasets from your existing data sources. These datasets can be useful for various tasks, such as testing your retrieval capabilities, evaluating and improving your RAG workflows, tuning your prompts and more. In this sample, we will explore how to use the QADataGenerator to generate high-quality questions and answers from your data using LLMs.\n",
    "\n",
    "This sample will be useful to developers and for data scientists who need data for developing RAG workflows or evaluating and improving RAG workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this sample we will use data from 2 sources. First, we will generate text data from Wikipedia. We will also use data from files to generate QnA. For this we will use files from the `data/data_generator_texts` folder in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-identity azure-ai-generative\n",
    "%pip install wikipedia langchain nltk unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Lets initialize some variables. For `subscription_id`, `resource_group_name` and `project_name`, you can go to the Project Overview page in the AI Studio. Replace the items in <> with values for your project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project details\n",
    "subscription_id: str = \"<your-subscription-id>\"\n",
    "resource_group_name: str = \"<your-resource-group>\"\n",
    "project_name: str = \"<your-project-name>\"\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "p = Path(\"config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us connect to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.resources.client import AIClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# connects to project defined in the first config.json found in this or parent folders\n",
    "client = AIClient.from_config(DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Azure OpenAI details\n",
    "We will use an Azure Open AI service to access the LLM. Let us get the details of these from your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default Azure Open AI connection for your project\n",
    "default_aoai_connection = client.get_default_aoai_connection()\n",
    "default_aoai_connection.set_current_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA\n",
    "Initialize a QA data generator by passing in your Azure OpenAI details for your gpt-4 or gpt-35-turbo deployment.\n",
    "We'll use it to generate different types of QA for sample text.\n",
    "\n",
    "Supported QA types:\n",
    "\n",
    "|Type|Description|\n",
    "|--|--|\n",
    "|SHORT_ANSWER|Short answer QAs have answers that are only a few words long. These words are generally relevant details from text like dates, names, statistics, etc.|\n",
    "|LONG_ANSWER|Long answer QAs have answers that are one or more sentences long. ex. Questions where answer is a definition: What is a {topic_from_text}?|\n",
    "|BOOLEAN|Boolean QAs have answers that are either True or False.|\n",
    "|SUMMARY|Summary QAs have questions that ask to write a summary for text's title in a limited number of words. It generates just one QA.|\n",
    "|CONVERSATION|Conversation QAs have questions that might reference words or ideas from previous QAs. ex. If previous conversation was about some topicX from text, next question might reference it without using its name: How does **it** compare to topicY?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.synthetic.qa import QADataGenerator, QAType\n",
    "\n",
    "# For granular logs you may set DEBUG log level:\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "model_config = {\n",
    "    \"deployment\": \"gpt-35-turbo\",\n",
    "    \"model\": \"gpt-35-turbo\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA from raw text\n",
    "In this example we use a wikipedia article as raw text generate different types of Question and Answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wiki_title = wikipedia.search(\"Leonardo da vinci\")[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out with different QATypes like LONG_ANSWER or CONVERSATION\n",
    "qa_type = QAType.SHORT_ANSWER\n",
    "\n",
    "result = qa_generator.generate(\n",
    "    text=text,\n",
    "    qa_type=qa_type,\n",
    "    num_questions=5,\n",
    ")\n",
    "for question, answer in result[\"question_answers\"]:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA from files\n",
    "To generate QA from files, we need to consider two aspects: the file type and the text length. Different file types may require different loaders to extract the raw text. Also, the text length may exceed the model's context limit, which can affect the QA generation performance. Therefore, we use Langchain's Unstructured File loader and NLTKText Splitter to handle these issues. The Unstructured File loader can read various file types and convert them to raw text. The NLTKText Splitter can divide the text into smaller chunks that fit the model's context. It also avoids splitting the text in the middle of a sentence, as this can result in incorrect QAs. We should always ensure that the text chunks are complete sentences.\n",
    "\n",
    "We'll read sample files from `data/data_generator_texts` folder to generate QAs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_glob = Path(\"data\", \"data_generator_texts\")\n",
    "files = Path.glob(texts_glob, pattern=\"**/*\")\n",
    "files = [file for file in files if Path.is_file(file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us chunk and split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "# download pre-trained Punkt tokenizer for sentence splitting\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text_splitter = NLTKTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # encoding for gpt-4 and gpt-35-turbo\n",
    "    chunk_size=300,  # number of tokens to split on\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = []\n",
    "for file in files:\n",
    "    loader = UnstructuredFileLoader(file)\n",
    "    docs = loader.load()\n",
    "    data = docs[0].page_content\n",
    "    texts += text_splitter.split_text(data)\n",
    "print(f\"Number of texts after splitting: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate QA asynchronously\n",
    "To improve the performance of our file processing, we can leverage the generate_async method from `QADataGenerator`. This method allows us to send multiple chunks of text from the file to the API in parallel, and then retrieve the results asynchronously. This way, we can avoid waiting for each chunk to be processed sequentially, and reduce the overall latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "concurrency = 3  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "qa_type = QAType.CONVERSATION\n",
    "\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )\n",
    "\n",
    "\n",
    "results = await asyncio.gather(*[generate_async(text) for text in texts], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use\n",
    "Let us save the generated QnA in a format which can be understood by prompt flow (for evaluation, batch runs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"generated_qa.jsonl\"\n",
    "qa_generator.export_to_file(output_file, qa_type, question_answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use in promptflow\n",
    "\n",
    "To use the above data in promptflow, please refer to the documentation [here](https://learn.microsoft.com/azure/ai-studio/how-to/generate-data-qa?#using-the-generated-data-in-prompt-flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # add clean up steps if needed\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
