{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Video Chunk Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9cb96",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Sequential processing of video chunks in GPT-4V.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d46d8",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a09083",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308be02",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_video, process_video_indexing\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d4a0f",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "\n",
    "Here we will load the configurations from _config.json_ file to setup vision_api_key, vision_api_endpoint, video_SAS_url, video_index_name, and video_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_script_dir = Path(parent_dir)\n",
    "\n",
    "# Load config values\n",
    "with Path(current_script_dir / \"config.json\").open() as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# Setting up the vision resource key\n",
    "vision_api_key = os.getenv(\"VISION_API_KEY\")\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "# You must create your resource in the East US region.\n",
    "vision_api_endpoint = config_details[\"VISION_API_ENDPOINT\"]\n",
    "\n",
    "# Insert your video SAS URL, e.g. https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-video-name>?<SAS-token>\n",
    "video_SAS_url = \"https://gpt4vsamples.blob.core.windows.net/videos/Redwire%20Field%20Trip%20-%203D%20Printing%20a%20Zune.mkv\"  # config_details[\"VIDEO_SAS_URL\"]\n",
    "\n",
    "# This index name must be unique\n",
    "video_index_name = config_details[\"VIDEO_INDEX_NAME\"]\n",
    "\n",
    "# This video ID must be unique\n",
    "video_id = config_details[\"VIDEO_INDEX_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181ebb5",
   "metadata": {},
   "source": [
    "### Create Video Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ffbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this cell once to create the index\n",
    "process_video_indexing(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1338c",
   "metadata": {},
   "source": [
    "### Call GPT-4V API with Video Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4343a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call GPT-4V API with Video Index on Each Video Chunk Sequentially\n",
    "\n",
    "\n",
    "def download_video(sas_url: str, local_file_path: str) -> bool:\n",
    "    try:\n",
    "        response = requests.get(sas_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with Path(local_file_path).open(\"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            return True\n",
    "\n",
    "        print(f\"Download failed with status code: {response.status_code}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during download: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_video_length(file_path: str) -> int or None:\n",
    "    try:\n",
    "        with VideoFileClip(file_path) as video:\n",
    "            return video.duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error in getting video length: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define the config values\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "video_config = {\n",
    "    \"video_SAS_url\": video_SAS_url,\n",
    "    \"video_index_name\": video_index_name,\n",
    "}\n",
    "\n",
    "# Define the number of seconds for each segment\n",
    "chunk_size = 120  # seconds\n",
    "# Download the video\n",
    "local_file_path = \"downloaded_video.mp4\"\n",
    "if download_video(video_SAS_url, local_file_path):\n",
    "    video_length = get_video_length(local_file_path)\n",
    "    Path(local_file_path).unlink()  # Delete the downloaded video\n",
    "\n",
    "    if video_length is not None:\n",
    "        print(f\"Video Length: {video_length} seconds\")\n",
    "        sys_message = f\"\"\"\n",
    "        The total length of the video is {video_length}s. Your job is to analyze a {chunk_size}-\n",
    "        sec segment of the video and 20 frames from that segment. You will then provide a Current Scene Breakdown of the \n",
    "        video so far. Scenes must cover the entire video and non-overlapping. This breakdown should be a JSON object, with \n",
    "        each scenes being a key, and the value being an array of information about the scene, including topic, visual description,\n",
    "        start and end times formated MM:SS.\n",
    "        \"\"\"\n",
    "        number_of_segments = int(video_length // chunk_size)\n",
    "        updated_response = \"\"\n",
    "        for i in range(number_of_segments + 1):  # Include the last segment\n",
    "            start_time = i * chunk_size\n",
    "            end_time = min((i + 1) * chunk_size, video_length)\n",
    "            user_prompt = f\"How many scenes from {start_time}s to {end_time}s?\"\n",
    "            print(f\"Segment {i+1}: {user_prompt}\")\n",
    "            if i > 0:\n",
    "                user_prompt += f\"\"\"And here are scenes in the previous segments: {updated_response}. \n",
    "                                You need to combine the scenes in the previous segments with the scenes in this segment and provide a summary.\n",
    "                                \"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"acv_document_id\", \"acv_document_id\": video_id}]},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]},\n",
    "            ]\n",
    "\n",
    "            response = call_GPT4V_video(messages, vision_api=vision_api_config, video_index=video_config)\n",
    "            updated_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(f\"Response for segment {i+1}: {updated_response}\")\n",
    "            time.sleep(2)  # Avoid throttling\n",
    "\n",
    "        # Print the final response\n",
    "        sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", updated_response)\n",
    "        for sentence in sentences:  # Print the content of the response\n",
    "            print(sentence)\n",
    "    else:\n",
    "        print(\"Failed to process video length.\")\n",
    "else:\n",
    "    print(\"Failed to download video.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66c9b7",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
