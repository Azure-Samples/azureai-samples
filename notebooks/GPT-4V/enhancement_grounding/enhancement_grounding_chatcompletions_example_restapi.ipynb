{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Grounding Enhanchment Samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c979e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Applying grounding techniques to image inputs in GPT-4V.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80aa3a",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c95aa",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f1146",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf25b",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION, VISION_API_ENDPOINT.\n",
    "\n",
    "Add \"OPENAI_API_KEY\" and \"VISION_API_KEY\" as variable name and \\<Your API Key Value\\> and \\<Your VISION Key Value\\> as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx VISION_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export VISION_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31280a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"<your GPT-4V deployment name>\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"<your resource base URL>\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"<your OpenAI API version>\"\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "vision_api_endpoint: str = \"<your vision resource endpoint>\"\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0101f5",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "    \"VISION_API_ENDPOINT\": vision_api_endpoint,\n",
    "}\n",
    "\n",
    "p = Path(\"../config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07279585",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef62557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from typing import Tuple\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_image\n",
    "\n",
    "# Setting up the vision resource key\n",
    "vision_api_key = os.getenv(\"VISION_API_KEY\")\n",
    "\n",
    "\n",
    "def random_color() -> Tuple[int, int, int]:\n",
    "    \"\"\"Generate a random color.\"\"\"\n",
    "    return (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "\n",
    "def draw_groundings(image_path: str, groundings: object) -> None:\n",
    "    # Load the image\n",
    "    original_image = Image.open(image_path)\n",
    "    width, height = original_image.size\n",
    "\n",
    "    box_width = max(2, width // 200)\n",
    "    font_size = max(10, height // 30)\n",
    "\n",
    "    extended_width = width + 200  # More space for text\n",
    "    image = Image.new(\"RGB\", (extended_width, height), \"white\")\n",
    "    image.paste(original_image, (0, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    predefined_colors = [\"red\", \"green\", \"blue\", \"purple\", \"orange\", \"pink\", \"cyan\"]\n",
    "    text_color_map = {}  # Dictionary to map text to colors\n",
    "    text_x = width + 20  # Starting just after the original image\n",
    "    text_y = 20  # Initial vertical position\n",
    "\n",
    "    # Iterate over each grounding in the data\n",
    "    for grounding in groundings:\n",
    "        text = grounding[\"text\"]\n",
    "        polygon = grounding[\"polygon\"]\n",
    "\n",
    "        if text not in text_color_map:\n",
    "            if predefined_colors:\n",
    "                text_color_map[text] = predefined_colors.pop(0)\n",
    "            else:\n",
    "                text_color_map[text] = random_color()\n",
    "\n",
    "        color = text_color_map[text]\n",
    "        absolute_polygon = [(p[\"x\"] * width, p[\"y\"] * height) for p in polygon]\n",
    "\n",
    "        bounding_box = [\n",
    "            min(p[0] for p in absolute_polygon),\n",
    "            min(p[1] for p in absolute_polygon),\n",
    "            max(p[0] for p in absolute_polygon),\n",
    "            max(p[1] for p in absolute_polygon),\n",
    "        ]\n",
    "        draw.rectangle(bounding_box, outline=color, width=box_width)\n",
    "\n",
    "    # Draw the text on the extended right side of the image\n",
    "    for text, color in text_color_map.items():\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"DejaVuSans.ttf\", font_size)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Text wrapping\n",
    "        wrapped_text = textwrap.fill(text, width=40)  # Adjust width as needed\n",
    "        for line in wrapped_text.split(\"\\n\"):\n",
    "            if text_y + font_size < height:\n",
    "                draw.text((text_x, text_y), line, fill=color, font=font)\n",
    "                text_y += font_size + 5  # Increment y position for next line\n",
    "            else:\n",
    "                break  # Stop if there's no more space\n",
    "\n",
    "    # Save or display the image\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Image Description Assistant\n",
    "image_file_path = \"ImageDescriptionAssistant.jpg\"  # Update with your image path\n",
    "sys_message = \"You are an AI assistant that helps people craft a clear and detailed sentence that describes the content depicted in an image.\"\n",
    "user_prompt = \"Describe image\"\n",
    "\n",
    "# Encode the image in base64\n",
    "with Path(image_file_path).open(\"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},  # Prompt for the user\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"},  # Image to be processed\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "# Send the request and handle the response\n",
    "try:\n",
    "    response_content = call_GPT4V_image(messages, grounding=True, vision_api=vision_api_config)\n",
    "    text = response_content[\"choices\"][0][\"message\"][\"content\"]\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    for sentence in sentences:  # Print the content of the response\n",
    "        print(sentence)\n",
    "    draw_groundings(image_file_path, response_content[\"choices\"][0][\"enhancements\"][\"grounding\"][\"lines\"][0][\"spans\"])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4V API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc58cb",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
