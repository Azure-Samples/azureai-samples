{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API RAG Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d36fb4",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Enhancing capabilities by bringing custom data to augment image inputs in GPT-4V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f04836",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb619be9",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d4a0f",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION, VISION_API_ENDPOINT, AZURE_SEARCH_SERVICE_ENDPOINT, AZURE_SEARCH_INDEX_NAME in _config.json_ file.\n",
    "\n",
    "```js\n",
    "{\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\":\"<GPT-4V Deployment Name>\",\n",
    "    \"OPENAI_API_BASE\":\"https://<Your Azure Resource Name>.openai.azure.com\",\n",
    "    \"OPENAI_API_VERSION\":\"<OpenAI API Version>\",\n",
    "\n",
    "    \"VISION_API_ENDPOINT\": \"https://<Your Azure Vision Resource Name>.cognitiveservices.azure.com\",\n",
    "    \n",
    "    \"AZURE_SEARCH_SERVICE_ENDPOINT\": \"https://<Your Azure Search Resource Name>.search.windows.net\",\n",
    "    \"AZURE_SEARCH_INDEX_NAME\": \"<Your Azure Search Index Name>\",\n",
    "}\n",
    "```\n",
    "Add \"OPENAI_API_KEY\", \"VISION_API_KEY\", and \"AZURE_SEARCH_QUERY_KEY\" as variable name and \\<Your API Key Value\\>, \\<Your VISION Key Value\\>, and \\<Your SEARCH Query Key Value\\> as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx VISION_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx AZURE_SEARCH_QUERY_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export VISION_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export AZURE_SEARCH_QUERY_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "And you will load the configurations from _config.json_ file to setup vision_api_key, vision_api_endpoint, search_service_endpoint, search_index_name, and search_query_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_image\n",
    "\n",
    "should_cleanup: bool = False\n",
    "\n",
    "current_script_dir = Path(parent_dir)\n",
    "\n",
    "# Load config values\n",
    "with Path(current_script_dir / \"config.json\").open() as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# Setting up the vision resource key\n",
    "vision_api_key = os.getenv(\"VISION_API_KEY\")\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "vision_api_endpoint = config_details[\"VISION_API_ENDPOINT\"]\n",
    "\n",
    "# Setting up the Azure Search service endpoint.e.g. https://<your search service name>.search.windows.net\n",
    "search_service_endpoint = config_details[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "\n",
    "# Setting up the Azure Search service index\n",
    "search_index_name = config_details[\"AZURE_SEARCH_INDEX_NAME\"]\n",
    "\n",
    "# Setting up the Azure Search service query key\n",
    "search_query_key = os.getenv(\"AZURE_SEARCH_QUERY_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbb148",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8ca0b",
   "metadata": {},
   "source": [
    "### Create Azure Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd94299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Azure Search service create the index with image embeddings\n",
    "# https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/azure-search-vector-image-index-creation-python-sample.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffb3f0",
   "metadata": {},
   "source": [
    "### Call GPT-4V API with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6165c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System messages and user prompt\n",
    "sys_message = \"You are an AI assistant that helps people find information.\"\n",
    "user_prompt = \"What are the types of the apple(s) shown in this image?\"\n",
    "\n",
    "# Encode the image in base64\n",
    "image_file_path = \"test_Gala.jpg\"\n",
    "with Path(image_file_path).open(\"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},  # Prompt for the user\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"},  # Image to be processed\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "in_context_config = {\"endpoint\": search_service_endpoint, \"key\": search_query_key, \"indexName\": search_index_name}\n",
    "\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "try:\n",
    "    response_content = call_GPT4V_image(messages, in_context=in_context_config, vision_api=vision_api_config)\n",
    "    display(Image(image_file_path))\n",
    "    print(response_content[\"choices\"][0][\"message\"][\"content\"])  # Print the content of the response\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4V API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9af58e",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0709c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
