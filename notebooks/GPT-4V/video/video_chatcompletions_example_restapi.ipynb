{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Video Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cfcba",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Conducting Q&A with video inputs in GPT-4V.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbcb1f",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13636fdc",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232525c",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ab502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d4a0f",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION, VISION_API_ENDPOINT, VIDEO_SAS_URL, VIDEO_INDEX_NAME, VIDEO_INDEX_ID  in _config.json_ file.\n",
    "\n",
    "```js\n",
    "{\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\":\"<GPT-4V Deployment Name>\",\n",
    "    \"OPENAI_API_BASE\":\"https://<Your Azure Resource Name>.openai.azure.com\",\n",
    "    \"OPENAI_API_VERSION\":\"<OpenAI API Version>\",\n",
    "\n",
    "    \"VISION_API_ENDPOINT\": \"https://<Your Azure Vision Resource Name>.cognitiveservices.azure.com\",\n",
    "\n",
    "    \"VIDEO_SAS_URL\": \"<Your Azure Blob Storage SAS URL>\",\n",
    "    \"VIDEO_INDEX_NAME\": \"<Your Azure Video Index Name>\",\n",
    "    \"VIDEO_INDEX_ID\": \"<Your Azure Video Index ID>\"    \n",
    "}\n",
    "```\n",
    "Add \"OPENAI_API_KEY\" and \"VISION_API_KEY\" as variable name and \\<Your API Key Value\\> and \\<Your VISION Key Value\\> as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx VISION_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export VISION_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "Then you will load the configurations from _config.json_ file to setup vision_api_key, vision_api_endpoint, video_SAS_url, video_index_name, and video_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_video, process_video_indexing\n",
    "\n",
    "should_cleanup: bool = False\n",
    "\n",
    "current_script_dir = Path(parent_dir)\n",
    "\n",
    "# Load config values\n",
    "with Path(current_script_dir / \"config.json\").open() as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# Setting up the vision resource key\n",
    "vision_api_key = os.getenv(\"VISION_API_KEY\")\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "# You must create your resource in the East US region.\n",
    "vision_api_endpoint = config_details[\"VISION_API_ENDPOINT\"]\n",
    "\n",
    "# Insert your video SAS URL, e.g. https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-video-name>?<SAS-token>\n",
    "video_SAS_url = \"https://gpt4vsamples.blob.core.windows.net/videos/Microsoft%20Copilot%20Short.mp4\"  # config_details[\"VIDEO_SAS_URL\"]\n",
    "\n",
    "# This index name must be unique\n",
    "video_index_name = \"copilot-video-demo-index\"  # config_details[\"VIDEO_INDEX_NAME\"]\n",
    "\n",
    "# This video ID must be unique\n",
    "video_id = \"copilot-video-1\"  # config_details[\"VIDEO_INDEX_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907668a",
   "metadata": {},
   "source": [
    "### Create Video Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ffbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this cell once to create the index\n",
    "process_video_indexing(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabcd3f",
   "metadata": {},
   "source": [
    "### Call GPT-4V API with Video Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6165c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System messages and user prompt\n",
    "sys_message = \"\"\"\n",
    "Your task is to assist in analyzing and optimizing creative assets. \n",
    "You will be presented with advertisement videos for products. \n",
    "First describe the video in detail paying close attention to Product characteristics highlighted, \n",
    "Background images, Lighting, Color Palette and Human characteristics for persons in the video. \n",
    "Finally provide a summary of the video and talk about the main message the advertisement video tries to convey to the viewer. \n",
    "\"\"\"\n",
    "user_prompt = \"Summarize the ad video\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"acv_document_id\", \"acv_document_id\": video_id}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]},  # Prompt for the user\n",
    "]\n",
    "\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "video_config = {\n",
    "    \"video_SAS_url\": video_SAS_url,\n",
    "    \"video_index_name\": video_index_name,\n",
    "}\n",
    "\n",
    "# Call GPT-4V API and print the response\n",
    "try:\n",
    "    response = call_GPT4V_video(messages, vision_api=vision_api_config, video_index=video_config)\n",
    "    text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    for sentence in sentences:  # Print the content of the response\n",
    "        print(sentence)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4V API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e4343",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb64f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
