{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulating and Evaluating Code Vulnerability\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook walks you through how to generate code using simulated prompts with the Simulator and evaluates that generated code for Code Vulnerability.\n",
    "\n",
    "## Time\n",
    "You should expect to spend about 30 minutes running this notebook. If you increase or decrease the amount of simulated code, the time will vary accordingly.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "The following simulator and evaluators require an Azure AI Studio project configuration and an Azure credential.\n",
    "Your project configuration will be what is used to log your evaluation results in your project after the evaluation run is finished.\n",
    "\n",
    "For full region supportability, see [our documentation](https://learn.microsoft.com/azure/ai-studio/how-to/develop/flow-evaluate-sdk#built-in-evaluators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_project = {\"subscription_id\": \"\", \"resource_group_name\": \"\", \"project_name\": \"\"}\n",
    "\n",
    "azure_openai_endpoint = \"\"\n",
    "azure_openai_deployment = \"\"\n",
    "azure_openai_api_version = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_API_VERSION\"] = azure_openai_api_version\n",
    "os.environ[\"AZURE_ENDPOINT\"] = azure_openai_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this example\n",
    "\n",
    "To keep this notebook lightweight, let's create a dummy application that calls an Azure OpenAI model, such as GPT-4. When testing your application for Code Vulnerability, it's important to have a way to auto generate code by providing user prompts for code generation. We will use the `Simulator` class and this is how we will generate a code against your application. Once we have this dataset, we can evaluate it with our `CodeVulnerabilityEvaluator` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import CodeVulnerabilityEvaluator\n",
    "from azure.ai.evaluation.simulator import AdversarialSimulator, AdversarialScenario\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "async def code_vuln_completion_callback(\n",
    "    messages: List[Dict], stream: bool = False, session_state: Optional[str] = None, context: Optional[Dict] = None\n",
    ") -> dict:\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    # Get a client handle for the model\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    # Call the model\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": messages[\"messages\"][0][\"content\"],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "        )\n",
    "        formatted_response = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    except Exception:\n",
    "        formatted_response = {\n",
    "            \"content\": \"I don't know\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"context\": {\"key\": {}},\n",
    "        }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": context,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Code Vulnerability\n",
    "\n",
    "When building your application, you want to test that vulnerable code is not being generated by your Generative AI applications. The following example uses an `AdversarialSimulator` paired with a code vulnerability scenario to prompt your model to respond with code that may or may not contain vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "code_vuln_scenario = AdversarialScenario.ADVERSARIAL_CODE_VULNERABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator below generates datasets that represent queries as user prompts and responses as code generated by LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    scenario=code_vuln_scenario,\n",
    "    max_conversation_turns=1,\n",
    "    max_simulation_results=1,\n",
    "    target=code_vuln_completion_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.ai.evaluation.simulator._utils import JsonLineChatProtocol\n",
    "from pathlib import Path\n",
    "\n",
    "with Path(\"adv_code_vuln_eval.jsonl\").open(\"w\") as file:\n",
    "    file.write(JsonLineChatProtocol(outputs[0]).to_eval_qr_json_lines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it for code vulnerability. The `CodeVulnerabilityEvaluator` class can take in the dataset and detect whether code vulnerability exists. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Foundry Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vuln_eval = CodeVulnerabilityEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"adv_code_vuln_eval.jsonl\",\n",
    "    evaluators={\"code_vulnerability\": code_vuln_eval},\n",
    "    # Optionally provide your AI Foundry project information to track your evaluation results in your Azure AI Foundry project\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
