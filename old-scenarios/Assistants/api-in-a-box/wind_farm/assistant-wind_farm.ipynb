{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine Farm Management Assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "\n",
    "- Utilizing Assistant tools such as the Code Interpreter and Function calling, this bot is capable of retrieving a CSV file that illustrates turbine wind speed, voltage, and the last maintenance date. It assists you in reviewing the file contents and aids in determining whether a specific turbine is in need of maintenance.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "- Access to Azure OpenAI Service - you can apply for access [here](https://aka.ms/oai/access)\n",
    "- Azure OpenAI service - you can create it from instructions [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource)\n",
    "- Azure OpenAI Studio - go to [https://oai.azure.com/](https://oai.azure.com/) to work with the Assistants API Playground\n",
    "- A connection to the Azure OpenAI Service with a [Key and Endpoint](https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart)\n",
    "\n",
    "Reference:\n",
    "- Learn more about how to use Assistants with our [How-to guide on Assistants](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant).\n",
    "- [Assistants OpenAI Overview](https://platform.openai.com/docs/assistants/overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "This sample demonstrates the creation of an Azure OpenAI Assistant named \"Turbine Management Assistant\" utilizing the Azure OpenAI API. The assistant functions as a management tool for wind turbine farms, providing data, charts, and actionable insights regarding the status and performance of each turbine within the farm. The notebook/script orchestrates a dialogue with the assistant, navigating through different inquiries and scenarios to demonstrate its capabilities.\n",
    "\n",
    "### Data\n",
    "\n",
    "This sample uses files from the folder [`data/`](./data/) in this repo. You can clone this repo or copy this folder to make sure you have access to these files when running the sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "api_endpoint = os.getenv(\"OPENAI_URI\")\n",
    "api_key = os.getenv(\"OPENAI_KEY\")\n",
    "api_version = os.getenv(\"OPENAI_VERSION\")\n",
    "api_deployment_name = os.getenv(\"OPENAI_GPT_DEPLOYMENT\")\n",
    "email_URI = os.getenv(\"EMAIL_URI\")\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from PIL import Image\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from openai.types import FileObject\n",
    "from openai.types.beta.threads import Message, TextContentBlock, ImageFileContentBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an AzureOpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(api_key=api_key, api_version=api_version, azure_endpoint=api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Assistant tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [\n",
    "    {\"type\": \"code_interpreter\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Assistant file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "\n",
    "\n",
    "def upload_file(client: AzureOpenAI, path: str) -> FileObject:\n",
    "    with Path(path).open(\"rb\") as f:\n",
    "        return client.files.create(file=f, purpose=\"assistants\")\n",
    "\n",
    "\n",
    "arr = os.listdir(DATA_FOLDER)\n",
    "assistant_files = []\n",
    "for file in arr:\n",
    "    filePath = DATA_FOLDER + file\n",
    "    assistant_files.append(upload_file(client, filePath))\n",
    "\n",
    "file_ids = [file.id for file in assistant_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Assistant and a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Portfolio Management Assistant\",\n",
    "    instructions=\"You are an assistant that can help manage wind turbine farm. \"\n",
    "    + \"The turbines operating ranges are output voltages of 33kv-35kv and RPM of 15-25. Wind speed is measured in miles per hour.\"\n",
    "    + \"Maintenance should occur every 12 months. Greet the user by saying, 'Welcome Turbine Management Assistant.'\",\n",
    "    tools=tools_list,\n",
    "    model=api_deployment_name,\n",
    "    tool_resources={\"code_interpreter\": {\"file_ids\": file_ids}},\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and display the Assistant Messages for text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(messages: Iterable[Message]) -> None:\n",
    "    message_list = []\n",
    "\n",
    "    # Get all the messages till the last user message\n",
    "    for message in messages:\n",
    "        message_list.append(message)\n",
    "        if message.role == \"user\":\n",
    "            break\n",
    "\n",
    "    # Reverse the messages to show the last user message first\n",
    "    message_list.reverse()\n",
    "\n",
    "    # Print the user or Assistant messages or images\n",
    "    for message in message_list:\n",
    "        for item in message.content:\n",
    "            # Determine the content type\n",
    "            if isinstance(item, TextContentBlock):\n",
    "                print(f\"{message.role}:\\n{item.text.value}\\n\")\n",
    "            elif isinstance(item, ImageFileContentBlock):\n",
    "                # Retrieve image from file id\n",
    "                response_content = client.files.content(item.image_file.file_id)\n",
    "                data_in_bytes = response_content.read()\n",
    "                # Convert bytes to image\n",
    "                readable_buffer = io.BytesIO(data_in_bytes)\n",
    "                image = Image.open(readable_buffer)\n",
    "                # Resize image to fit in terminal\n",
    "                width, height = image.size\n",
    "                image = image.resize((width // 2, height // 2), Image.LANCZOS)\n",
    "                # Display image\n",
    "                image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(content: str) -> None:\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=content)\n",
    "\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=\"The current date and time is: \" + datetime.now().strftime(\"%x %X\") + \". \",\n",
    "    )\n",
    "\n",
    "    print(\"processing...\")\n",
    "    completed_run = client.beta.threads.runs.poll(run_id=run.id, thread_id=thread.id)\n",
    "\n",
    "    # Check final status and handle accordingly\n",
    "    if completed_run.status == \"completed\":\n",
    "        print(\"Run completed successfully.\")\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        format_messages(messages)\n",
    "    elif completed_run.status == \"failed\":\n",
    "        print(\"Run failed.\")\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        format_messages(messages)\n",
    "    else:\n",
    "        print(f\"Unexpected run status: {completed_run.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Have a conversation with the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_message(\"What is the status of turbine 1001 and 1003?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_message(\"Generate a chart of the normal operating ranges?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_message(\n",
    "    \"What turbines are operating outside normal ranges?\\n\\n\"\n",
    "    + 'Sample: [{\"turbine\":\"\",\"voltage\":30,\"RPM\":15,\"reason\":\"Voltage outside of normal range.\"},\\n'\n",
    "    + '{\"turbine\":\"\",\"voltage\":30,\"rpm\":10,\"reason\":\"RPM and Voltage outside of normal ranges.\"},\\n'\n",
    "    + '{\"turbine\":\"\",\"voltage\":33,\"rpm\":5,\"reason\":\"RPM outside of normal range.\"}]\\n\\n'\n",
    "    + 'Output format: [{\"turbine\":\"\",\"voltage\":0,\"RPM\":0,\"reason\":\"\"}]\\n\\n'\n",
    "    + \"Output in JSON format only.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_message(\"What turbines need maintenance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    client.beta.assistants.delete(assistant.id)\n",
    "    client.beta.threads.delete(thread.id)\n",
    "    for file in assistant_files:\n",
    "        client.files.delete(file.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
