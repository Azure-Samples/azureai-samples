{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Assistant API Multi-Agent Example\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook we explore the transformative possibilities of building a multi-agent framework with the Azure Assistant API, a leap forward in making AI more accessible, customizable, and effective for developers and organizations alike. Whether you're an experienced developer or just starting, the Azure Assistant API is a tool that promises to revolutionize the way we think about and implement intelligent agents in our systems. The notebook below will walk you through an example pattern of creating a multi-agent system using the Azure OpenAI Assistant API. \n",
    "\n",
    "## This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Azure OpenAI Service\n",
    "\n",
    "    Azure OpenAI Service provides access to OpenAI's models including the GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, DALLE-3 and Embeddings model series with the security and enterprise capabilities of Azure.\n",
    "    \n",
    "    https://learn.microsoft.com/en-us/azure/ai-services/openai/\n",
    "\n",
    "- Azure OpenAI Assistants API\n",
    "\n",
    "    Assistants API makes it easier for developers to create applications with sophisticated copilot-like experiences that can sift through data, suggest solutions, and automate tasks.\n",
    "    \n",
    "    https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/assistants\n",
    "\n",
    "- GPT-4 Turbo with Vision\n",
    "\n",
    "    GPT-4 Turbo with Vision is a large multimodal model (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. It incorporates both natural language processing and visual understanding. \n",
    "    \n",
    "    https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/gpt-with-vision\n",
    "\n",
    "- Azure OpenAI Dall-e\n",
    "\n",
    "    A series of models in preview that can generate original images from natural language.\n",
    "    \n",
    "    https://learn.microsoft.com/en-us/azure/ai-services/openai/dall-e-quickstart\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample.\n",
    "\n",
    "## About this example\n",
    "\n",
    "The example provided in this notebook helps demonstrate how to build a multi-agent framework with Azure Assistant API and serves as a comprehensive guide for developers looking to harness the capabilities of multiple AI agents working in concert. The crux of the article is to showcase how agents can communicate and collaborate to process complex tasks, such as generating and enhancing images through multiple iterations based on user input. This is particularly relevant for developers and tech enthusiasts who are interested in exploring the frontiers of generative AI and multi-agent systems.\n",
    "\n",
    "Before getting started, one should have a basic understanding of AI and an interest in how agents can work together to enhance AI functionalities. The article does not delve into in-depth programming; however, a general knowledge of how APIs operate and the role of AI in automated systems would be beneficial in grasping the concepts presented. This example is an invitation to innovators and developers who wish to experiment with advanced AI systems and potentially integrate them into various industry solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "You must have these models provisioned on your Azure OpenAI Service\n",
    "\n",
    "- Assistant models - Latest 1106 models or 0125\n",
    "\n",
    "- GPT4 Vision Model - vision-preview\n",
    "\n",
    "- Dall-e 3 Model - dall-e-3 3.0\n",
    "\n",
    "Please make sure the assistant api model is available in the correct region:\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#assistants-preview\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best practice to keep your secrets and keys in an environment variable file. \n",
    "\n",
    "You can do this by editing the 'sample.env' file and renaming it to '.env'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Uncomment and run ' %pip install -r requirements.txt ' to install dependencies; re-comment after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from openai.types.beta import Thread\n",
    "from openai.types.beta import Assistant\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Create the AOAI client to use for the proxy agent.\n",
    "assistant_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"GPT4_AZURE_OPENAI_KEY\"),  # Your API key for the assistant api model\n",
    "    api_version=os.getenv(\"GPT4_AZURE_OPENAI_API_VERSION\"),  # API version  (i.e. 2024-02-15-preview)\n",
    "    azure_endpoint=os.getenv(\n",
    "        \"GPT4_AZURE_OPENAI_ENDPOINT\"\n",
    "    ),  # Your Azure endpoint (i.e. \"https://YOURENDPOINT.openai.azure.com/\")\n",
    ")\n",
    "# Assistant model should be '1106' or higher\n",
    "assistant_deployment_name = os.getenv(\n",
    "    \"GPT4_DEPLOYMENT_NAME\"\n",
    ")  # The name of your assistant model deployment in Azure OpenAI (i.e. \"GPT4Assistant\")\n",
    "\n",
    "# name of the model deployment for DALLÂ·E 3\n",
    "dalle_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"DALLE3_AZURE_OPENAI_KEY\"),\n",
    "    api_version=os.getenv(\"DALLE3_AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"DALLE3_AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "dalle_deployment_name = os.getenv(\"DALLE3_DEPLOYMENT_NAME\")\n",
    "\n",
    "# name of the model deployment for GPT 4 with Vision\n",
    "vision_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"GPT4VISION_AZURE_OPENAI_KEY\"),\n",
    "    api_version=os.getenv(\"GPT4VISION_AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"GPT4VISION_AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "vision_deployment_name = os.getenv(\"GPT4VISION_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generator Assistant\n",
    "\n",
    "This agent is responsible for generating images using a prompt to the Dalle-3 Model. The output is a .jpg file stored in the users local directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Image generator agent definition.\n",
    "\n",
    "- Assistant name = the name of the agent\n",
    "- Assistant instructions = description of the agent\n",
    "- Function = functions the agents has access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dl = \"dalle_assistant\"\n",
    "instructions_dl = \"\"\"As a premier AI specializing in image generation, you possess the expertise to craft precise visuals based on given prompts. It is essential that you diligently generate the requested image, ensuring its accuracy and alignment with the user's specifications, prior to delivering a response.\"\"\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_image\",\n",
    "            \"description\": \"Creates and displays an image\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The prompt to be used to create the image\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "verbose_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "Initializes the agent with the definition described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_assistant = assistant_client.beta.assistants.create(\n",
    "    name=name_dl, instructions=instructions_dl, model=assistant_deployment_name, tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generator \"generate_image\" function\n",
    "\n",
    "This function calls the Dalle-3 image generator given the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI Dall-e 3 model to generate an image from a text prompt.\n",
    "    Executes the call to the Azure OpenAI Dall-e 3 image creator, saves the file into the local directory, and displays the image.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Dalle Assistant Message: Creating the image ...\")\n",
    "\n",
    "    response = dalle_client.images.generate(\n",
    "        model=dalle_deployment_name, prompt=prompt, size=\"1024x1024\", quality=\"standard\", n=1\n",
    "    )\n",
    "\n",
    "    # Retrieve the image URL from the response (assuming response structure)\n",
    "    image_url = response.data[0].url\n",
    "\n",
    "    # Open the image from the URL and save it to a temporary file.\n",
    "    im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    # Define the filename and path where the image should be saved.\n",
    "    filename = \"temp.jpg\"\n",
    "    local_path = Path(filename)\n",
    "\n",
    "    # Save the image.\n",
    "    im.save(local_path)\n",
    "\n",
    "    # Get the absolute path of the saved image.\n",
    "    full_path = str(local_path.absolute())\n",
    "\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Convert the image from BGR to RGB for displaying with matplotlib,\n",
    "    # because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image with matplotlib.\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")  # Turn off axis labels.\n",
    "    plt.show()\n",
    "\n",
    "    # Return the full path of the saved image.\n",
    "    print(\"Dalle Assistant Message: \" + full_path)\n",
    "    return \"Image generated successfully and store in the local file system. You can now use this image to analyze it with the vision_assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Assistant\n",
    "\n",
    "This agent is responsible for analyzing images. The output is a new prompt to be used by the image creator agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vision assistant definition.\n",
    "\n",
    "- Assistant name = the name of the agent\n",
    "- Assistant instructions = description of the agent\n",
    "- Function = functions the agents has access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vs = \"vision_assistant\"\n",
    "instructions_vs = \"\"\"As a leading AI expert in image analysis, you excel at scrutinizing and offering critiques to refine and improve images. Your task is to thoroughly analyze an image, ensuring that all essential assessments are completed with precision before you provide feedback to the user. You have access to the local file system where the image is stored.\"\"\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"analyze_image\",\n",
    "            \"description\": \"analyzes and critics an image\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "verbose_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "Initializes the agent with the definition described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_assistant = assistant_client.beta.assistants.create(\n",
    "    name=name_vs, instructions=instructions_vs, model=assistant_deployment_name, tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Assistant \"analyze_image\" function\n",
    "\n",
    "This function calls the GPT4 Vision image analyzes given an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image() -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI GPT4 Vision model to analyze and critic an image and return the result.The resulting output should be a new prompt for dall-e that enhances the image based on the criticism and analysis\n",
    "    \"\"\"\n",
    "    print(\"Vision Assistant Message: \" + \"Analyzing the image...\")\n",
    "\n",
    "    import base64\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create a Path object for the image file\n",
    "    image_path = Path(\"temp.jpg\")\n",
    "\n",
    "    # Using a context manager to open the file with Path.open()\n",
    "    with image_path.open(\"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    content_images = [\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "        for base64_image in [base64_image]\n",
    "    ]\n",
    "    response = vision_client.chat.completions.create(\n",
    "        model=vision_deployment_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze and critic this image and generate a new enhanced prompt for Dall-e with the criticism and analysis.\",\n",
    "                    },\n",
    "                    *content_images,\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    print(\"Vision Assistant Message: \" + response.choices[0].message.content)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Proxy\n",
    "\n",
    "This agent facilitates the conversation between the user and other agents, ensuring successful completion of the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy Assistant definition.\n",
    "\n",
    "- Assistant name = the name of the agent\n",
    "- Assistant instructions = description of the agent including agents this user proxy has access to.\n",
    "- Function = functions the agents has access to. send_message sends messages to the other agents and agent_name specifies which agent to communicate with\n",
    "\n",
    "*** PLEASE NOTE THAT THE PLAN IS OPTIONAL ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pa = \"user_proxy\"\n",
    "agent_arr = [\"dalle_assistant\", \"vision_assistant\"]\n",
    "agent_string = \"\"\n",
    "for item in agent_arr:\n",
    "    agent_string += f\"{item}\\n\"\n",
    "\n",
    "instructions_pa = f\"\"\"As a user proxy agent, your primary function is to streamline dialogue between the user and the specialized agents within this group chat. You are tasked with articulating user inquiries with clarity to the relevant agents and maintaining a steady flow of communication to guarantee the user's request is comprehensively addressed. Please withhold your response to the user until the task is completed, unless an issue is flagged by the respective agent or when you can provide a conclusive reply.\n",
    "\n",
    "You have access to the local file system where files are stores. For example, you can access the image generated by the Dall-e assistant and send it to the Vision assistant for analysis.\n",
    "\n",
    "You have access to the following agents to accomplish the task:\n",
    "{agent_string}\n",
    "If the agents above are not enough or are out of scope to complete the task, then run send_message with the name of the agent.\n",
    "\n",
    "When outputting the agent names, use them as the basis of the agent_name in the send message function, even if the agent doesn't exist yet.\n",
    "\n",
    "Run the send_message function for each agent name generated. \n",
    "\n",
    "Do not ask for followup questions, run the send_message function according to your initial input.\n",
    "\n",
    "Plan:\n",
    "1. dalle_assistant creates image \n",
    "2. vision_assistant analyzes images and creates a new prompt for dalle_assistant\n",
    "3. dalle_assistant creates a new image based on the new prompt\n",
    "4. vision_assistant analyzes images and creates a new prompt for dalle_assistant\n",
    "5. dalle_assistant creates a new image based on the new prompt\n",
    "\n",
    "Now take a deep breath and accomplish the plan above. Always follow the plan step by step in the exact order and do not ask for followup questions. Do not skip any steps in the plan, do not repeat any steps and always complete the entire plan in order step by step.  \n",
    "The dall-e assistant can never run more than one time in a row, review your plan before running the next step.\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\"type\": \"code_interpreter\"},\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"send_message\",\n",
    "            \"description\": \"Send messages to other agents in this group chat.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The message to be sent\",\n",
    "                    },\n",
    "                    \"agent_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the agent to execute the task.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\", \"agent_name\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "verbose_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "Initializes the agent with the definition described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = assistant_client.beta.assistants.create(\n",
    "    name=name_pa, instructions=instructions_pa, model=assistant_deployment_name, tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy Assistant \"send_message\" function\n",
    "\n",
    "This function calls the Assistant API to generate a main thread of communication between the agents listed in the agents_threads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "agents_threads: Dict[str, Dict[str, Optional[str]]] = {\n",
    "    \"dalle_assistant\": {\"agent\": dalle_assistant, \"thread\": None},\n",
    "    \"vision_assistant\": {\"agent\": vision_assistant, \"thread\": None},\n",
    "}\n",
    "\n",
    "\n",
    "# Define the send_message function with only the query parameter\n",
    "def send_message(query: str, agent_name: str) -> str:\n",
    "    # Check if the agent_name is in agents_threads\n",
    "    if agent_name not in agent_arr:\n",
    "        print(\n",
    "            f\"Agent '{agent_name}' does not exist. This means that the multi-agent system does not have the necessary agent to execute the task. *** FUTURE CODE: AGENT SWARM***\"\n",
    "        )\n",
    "        # return None\n",
    "    # If the program has not exited, proceed with setting the agent recipient\n",
    "    recipient_type = agent_name\n",
    "    recipient_info = agents_threads[recipient_type]\n",
    "\n",
    "    # If the program has not exited, proceed with setting the agent recipient\n",
    "    recipient_type = agent_name\n",
    "    recipient_info = agents_threads[recipient_type]\n",
    "\n",
    "    # Create a new thread if user proxy and agent thread does not exist\n",
    "    if not recipient_info[\"thread\"]:\n",
    "        thread_object = assistant_client.beta.threads.create()\n",
    "        recipient_info[\"thread\"] = thread_object\n",
    "\n",
    "    # This function dispatches a message to the proper agent and it's thread\n",
    "    return dispatch_message(query, recipient_info[\"agent\"], recipient_info[\"thread\"])\n",
    "    # print(\"Proxy Assistant Message: \" + message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main multi-agent communication flow\n",
    "\n",
    "This agent facilitates the conversation between the user and other agents, ensuring successful completion of the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def dispatch_message(message: str, agent: Assistant, thread: Thread) -> str:\n",
    "    # Loops through all the agents functions to determine which function to use\n",
    "\n",
    "    available_functions = {}\n",
    "    # Iterate through each tool in the agent.tools list\n",
    "    for tool in agent.tools:\n",
    "        # Check if the tool has a 'function' attribute\n",
    "        if hasattr(tool, \"function\"):\n",
    "            function_name = tool.function.name\n",
    "            # Attempt to retrieve the function by its name and add it to the available_functions dictionary\n",
    "            if function_name in globals():\n",
    "                available_functions[function_name] = globals()[function_name]\n",
    "        else:\n",
    "            # Handle the case where the tool does not have a 'function' attribute\n",
    "            print(\"This tool does not have a 'function' attribute.\")\n",
    "    # Draft a new message as part of the ongoing conversation.\n",
    "    message = assistant_client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=message)\n",
    "    # Carry out the tasks delineated in this discussion thread.\n",
    "    run = assistant_client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=agent.id,\n",
    "    )\n",
    "    while True:\n",
    "        # Await the completion of the thread execution.\n",
    "        while run.status in [\"queued\", \"in_progress\"]:\n",
    "            run = assistant_client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "            time.sleep(1)\n",
    "\n",
    "        # If an action is necessary, initiate the appropriate function to perform the task.\n",
    "        if run.status == \"requires_action\":\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            for _tool_call in tool_calls:\n",
    "                tool_responses = []\n",
    "                if (\n",
    "                    run.required_action.type == \"submit_tool_outputs\"\n",
    "                    and run.required_action.submit_tool_outputs.tool_calls is not None\n",
    "                ):\n",
    "                    tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "                    for call in tool_calls:\n",
    "                        if call.type == \"function\":\n",
    "                            if call.function.name not in available_functions:\n",
    "                                raise Exception(\"Function requested by the model does not exist\")\n",
    "\n",
    "                            # Assign the appropriate function to the agent for invocation.\n",
    "                            function_to_call = available_functions[call.function.name]\n",
    "                            tool_response = function_to_call(**json.loads(call.function.arguments))\n",
    "                            tool_responses.append({\"tool_call_id\": call.id, \"output\": tool_response})\n",
    "\n",
    "            # Present the outcomes produced by the tool.\n",
    "            run = assistant_client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread.id, run_id=run.id, tool_outputs=tool_responses\n",
    "            )\n",
    "\n",
    "        # if the run is completed, return the assistant message else provide error\n",
    "        elif run.status == \"failed\":\n",
    "            raise Exception(\"Run Failed. \", run.last_error)\n",
    "        # Craft a reply from the assistant.\n",
    "        else:\n",
    "            messages = assistant_client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "            # Transmit the response message back to the facilitating agent.\n",
    "            return messages.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Questions:\n",
    "\n",
    "\n",
    "1.   Generate an image of a boat drifting in the water and analyze it and enhance the image\n",
    "2.   Following your plan strictly and step by step. Generate an image of a space civilization, analyze it and enhance it. Analyze and enhance it several times until image satisfies request\n",
    "\n",
    "Note: the assistants are configured to work together to complete the task. The user proxy agent is responsible for sending messages to the other agents and ensuring the task is completed successfully. It may take several iterations and several minutes to complete the task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = assistant_client.beta.threads.create()\n",
    "\n",
    "# Initiate proxy agent and the main thread. This thread will remain active until the task is completed and will serve as the main communication thread between the other agents.\n",
    "user_message = input(\"User Query: \")\n",
    "message = dispatch_message(user_message, user_proxy, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "To clean up all Azure Assistant threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = assistant_client.beta.assistants.delete(user_proxy.id)\n",
    "response = assistant_client.beta.assistants.delete(dalle_assistant.id)\n",
    "response = assistant_client.beta.assistants.delete(vision_assistant.id)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTFqnHcNCOfGXY2H+UtzIz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
