{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Safety Multi-modal Evaluations\n",
    "This following demo notebook demonstrates the evaluation of quality and safety evaluations for following multi-modal (text + images) scenarios.\n",
    "\n",
    "Azure AI evaluations provides a comprehensive Python SDK and studio UI experience for running evaluations for your generative AI applications. The notebook is broken up into the following sections:\n",
    "\n",
    "1. Setup and configuration\n",
    "2. Multi-modal Content Safety evaluator \n",
    "3. Using Evaluate API \n",
    "\n",
    "## 1. Setup and configuration\n",
    "First ensure you install the latest version of the evaluation package `azure-ai-evaluation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following multi-modal evaluators in this sample require an Azure AI Studio project configuration and an Azure credential to use. \n",
    "\n",
    "- ProtectedMaterialEvaluator\n",
    "\n",
    "- ContentSafetyEvaluator (This is composite version of following evaluators)\n",
    "\t\n",
    "    - ViolenceEvaluator\t\n",
    "    - SexualEvaluator\t\n",
    "    - SelfHarmEvaluator\t\n",
    "    - HateUnfairnessEvaluator\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the assignments below with the required values to run the rest of this sample. \n",
    "Ensure that you have downloaded and installed the Azure CLI and logged in with your Azure credentials using `az login` in your CLI prior to these steps. \n",
    "\n",
    "*Important*: We recommend using East US 2 as your AI Hub/AI project region to support all built-in safety evaluators. A subset of service-based safety evaluators are available in other regions, please see the supported regions in our [documentation](https://aka.ms/azureaistudiosafetyevalhowto). Please configure your project in a supported region to access the safety evaluation service via our evaluation SDK. Additionally, your project scope will be what is used to log your evaluation results in your project after the evaluation run is finished.\n",
    "\n",
    "Set the following environment variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_SUBSCRIPTION_ID\"] = \"\"\n",
    "os.environ[\"AZURE_RESOURCE_GROUP\"] = \"\"\n",
    "os.environ[\"AZURE_PROJECT_NAME\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "azure_cred = DefaultAzureCredential()\n",
    "project_scope = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-modal Content Safety Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.ai.evaluation import (\n",
    "    ViolenceEvaluator,\t\n",
    "    SexualEvaluator,\n",
    "    SelfHarmEvaluator,\t\n",
    "    HateUnfairnessEvaluator\n",
    ")\n",
    "\n",
    "violence_evaluator = ViolenceEvaluator(credential=azure_cred, azure_ai_project=project_scope)\n",
    "sexual_evaluator = SexualEvaluator(credential=azure_cred, azure_ai_project=project_scope)\t\n",
    "self_harm_evaluator = SelfHarmEvaluator(credential=azure_cred, azure_ai_project=project_scope)\t\n",
    "hate_unfair_evaluator = HateUnfairnessEvaluator(credential=azure_cred, azure_ai_project=project_scope)\n",
    "\n",
    "conversation = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"You are an AI Assistant that can describe images\"}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Can you describe this image?\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": \"https://cdn.britannica.com/68/178268-050-5B4E7FB6/Tom-Cruise-2013.jpg\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"The image shows a man with short brown hair smiling, wearing a dark-colored shirt.\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "result = violence_evaluator(conversation=conversation)\n",
    "pprint(result)\n",
    "result = sexual_evaluator(conversation=conversation)\n",
    "pprint(result)\n",
    "result = self_harm_evaluator(conversation=conversation)\n",
    "pprint(result)\n",
    "result = hate_unfair_evaluator(conversation=conversation)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Safety Evaluator supports multi-modal images + text\n",
    "Following code can run all the above individual safety evaluator together in one composite evaluator called Content Safety Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.ai.evaluation import ContentSafetyEvaluator\n",
    "\n",
    "evaluator = ContentSafetyEvaluator(credential=azure_cred, azure_ai_project=project_scope)\n",
    "result = evaluator(conversation=conversation)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protected Material Multi-modal Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.ai.evaluation import ProtectedMaterialEvaluator\n",
    "\n",
    "evaluator = ProtectedMaterialEvaluator(credential=azure_cred, azure_ai_project=project_scope)\n",
    "result = evaluator(conversation=conversation)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "data_path = os.path.join(pathlib.Path().resolve(), \"datasets\")\n",
    "file_path = os.path.join(data_path, \"dataset_messages_image_urls.jsonl\")\n",
    "    \n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "content_safety_eval = ContentSafetyEvaluator(\n",
    "    azure_ai_project=project_scope, credential=azure_cred\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    data=file_path,\n",
    "    azure_ai_project=project_scope,\n",
    "    evaluators={\"content_safety\": content_safety_eval},\n",
    ")\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
