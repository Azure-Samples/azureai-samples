{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07dafd54",
   "metadata": {},
   "source": [
    "# Fine-tuning and deploying an Azure OpenAI model with function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbeb6e",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc71b0e",
   "metadata": {},
   "source": [
    "This notebook walks you through fine-tuning and deploying a gpt-35-turbo-0613 model with function calling using stock use case datasets.\n",
    "\n",
    "Please note, fine-tuning with function calling is currently available for the gpt-35-turbo (0613) and gpt-35-turbo-16k (1106) models. With support for function calling, you can incorporate functions into your training data, and have your fine-tuned model make function calls. You can find more details [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a005a99",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f173b2",
   "metadata": {},
   "source": [
    "You should expect to sepnd 60-90 min running this sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070ffcf",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34031134",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1be605",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai json requests os tiktoken time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c3c20",
   "metadata": {},
   "source": [
    "## Prepare your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4013c96",
   "metadata": {},
   "source": [
    "The training data you use must be formatted as a JSON Lines (JSONL) document. Structure your examples as demonstrated, with each line including a list of \"messages\" and an optional list of \"functions\". The example below features two functions: the first one retrieves the current stock price, and the second one gets the stock price of last n days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3aca15",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. If you can't find the exact stock ticker symbol, you can ask for clarification. \"}, {\"role\": \"user\", \"content\": \"What was the highest price that Bank of America's stock reached last month?\"}, {\"role\": \"assistant\", \"function_call\": {\"name\": \"get_last_nday_stock_price\", \"arguments\": \"{\\\"symbol\\\": \\\"BAC\\\", \\\"period\\\": \\\"1mo\\\"}\"}}], \"functions\": [{\"name\": \"get_current_stock_price\", \"description\": \"Get the current stock price\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\", \"description\": \"The stock symbol\"}}, \"required\": [\"symbol\"]}}, {\"name\": \"get_last_nday_stock_price\", \"description\": \"Get stock price last n days\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\", \"description\": \"The stock symbol\"}, \"period\": {\"type\": \"string\", \"description\": \"Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\"}}, \"required\": [\"symbol\", \"period\"]}}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4f4e2",
   "metadata": {},
   "source": [
    "While this one example is helpful to give you the general format, if you want to steer your custom fine-tuned model to respond in a similar way you would need more examples. Generally you want **at least 100 high quality examples**.\n",
    "\n",
    "We have already created training and test datasets for our two scenarios:\n",
    "\n",
    "**Hallucination:** A common problem with large language models is hallucinations â€“ providing plausible but false responses. With function calling, hallucinations can happen when the model calls a function in the wrong context or provides incorrect information to for the function call. We will evaluate whether the fine-tuned model can correctly identify fake companies, and respond appropriately, instead of trying to quote a stock price. \n",
    "\n",
    "Hallucination datasets: `stock-train-hallucination.jsonl` and `stock-test-hallucination.jsonl`\n",
    "\n",
    "**Token reduction:** The inclusion of functions in the system message directly impacts token usage. As the number of functions grows, so does the number of tokens within the system message, resulting in verbose prompts and increased costs.  Fine tuning lets you shorten your function calls.\n",
    "\n",
    "Token reduction datasets: `stock-train-token-reduction.jsonl` and `stock-test-token-reduction.jsonl`\n",
    "\n",
    "You can finetune a model with function calling for any of these two use cases. The fine-tuned models will be used in the inference notebooks in this repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1b6ed",
   "metadata": {},
   "source": [
    "Let's run some preliminary checks on our training and validation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you need to run some preliminary checks on our training and validation files.\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the training set\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming the current directory is the root of your repository\n",
    "with Path(\"Data/stock-train-hallucination.jsonl\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    training_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# Training dataset stats\n",
    "print(\"Number of examples in training set:\", len(training_dataset))\n",
    "print(\"First example in training set:\")\n",
    "for message in training_dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fc9fd",
   "metadata": {},
   "source": [
    "Now you can then run some additional code from OpenAI using the tiktoken library to validate the token counts. Individual examples need to remain under the gpt-35-turbo-0613 model's input token limit of 4096 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac38ca-fcdd-445b-a5ab-c52d03c0c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If that completes successfully, you can then run some additional code from OpenAI using the tiktoken library to validate the token counts.\n",
    "\n",
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "encoding = tiktoken.get_encoding(\n",
    "    \"cl100k_base\"\n",
    ")  # default encoding used by gpt-4, turbo, and text-embedding-ada-002 models\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(\n",
    "    messages: List[Dict[str, Any]], tokens_per_message: int = 3, tokens_per_name: int = 1\n",
    ") -> int:\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if isinstance(value, str):\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "            else:\n",
    "                num_tokens += len(encoding.encode(str(value)))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages: List[Dict[str, Any]]) -> int:\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        content = message.get(\"content\")\n",
    "        if content and message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(content))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def print_distribution(values: List[int], name: str) -> None:\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "\n",
    "files: List[str] = [\"Data/stock-train-hallucination.jsonl\"]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    with Path(file).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    total_tokens = []\n",
    "    assistant_tokens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex.get(\"messages\", {})\n",
    "        total_tokens.append(num_tokens_from_messages(messages))\n",
    "        assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print_distribution(total_tokens, \"total tokens\")\n",
    "    print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb215556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Upload fine-tuning files\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-12-01-preview\",  # This API version or later is required to access fine-tuning for turbo\n",
    ")\n",
    "\n",
    "training_file_name = \"Data/stock-train-hallucination.jsonl\"\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "with Path(training_file_name).open(\"rb\") as file:\n",
    "    training_response = client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "training_file_id = training_response.id\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75243ac5",
   "metadata": {},
   "source": [
    "Now that the fine-tuning files have been successfully uploaded you can submit your fine-tuning training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-35-turbo-0613\",  # Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters.\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa18af8",
   "metadata": {},
   "source": [
    "## Track training job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training status\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the status of our fine-tuning job.\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "status = response.status\n",
    "\n",
    "# If the job isn't done yet, poll it every 10 seconds.\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)\n",
    "\n",
    "    response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    print(response.model_dump_json(indent=2))\n",
    "    print(\n",
    "        \"Elapsed time: {} minutes {} seconds\".format(\n",
    "            int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)\n",
    "        )\n",
    "    )\n",
    "    status = response.status\n",
    "    print(f\"Status: {status}\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"Fine-tuning job {job_id} finished with status: {status}\")\n",
    "\n",
    "# List all fine-tuning jobs for this resource.\n",
    "print(\"Checking other fine-tune jobs for this resource.\")\n",
    "response = client.fine_tuning.jobs.list()\n",
    "print(f\"Found {len(response.data)} fine-tune jobs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d1f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve fine_tuned_model name\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "fine_tuned_model = response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bcf98",
   "metadata": {},
   "source": [
    "## Deploy fine-tune dmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01c18f",
   "metadata": {},
   "source": [
    "Here is how you can deploy your fine-tuned model using the [Rest API](https://learn.microsoft.com/en-us/rest/api/cognitiveservices/accountmanagement/deployments/create-or-update?tabs=HTTP) which requires separate authorization, a different API path, and a different API version. Alternatively, you can deploy your fine-tuned model using any of the other common deployment methods like [Azure OpenAI Studio](https://oai.azure.com/), or [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account/deployment#az-cognitiveservices-account-deployment-create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "token = os.getenv(\"TEMP_AUTH_TOKEN\")\n",
    "subscription = \"<YOUR_SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<YOUR_RESOURCE_GROUP_NAME>\"\n",
    "resource_name = \"<YOUR_AZURE_OPENAI_RESOURCE_NAME>\"\n",
    "model_deployment_name = \"YOUR_CUSTOM_MODEL_DEPLOYMENT_NAME\"\n",
    "\n",
    "deploy_params = {\"api-version\": \"2023-05-01\"}\n",
    "deploy_headers = {\"Authorization\": \"Bearer {}\".format(token), \"Content-Type\": \"application/json\"}\n",
    "\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"standard\", \"capacity\": 1},\n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": \"<YOUR_FINE_TUNED_MODEL>\",  # retrieve this value from the previous call, it will look like gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83\n",
    "            \"version\": \"1\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "request_url = f\"https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}\"\n",
    "\n",
    "print(\"Creating a new deployment...\")\n",
    "\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d5ed5",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92eb99b",
   "metadata": {},
   "source": [
    "You can delete a custom model on the Models pane in Azure OpenAI Studio. Select the custom model to delete from the Customized models tab, and then select Delete to delete the custom model.\n",
    "\n",
    "You can delete the deployment for your custom model on the Deployments pane in Azure OpenAI Studio. Select the deployment to delete, and then select Delete to delete the deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
