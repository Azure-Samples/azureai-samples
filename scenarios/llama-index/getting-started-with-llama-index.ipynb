{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Llama-Index with models in the Azure AI model catalog\n",
    "\n",
    "In this notebook, you learn how to use `llama-index` with models deployed from the Azure AI model catalog deployed to Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "To run this tutorial you need either:\n",
    "\n",
    "1. Using GitHub Models:\n",
    "\n",
    "    1. You can use [GitHub models](https://github.com/marketplace/models) endpoint including the free tier experience.\n",
    "    2. Use the endpoint `https://models.inference.ai.azure.com` along with your GitHub Token.\n",
    "\n",
    "1. Using Azure AI Foundry:\n",
    "\n",
    "    1. Create an [Azure subscription](https://azure.microsoft.com).\n",
    "    2. Create an Azure AI hub resource as explained at [How to create and manage an Azure AI Studio hub](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource).\n",
    "    3. Deploy one model supporting the [Azure AI model inference API](https://aka.ms/azureai/modelinference). In this example we use a `Mistral-Large` deployment. \n",
    "\n",
    "        * You can follow the instructions at [Add and configure models to Azure AI model inference service](https://learn.microsoft.com/azure/ai-studio/ai-services/how-to/create-model-deployments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies\n",
    "\n",
    "Ensure you have `llama-index` installed:\n",
    "\n",
    "```bash\n",
    "pip install llama-index\n",
    "```\n",
    "\n",
    "Models deployed to Azure AI studio or Azure Machine Learning can be used with `llama-index` in two ways:\n",
    "\n",
    "- **Using the Azure AI model inference API:** All models deployed to Azure AI studio and Azure Machine Learning support the Azure AI model inference API, which offers a common set of functionalities that can be used for most of the models in the catalog. The benefit of this API is that, since it's the same for all the models, changing from one to another is as simple as changing the model deployment being use. No further changes are required in the code. When working with `llama-index`, install the extensions `llama-index-llms-azure-inference` and `llama-index-embeddings-azure-inference`.\n",
    "- **Using the model's provider specific API:** Some models, like OpenAI, Cohere, or Mistral, offer their own set of APIs and extensions for `llama-index`. Those extensions may include specific functionalities that the model support and hence are suitable if you want to exploit them. When working with `llama-index`, install the extension specific for the model you want to use, like `llama-index-llms-openai` or `llama-index-llms-cohere`.\n",
    "\n",
    "\n",
    "In this example, we are working with the Azure AI model inference API, hence we install the following packages:\n",
    "\n",
    "```bash\n",
    "pip install -U llama-index-llms-azure-inference\n",
    "pip install -U llama-index-embeddings-azure-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set environment variables\n",
    "\n",
    "Follow these steps to get the information you need from the model you want to use:\n",
    "\n",
    "1. Go to the [Azure AI Foundry portal](https://ai.azure.com/) or [Azure Machine Learning studio](https://ml.azure.com), depending on the product you are using.\n",
    "\n",
    "2. Go to **Models + Endpoints** (**Endpoints** in Azure Machine Learning) and select the model you deployed as indicated in the prerequisites.\n",
    "\n",
    "3. Copy the endpoint URL and the key.\n",
    "    \n",
    "> If your model was deployed with Microsoft Entra ID support, you don't need a key.\n",
    "\n",
    "In this scenario, we placed both the endpoint URL and key in the following environment variables:\n",
    "\n",
    "```bash\n",
    "export AZURE_INFERENCE_ENDPOINT=\"<your-model-endpoint-goes-here>\"\n",
    "export AZURE_INFERENCE_CREDENTIAL=\"<your-key-goes-here>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to your deployment and endpoint\n",
    "\n",
    "To use LLMs deployed in Azure AI Foundry or Azure Machine Learning, you need the endpoint and credentials to connect to it. The parameter `model_name` is not required for endpoints serving a single model, like Managed Online Endpoints or Serverless API Endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.azure_inference import AzureAICompletionsModel\n",
    "\n",
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model_name=\"mistral-large-2407\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are using OpenAI models, the parameter `api_version` may be required in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if your endpoint support Microsoft Entra ID, you can use the following code to create the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=DefaultAzureCredential(),\n",
    "    model_name=\"mistral-large-2407\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: When using Microsoft Entra ID, make sure that the endpoint was deployed with that authentication method and that you have the required permissions to invoke it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to use asynchronous calling, it's a best practice to use the asynchronous version for the credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity.aio import (\n",
    "    DefaultAzureCredential as DefaultAzureCredentialAsync,\n",
    ")\n",
    "\n",
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=DefaultAzureCredentialAsync(),\n",
    "    model_name=\"mistral-large-2407\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use LLMs models\n",
    "\n",
    "Use the `complete` endpoint for text completion. The `complete` method is still available for model of type `chat-completions`. On those cases, your input text is converted to a message with `role=\"user\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(\"The sky is a beautiful blue and\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_complete(\"The sky is a beautiful blue and\")\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use `chat` for chat completion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a pirate with colorful personality.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Hello\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_chat(messages)\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than adding same parameters to each chat or completion call, you can set them at the client instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model_name=\"mistral-large-2407\",\n",
    "    temperature=0.0,\n",
    "    model_kwargs={\"top_p\": 1.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters not supported in the Azure AI model inference API ([reference](https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-chat-completions.md)) but available in the underlying model, you can use the `model_extras` argument. In the following example, the parameter `safe_prompt`, only available for Mistral models, is being passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    temperature=0.0,\n",
    "    model_kwargs={\"model_extras\": {\"safe_prompt\": True}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use embeddings models\n",
    "\n",
    "In the same way you create an LLM client, you can connect to an embedding model. In the following example, we are setting again the environment variable to now point to an embeddings model:\n",
    "\n",
    "```bash\n",
    "export AZURE_INFERENCE_ENDPOINT=\"<your-model-endpoint-goes-here>\"\n",
    "export AZURE_INFERENCE_CREDENTIAL=\"<your-key-goes-here>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel\n",
    "\n",
    "embed_model = AzureAIEmbeddingsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model_name=\"cohere-embed-v3-english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then configure your session to use the embeddings model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure the models to be used by your code\n",
    "\n",
    "You can use the LLM or embeddings model client individually in the code you develop with `llama-index` or you can configure the entire session using the `Settings` options. Configuring the session has the advantage of all your code using the same models for all the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are scenarios where you want to use a general model for most of the operations but a specific one for a given task. On those cases, it's useful to set the LLM or embedding model you are using for each `llama-index` construct. In the following example, we set a specific model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you use a combination of both strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
