{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Simulator for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import freeze_support\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from azure.ai.generative.evaluate import evaluate\n",
    "import json\n",
    "from azure.ai.generative.synthetic.simulator import Simulator\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.identity import InteractiveBrowserCredential, DefaultAzureCredential\n",
    "from azure.ai.resources.entities import AzureOpenAIModelConfiguration\n",
    "import numpy as np\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "sub = \"\"\n",
    "rg = \"\"\n",
    "project_name = \"\"\n",
    "oai_client = AsyncAzureOpenAI(\n",
    "    api_key = \"\",\n",
    "    azure_endpoint = \"\",\n",
    "    api_version = \"2023-12-01-preview\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_oai_chat_completion_fn = oai_client.chat.completions.create\n",
    "client = AIClient(\n",
    "    subscription_id=sub,\n",
    "    resource_group_name=rg,\n",
    "    project_name=project_name,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "simulator = Simulator.from_fn(\n",
    "    fn=async_oai_chat_completion_fn,\n",
    "    ai_client=client,\n",
    "    model=\"gpt-4\",\n",
    "    max_tokens=300\n",
    ")\n",
    "adv_template = Simulator.get_template(\"adv_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator.simulate_async(\n",
    "    adv_template,\n",
    "    max_conversation_turns=2,\n",
    "    simulation_result_limit=3,\n",
    "    api_call_delay_sec=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for line in outputs:\n",
    "    print(json.dumps(line, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store outputs in file for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = \"conv-oai-qa-harm-eval\"\n",
    "file_name = f\"\\{eval_name}.jsonl\"\n",
    "jsonl_object = outputs.to_eval_qa_json_lines()\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write(jsonl_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_uri = client.tracking_uri\n",
    "connection = client.connections.get(\"Default_AzureOpenAI\")\n",
    "config = AzureOpenAIModelConfiguration.from_connection(\n",
    "    connection, \n",
    "    model_name=\"gpt-4\",\n",
    "    deployment_name=\"gpt-4\"\n",
    ")\n",
    "result = evaluate(\n",
    "    evaluation_name=eval_name, \n",
    "    data=os.path.join(os.getcwd() + f\"\\{eval_name}.jsonl\"),\n",
    "    task_type=\"qa\",\n",
    "    metrics_list=[\"hate_fairness\",\"violence\",\"self_harm\", \"sexual\", \"gpt_groundedness\", \"gpt_relevance\", \"gpt_coherence\", \"gpt_fluency\", \"gpt_similarity\"],\n",
    "    model_config=config,\n",
    "    data_mapping={\n",
    "        \"questions\": \"question\",\n",
    "        \"answer\": \"answer\",\n",
    "    },\n",
    "    tracking_uri=tracking_uri,\n",
    "    output_path=os.getcwd() + \"/downloaded_artifacts/remote\"\n",
    ")\n",
    "print(result)\n",
    "print(result.metrics_summary) # will print the defect rate for each content harm\n",
    "print(\"Studio URL\") \n",
    "print(result.studio_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
