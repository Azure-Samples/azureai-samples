{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Queries and Responses from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Use the Simulator to generate high-quality queries and responses from your data using LLMs.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Access to Azure OpenAI Service - you can apply for access [here](https://go.microsoft.com/fwlink/?linkid=2222006)\n",
    "- An Azure AI Studio project - go to [aka.ms/azureaistudio](https://aka.ms/azureaistudio) to create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "Large Language Models (LLMs) can help you create query and response datasets from your existing data sources such as text or index. These datasets can be useful for various tasks, such as testing your retrieval capabilities, evaluating and improving your RAG workflows, tuning your prompts and more. In this sample, we will explore how to use the Simulator to generate high-quality query and response pairs from your data using LLMs and simulate interactions with your application with them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this sample we will generate text data from Wikipedia. You can follow the same steps replacing the text with any other source documents of your application's interest. Make sure that the length of the text is within the selected Azure AI model's context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-identity azure-ai-evaluation\n",
    "%pip install wikipedia openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Lets initialize some variables. We need a way to connect to a LLM to use the notebook. This sample suggests a way to use `gpt-4o-mini` deployment in your Azure AI project. Replace the `azure_endpoint` with a link to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project details\n",
    "azure_endpoint = \"https://<ai-hub>.openai.azure.com\"\n",
    "azure_deployment = \"gpt-4o-mini\"  # replace with your deployment name, if different\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": azure_endpoint,\n",
    "    \"azure_deployment\": azure_deployment,\n",
    "}\n",
    "\n",
    "# JSON mode supported model preferred to avoid errors ex. gpt-4o-mini, gpt-4o, gpt-4 (1106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us connect to the project. DefaultAzureCredentails will be picked up by the SDK which runs the prompty files to authenticate your requests. If you want to use your AzureOpenAI key to authenticate, you can do so by setting the `api_key` in your `model_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source for the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia article\n",
    "In this example we use a wikipedia article as raw text generate Query Response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "search_term = \"Leonardo da vinci\"\n",
    "wiki_title = wikipedia.search(search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data from an Azure AI Search Index\n",
    "Use this step if you want to obtain text from your search index. This part assumes that you have a search index and have populated the index with necessary data that is relevant to your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "search_term = \"camping stove\"\n",
    "endpoint = \"https://<your-search-service-name>.search.windows.net\"\n",
    "index_name = \"<your-index-name>\"\n",
    "api_key = \"<your-api-key>\"\n",
    "url = f\"{endpoint}/indexes/{index_name}/docs/search?api-version=2024-07-01\"\n",
    "\n",
    "headers = {\n",
    "    \"api-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "search_query = {\"search\": search_term, \"top\": 1}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(search_query))\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    for result in results[\"value\"]:\n",
    "        text = result[\"content\"]\n",
    "\n",
    "text = text[:5000]  # lets make sure the text isn't too long to exceed the model's token limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the simulator to your application\n",
    "\n",
    "This part assumes that you application is a call to `AzureOpenAI`'s chat completion endpoint. It also assumes that the following values are set in the environment. Feel free to change this method to call your application with its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<your-api-key>\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<your-endpoint>\"\n",
    "os.environ[\"AZURE_DEPLOYMENT\"] = \"<your-deployment>\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"<api version>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        api_key=os.environ.get(\"AZURE_API_KEY\"),\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    try:\n",
    "        # call your endpoint or ai application here\n",
    "        response = call_to_your_ai_application(query)\n",
    "    except Exception as e:\n",
    "        response = f\"An error occurred: {e!s}\"\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to simulator\n",
    "This call to the simulator generates 4 query response pairs in its first pass.\n",
    "In the second pass, it picks up one task, pairs it with a query (generated in previous pass) and sends it to the configured llm to build the first user turn. This user turn is then passed to the `callback` method. The conversation continutes till the `max_conversation_turns` turns.\n",
    "\n",
    "The output of the simulator will have the original task, original query, the original query and the response generated from the first turn as expected response. You can find them in the `context` key of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=text,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=1,\n",
    "    tasks=[\n",
    "        f\"I am a student and I want to learn more about {search_term}\",\n",
    "        f\"I am a teacher and I want to teach my students about {search_term}\",\n",
    "        f\"I am a researcher and I want to do a detailed research on {search_term}\",\n",
    "        f\"I am a statistician and I want to do a detailed table of factual data concerning {search_term}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the user simulating behavior\n",
    "\n",
    "Internally, the SDK has a `prompty` file that defines how the LLM which simulates the user should behave. Our SDK also offers an option for users to override the file, to support your own prompty files. Here's a brief overview of how to accomplish overriding the user behavior.\n",
    "\n",
    "Make sure you have `user_override.prompty` file in the same directory. The file in this repo takes an additional argument called `mood`. This is to show how you can add any additional keyword arguments to your prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = Path.cwd()\n",
    "user_override_prompty = Path(current_directory) / \"user_override.prompty\"\n",
    "user_prompty_kwargs = {\"mood\": \"happy\"}\n",
    "\n",
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=text,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=1,\n",
    "    tasks=[\n",
    "        f\"I am a student and I want to learn more about {search_term}\",\n",
    "        f\"I am a teacher and I want to teach my students about {search_term}\",\n",
    "        f\"I am a researcher and I want to do a detailed research on {search_term}\",\n",
    "        f\"I am a statistician and I want to do a detailed table of factual data concerning {search_term}\",\n",
    "    ],\n",
    "    user_simulator_prompty=user_override_prompty,\n",
    "    user_simulator_prompty_kwargs=user_prompty_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(\"output.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluations on the simulated data\n",
    "\n",
    "Here we will try to run GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator on the output data from the simulator. \n",
    "\n",
    "From the [documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk) we know that running those evaluators need the following data:\n",
    "`query`, `response`, `context`, `ground_truth`\n",
    "\n",
    "For simplicity's sake, we can use our source document `text` as both `context` and `ground_truth`. This step only evaluates the first user message and first response from your AI Application for each of the simulated conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_json_lines = \"\"\n",
    "for output in outputs:\n",
    "    query = None\n",
    "    response = None\n",
    "    context = text\n",
    "    ground_truth = text\n",
    "    for message in output[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            query = message[\"content\"]\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            response = message[\"content\"]\n",
    "    if query and response:\n",
    "        eval_input_data_json_lines += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"response\": response,\n",
    "                    \"context\": context,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_file = Path(\"eval_input_data.jsonl\")\n",
    "with eval_input_data_file.open(\"w\") as f:\n",
    "    f.write(eval_input_data_json_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation\n",
    "\n",
    "`QAEvaluator` is a composite evaluator which runs GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "Optionally set the `azure_ai_project` to upload the evaluation results to Azure AI Studio. To support that, we need to install `promptflow-azure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"<your-subscription-id>\",\n",
    "    \"resource_group\": \"<your-resource-group>\",\n",
    "    \"workspace_name\": \"<your-workspace-name>\",\n",
    "}\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"fac34303-435d-4486-8c3f-7094d82a0b60\",\n",
    "    \"resource_group_name\": \"rg-naarkalgaihub\",\n",
    "    \"project_name\": \"naarkalg-rai-test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=model_config)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(eval_input_data_file),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # optional to store the evaluation results in Azure AI Studio\n",
    "    output_path=\"./myevalresults.json\",  # optional to store the evaluation results in a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
