{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Simulator for Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to leverage Generative AI's simulator to simulate an adversarial question answering scenario against an Azure OpenAI chat completions endpoint.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Generative AI's simulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 30 minutes running this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "This example demonstrates a simulated adversarial question answering and evaluation. It is important to have access to AzureOpenAI credentials and an AzureAI project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-ai-generative[simulator,evaluate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from azure.ai.generative.evaluate import evaluate\n",
    "import json\n",
    "from azure.ai.generative.synthetic.simulator import Simulator\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources.entities import AzureOpenAIModelConfiguration\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "sub = \"\"\n",
    "rg = \"\"\n",
    "project_name = \"\"\n",
    "oai_client = AsyncAzureOpenAI(api_key=\"\", azure_endpoint=\"\", api_version=\"2023-12-01-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_oai_chat_completion_fn = oai_client.chat.completions.create\n",
    "client = AIClient(\n",
    "    subscription_id=sub, resource_group_name=rg, project_name=project_name, credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "simulator = Simulator.from_fn(fn=async_oai_chat_completion_fn, ai_client=client, model=\"gpt-4\", max_tokens=300)\n",
    "adv_template = Simulator.get_template(\"adv_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator.simulate_async(\n",
    "    adv_template, max_conversation_turns=1, max_simulation_results=3, api_call_delay_sec=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in outputs:\n",
    "    print(json.dumps(line, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store outputs in file for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = \"conv-oai-qa-harm-eval\"\n",
    "file_name = f\"\\{eval_name}.jsonl\"\n",
    "jsonl_object = outputs.to_eval_qa_json_lines()\n",
    "with Path.open(file_name, \"w\") as f:\n",
    "    f.write(jsonl_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_uri = client.tracking_uri\n",
    "connection = client.connections.get(\"Default_AzureOpenAI\")\n",
    "config = AzureOpenAIModelConfiguration.from_connection(connection, model_name=\"gpt-4\", deployment_name=\"gpt-4\")\n",
    "data_path = Path.cwd() / f\"\\{eval_name}.jsonl\"\n",
    "output_path = Path.cwd() / \"downloaded_artifacts\" / \"remote\"\n",
    "result = evaluate(\n",
    "    evaluation_name=eval_name,\n",
    "    data=data_path,\n",
    "    task_type=\"qa\",\n",
    "    metrics_list=[\n",
    "        \"hate_unfairness\",\n",
    "        \"violence\",\n",
    "        \"self_harm\",\n",
    "        \"sexual\",\n",
    "        \"gpt_groundedness\",\n",
    "        \"gpt_relevance\",\n",
    "        \"gpt_coherence\",\n",
    "        \"gpt_fluency\",\n",
    "        \"gpt_similarity\",\n",
    "    ],\n",
    "    model_config=config,\n",
    "    data_mapping={\n",
    "        \"questions\": \"question\",\n",
    "        \"answer\": \"answer\",\n",
    "    },\n",
    "    tracking_uri=tracking_uri,\n",
    "    output_path=output_path,\n",
    ")\n",
    "print(result)\n",
    "print(result.metrics_summary)  # will print the defect rate for each content harm\n",
    "print(\"Studio URL\")\n",
    "print(result.studio_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
