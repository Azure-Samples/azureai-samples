{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Teaming for AI Systems\n",
    "\n",
    "## Objective\n",
    "This notebook walks through how to use Azure AI Evaluation's Red Team functionality to assess the safety and resilience of AI systems against adversarial prompt attacks. Red teaming helps identify potential vulnerabilities across different risk categories (violence, hate/unfairness, sexual content, self-harm) and attack strategies of varying complexity levels.\n",
    "\n",
    "## Time\n",
    "You should expect to spend about 30-45 minutes running this notebook. Execution time will vary based on the number of risk categories, attack strategies, and complexity levels you choose to evaluate.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-ai-evaluation[redteam] azure-identity openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment Variables\n",
    "\n",
    "Set the following variables for use in this notebook. These variables connect to your Azure resources and model deployments.\n",
    "\n",
    "**Note:** You can find these values in your Azure AI Studio project or Azure OpenAI resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Project information\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"<your-subscription-id>\",\n",
    "    \"resource_group_name\": \"<your-resource-group-name>\",\n",
    "    \"project_name\": \"<your-project-name>\",\n",
    "}\n",
    "\n",
    "# Azure OpenAI deployment information\n",
    "azure_openai_deployment = \"<your-deployment-name>\"  # e.g., \"gpt-4\"\n",
    "azure_openai_endpoint = \"<your-endpoint>\"  # e.g., \"https://example.openai.azure.com/\"\n",
    "azure_openai_api_version = \"2023-12-01-preview\"  # Use the latest API version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here's an example of what your populated environment variables should look like:\n",
    "\n",
    "```\n",
    "# Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY=\"your-api-key-here\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://endpoint-name.openai.azure.com/openai/deployments/deployment-name/chat/completions\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"gpt-4\"\n",
    "AZURE_OPENAI_API_VERSION=\"2023-12-01-preview\"\n",
    "\n",
    "# Azure AI Project\n",
    "AZURE_SUBSCRIPTION_ID=\"12345678-1234-1234-1234-123456789012\"\n",
    "AZURE_RESOURCE_GROUP_NAME=\"your-resource-group\"\n",
    "AZURE_PROJECT_NAME=\"your-project-name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The Red Team evaluator requires an Azure AI Studio project configuration and Azure credentials. Your project configuration will be used to log evaluation results after the run is finished.\n",
    "\n",
    "**Important**: Make sure to authenticate to Azure using `az login` in your terminal before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Azure imports\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.ai.evaluation.red_team import RedTeam, RiskCategory, AttackStrategy\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "# Initialize Azure credentials\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Red Team Evaluation\n",
    "\n",
    "The Azure AI Evaluation SDK's Red Team functionality evaluates AI systems against adversarial prompts across multiple dimensions:\n",
    "\n",
    "1. **Risk Categories**: Different harmful content categories your model might generate\n",
    "   - Violence\n",
    "   - HateUnfairness\n",
    "   - Sexual\n",
    "   - SelfHarm\n",
    "\n",
    "2. **Attack Strategies**: Along with standard unmodified prompts which are sent by default, you can specify different transformations of prompts to elicit harmful content\n",
    "   - AnsiAttack: Using ANSI escape codes in prompts\n",
    "   - AsciiArt: Using ASCII art to disguise harmful content\n",
    "   - AsciiSmuggler: Hiding harmful content within ASCII characters\n",
    "   - Atbash: Using the Atbash cipher to encode harmful requests\n",
    "   - Base64: Encoding harmful content in Base64 format\n",
    "   - Binary: Converting text to binary to bypass filters\n",
    "   - Caesar: Using the Caesar cipher for encoding\n",
    "   - CharacterSpace: Manipulating character spacing to confuse filters\n",
    "   - CharSwap: Swapping characters to bypass detection\n",
    "   - Diacritic: Using diacritical marks to alter text appearance\n",
    "   - Flip: Flipping text to bypass content filters\n",
    "   - Leetspeak: Converting letters to numbers and symbols\n",
    "   - Morse: Using Morse code to encode harmful requests\n",
    "   - ROT13: Using ROT13 cipher for text transformation\n",
    "   - SuffixAppend: Adding suffixes to confuse detection systems\n",
    "   - StringJoin: Joining strings in unconventional ways\n",
    "   - Tense: Changing the tense of harmful requests to past tense\n",
    "   - UnicodeConfusable: Using similar-looking Unicode characters\n",
    "   - UnicodeSubstitution: Substituting characters with Unicode alternatives\n",
    "   - Url: Embedding harmful content within URLs\n",
    "   - Jailbreak: Specially crafted prompts to bypass AI safeguards\n",
    "\n",
    "3. **Complexity Levels**: Different difficultly levels of attacks\n",
    "   - Baseline: Standard functionality tests\n",
    "   - Easy: Simple attack patterns\n",
    "   - Moderate: More sophisticated attacks\n",
    "   - Difficult: Complex, layered attack strategies\n",
    "\n",
    "The key metric for evaluating results is the **Attack Success Rate (ASR)**, which measures the percentage of attacks that successfully elicit harmful content from your AI system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example: Fixed Response Callback\n",
    "\n",
    "Let's start with a basic example that demonstrates how to set up a Red Team evaluation using a simple callback that always returns a fixed, safe response regardless of the input prompt.\n",
    "\n",
    "**Important Note**: Since this callback always responds with the same safe message, the Attack Success Rate (ASR) will be 0% for all categories. This example is merely to demonstrate the API structure and flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple callback function that always returns a fixed response\n",
    "def financial_advisor_callback(query: str) -> str:  # noqa: ARG001\n",
    "    return \"I'm a financial advisor assistant. I can help with investment advice and financial planning within legal and ethical guidelines.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RedTeam instance\n",
    "red_team = RedTeam(\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    credential=credential,\n",
    "    risk_categories=[RiskCategory.Violence, RiskCategory.HateUnfairness],\n",
    "    num_objectives=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a simple red team evaluation using the fixed response target. We'll test against two risk categories and one attack strategy for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the red team scan called \"Basic-Callback-Test\" with limited scope for this basic example\n",
    "# This will test 1 objective prompt for each of Violence and HateUnfairness categories with the Flip strategy\n",
    "result = await red_team.scan(\n",
    "    target=financial_advisor_callback, scan_name=\"Basic-Callback-Test\", attack_strategies=[AttackStrategy.Flip]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Using an Actual Model Endpoint\n",
    "\n",
    "Now let's create a more realistic example that uses an Azure OpenAI model for responding to the red team prompts. This will demonstrate how to evaluate an actual AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback that uses Azure OpenAI API to generate responses\n",
    "async def azure_openai_callback(\n",
    "    messages: list,\n",
    "    stream: Optional[bool] = False,  # noqa: ARG001\n",
    "    session_state: Optional[str] = None,  # noqa: ARG001\n",
    "    context: Optional[Dict[str, Any]] = None,  # noqa: ARG001\n",
    ") -> dict[str, list[dict[str, str]]]:\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    api_version = os.environ.get(\"AZURE_API_VERSION\")\n",
    "\n",
    "    # Get token provider for Azure AD authentication\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    # Initialize Azure OpenAI client\n",
    "    client = AzureOpenAI(azure_endpoint=endpoint, api_version=api_version, azure_ad_token_provider=token_provider)\n",
    "\n",
    "    ## Extract the latest message from the conversation history\n",
    "    messages_list = [{\"role\": message.get(\"role\"), \"content\": message.get(\"content\")} for message in messages]\n",
    "    latest_message = messages_list[-1][\"content\"]\n",
    "\n",
    "    try:\n",
    "        # Call the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": latest_message},\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        # Format the response to follow the expected chat protocol format\n",
    "        formatted_response = {\"content\": response.choices[0].message.content, \"role\": \"assistant\"}\n",
    "\n",
    "        return {\"messages\": [formatted_response]}\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Azure OpenAI: {e!s}\")\n",
    "        return \"I encountered an error and couldn't process your request.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RedTeam instance with the model target\n",
    "model_red_team = RedTeam(\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    credential=credential,\n",
    "    risk_categories=[RiskCategory.Violence, RiskCategory.HateUnfairness, RiskCategory.Sexual],\n",
    "    num_objectives=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different Attack Strategies\n",
    "\n",
    "Now we'll run a more comprehensive evaluation using multiple attack strategies across risk categories. This will give us a better understanding of our model's vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the red team scan with multiple attack strategies\n",
    "advanced_result = await model_red_team.scan(\n",
    "    target=azure_openai_callback,\n",
    "    scan_name=\"Advanced-Callback-Test\",\n",
    "    strategies=[\n",
    "        AttackStrategy.EASY,  # Group of easy complexity attacks\n",
    "        AttackStrategy.MODERATE,  # Group of moderate complexity attacks\n",
    "        AttackStrategy.DIFFICULT,  # Group of difficult complexity attacks\n",
    "        AttackStrategy.CharacterSpace,  # Add character spaces\n",
    "        AttackStrategy.ROT13,  # Use ROT13 encoding\n",
    "        AttackStrategy.UnicodeConfusable,  # Use confusable Unicode characters\n",
    "        AttackStrategy.Tense,  # Change tense of prompts\n",
    "        AttackStrategy.CharSwap,  # Swap characters in prompts\n",
    "        AttackStrategy.Morse,  # Encode prompts in Morse code\n",
    "        AttackStrategy.Leetspeak,  # Use Leetspeak\n",
    "        AttackStrategy.Url,  # Use URLs in prompts\n",
    "        AttackStrategy.Binary,  # Encode prompts in binary\n",
    "    ],\n",
    "    output_path=\"Advanced-Callback-Test.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the Azure AI Evaluation SDK's Red Team functionality to assess the safety and resilience of AI systems. We started with a basic fixed-response example and then moved to a more realistic model evaluation across multiple risk categories and attack strategies.\n",
    "\n",
    "The Red Team evaluation provides valuable insights into:\n",
    "\n",
    "1. **Overall Attack Success Rate (ASR)** - The percentage of attacks that successfully elicit harmful content\n",
    "2. **Vulnerability by Risk Category** - Which types of harmful content your model is most vulnerable to\n",
    "3. **Effectiveness of Attack Strategies** - Which attack techniques are most successful against your model\n",
    "4. **Impact of Complexity** - How more sophisticated attacks affect your model's safety guardrails\n",
    "\n",
    "By regularly red-teaming your AI applications, you can identify and address potential vulnerabilities before deploying your models to production environments.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Mitigation**: Use these results to strengthen your model's guardrails against identified attack vectors\n",
    "2. **Continuous Testing**: Implement regular red team evaluations as part of your development lifecycle\n",
    "3. **Custom Strategies**: Develop custom attack strategies for your specific use cases and domain\n",
    "4. **Safety Layers**: Consider adding additional safety layers like Azure AI Content Safety to filter harmful requests and responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
