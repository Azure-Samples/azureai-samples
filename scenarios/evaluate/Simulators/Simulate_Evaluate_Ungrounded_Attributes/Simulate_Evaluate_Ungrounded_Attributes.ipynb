{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulating and Evaluating Ungrounded Inference of Human Attributes\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook walks you through how to generate a simulated single-turn conversation targeting a deployed AzureOpenAI model and then evaluate that test dataset for ungrounded inference of human attributes. \n",
    "\n",
    "## Time\n",
    "You should expect to spend about 30 minutes running this notebook. If you increase or decrease the amount of simulated conversation, the time will vary accordingly.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "The following simulator and evaluators require an Azure AI Foundry project configuration and an Azure credential.\n",
    "Your project configuration will be what is used to log your evaluation results in your project after the evaluation run is finished.\n",
    "\n",
    "For full region supportability, see [our documentation](https://learn.microsoft.com/azure/ai-studio/how-to/develop/flow-evaluate-sdk#built-in-evaluators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_project = {\"subscription_id\": \"\", \"resource_group_name\": \"\", \"project_name\": \"\"}\n",
    "\n",
    "\n",
    "azure_openai_endpoint = \"\"\n",
    "azure_openai_deployment = \"\"\n",
    "azure_openai_api_version = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_API_VERSION\"] = azure_openai_api_version\n",
    "os.environ[\"AZURE_ENDPOINT\"] = azure_openai_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this example\n",
    "\n",
    "To keep this notebook lightweight, let's create a dummy application that calls an Azure OpenAI model, such as GPT-4 to generate responses. When testing your application for ungrounded attributes, it's important to have a way to simulate query, response and context as conversation to check if it contains information about protected class or emotional state of a person. We will use the `Simulator` class and this is how we will generate responses using your application. Once we have this dataset, we can evaluate it with our `UngroundedAttributesEvaluator` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import UngroundedAttributesEvaluator\n",
    "from azure.ai.evaluation.simulator import AdversarialSimulator, AdversarialScenario\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "async def ungrounded_att_completion_callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Optional[str] = None,\n",
    ") -> dict:\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    # Get a client handle for the model\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "\n",
    "    generated_text = messages[\"messages\"][0][\"content\"]\n",
    "\n",
    "    # Extract conversation from the generated text\n",
    "    conversation = messages.get(\"template_parameters\", {}).get(\"conversation\", \"\")\n",
    "\n",
    "    # Call the model\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": generated_text,\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "        )\n",
    "        formatted_response = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    except Exception:\n",
    "        formatted_response = {\n",
    "            \"content\": \"I don't know\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"context\": {\"key\": {}},\n",
    "        }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": conversation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Ungrounded Attributes\n",
    "\n",
    "When building your application, you want to test if your Gen AI Application generates information about protected class or emotional state of a person based the provided context of a multi-turn conversation. This tests whether or not your Gen AI application will infer ungrounded personal attributes about a person given a transcript or chat history. The following example uses an `AdversarialSimulator` paired with an ungrounded attributes scenario, to prompt your model with conversations that may or may not contain protected class or emotional state of the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "ungrounded_att_scenario = AdversarialScenario.ADVERSARIAL_UNGROUNDED_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator below generates datasets that represent queries along with conversation as context and combines it with response generated by LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    scenario=ungrounded_att_scenario,\n",
    "    max_conversation_turns=1,\n",
    "    max_simulation_results=1,\n",
    "    target=ungrounded_att_completion_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.ai.evaluation.simulator._utils import JsonLineChatProtocol\n",
    "from pathlib import Path\n",
    "\n",
    "with Path(\"adv_ungrounded_att_eval.jsonl\").open(\"w\") as file:\n",
    "    file.write(JsonLineChatProtocol(outputs[0]).to_eval_qr_json_lines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it for ungrounded attributes. The `UngroundedAttributesEvaluator` class can take in the dataset and detect whether inference for human attributes exists. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Foundry Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_eval = UngroundedAttributesEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"adv_ungrounded_att_eval.jsonl\",\n",
    "    evaluators={\"ungrounded_attributes\": ua_eval},\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
