{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulate conversations from conversation starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Use the Simulator to generate high-quality simulated data with pre defined conversation starters using LLMs.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Access to Azure OpenAI Service - you can apply for access [here](https://go.microsoft.com/fwlink/?linkid=2222006)\n",
    "- An Azure AI Studio project - go to [aka.ms/azureaistudio](https://aka.ms/azureaistudio) to create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "Large Language Models (LLMs) can help you create query and response datasets from your existing data sources. These datasets can be useful for various tasks, such as testing your retrieval capabilities, evaluating and improving your RAG workflows, tuning your prompts and more. In this sample, we will explore how to use the Simulator to generate high-quality simulated data with pre defined conversation starters using LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-identity azure-ai-evaluation\n",
    "%pip install wikipedia openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details. For `subscription_id`, `resource_group_name` and `project_name`, you can go to the Project Overview page in the AI Studio. Replace the items in <> with values for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the following code to set the variables with your values\n",
    "\n",
    "azure_openai_api_version = \"<your-api-version>\"\n",
    "azure_openai_deployment = \"<your-deployment>\"\n",
    "azure_openai_endpoint = \"<your-endpoint>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us connect to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting the simulator to your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Query Responses with conversation starter\n",
    "Here's how we would define a conversation starter. The code below shows you how to define 2 simulations. Each of the simulation has 3 pre-defined turns to start the conversation. Customize these turns (number of turns and/or the content of them) to make them relevant to your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_turns = [\n",
    "    # simulation 1\n",
    "    [\n",
    "        \"Hello, how are you?\",  # simulator conversation starter (turn 1)\n",
    "        \"I want to learn more about Paris\",  # conversation turn 2,\n",
    "        \"Thanks for helping me. What else should I know about Paris for my project\",  # conversation turn 3\n",
    "    ],\n",
    "    # simulation 2\n",
    "    [\n",
    "        \"Hey, I really need your help to finish my homework.\",\n",
    "        \"I need to write an essay about Paris\",\n",
    "        \"Thanks, can you rephrase your last response to make me sound smarter?\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to simulator\n",
    "The call to the simulator here descibes the basic usage of having a conversation with your application using the conversation starters defined above.\n",
    "The conversation starts with the first turn of the simulation as the user's turn being passed to the `callback` method. These steps are continued up to `max_conversation_turns` turns. If the number of turns specified in the conversation starter is less than the `max_conversation_turns`, the simulator simulates user's messages with utilizing the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    conversation_turns=conversation_turns,\n",
    "    max_conversation_turns=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "output_file = Path(\"output.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
