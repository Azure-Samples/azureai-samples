{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulate Queries and Responses from input text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Use the Simulator to generate high-quality queries and responses from your data using LLMs.\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- Access to Azure OpenAI Service - you can apply for access [here](https://go.microsoft.com/fwlink/?linkid=2222006)\n",
    "- An Azure AI Studio project - go to [aka.ms/azureaistudio](https://aka.ms/azureaistudio) to create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "\n",
    "Large Language Models (LLMs) can help you create query and response datasets from your existing data sources such as text or index. These datasets can be useful for various tasks, such as testing your retrieval capabilities, evaluating and improving your RAG workflows, tuning your prompts and more. In this sample, we will explore how to use the Simulator to generate high-quality query and response pairs from your data using LLMs and simulate interactions with your application with them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this sample we will generate text data from Wikipedia. You can follow the same steps replacing the text with any other source documents of your application's interest. Make sure that the length of the text is within the selected Azure AI model's context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install azure-identity azure-ai-evaluation\n",
    "%pip install promptflow-azure\n",
    "%pip install wikipedia openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters\n",
    "\n",
    "Lets initialize some variables. We need a way to connect to a LLM to use the notebook. This sample suggests a way to use `gpt-4o-mini` deployment in your Azure AI project. Replace the `azure_openai_endpoint` with a link to your endpoint. If your applications calls `AzureOpenAI`'s chat completion endpoint, you will need to replace the values in `<>` with your `AzureOpenAI` deployment details. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# project details\n",
    "azure_openai_api_version = \"<your-api-version>\"\n",
    "azure_openai_endpoint = \"<your-endpoint>\"\n",
    "azure_openai_deployment = \"gpt-4o-mini\"  # replace with your deployment name, if different\n",
    "\n",
    "# Optionally set the azure_ai_project to upload the evaluation results to Azure AI Studio.\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"<your-subscription-id>\",\n",
    "    \"resource_group\": \"<your-resource-group>\",\n",
    "    \"workspace_name\": \"<your-workspace-name>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": azure_openai_endpoint,\n",
    "    \"azure_deployment\": azure_openai_deployment,\n",
    "    \"api_version\": azure_openai_api_version,\n",
    "}\n",
    "\n",
    "# JSON mode supported model preferred to avoid errors ex. gpt-4o-mini, gpt-4o, gpt-4 (1106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us connect to the project. DefaultAzureCredentails will be picked up by the SDK which runs the prompty files to authenticate your requests. If you want to use your AzureOpenAI key to authenticate, you can do so by setting the `api_key` in your `model_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the simulator to your application\n",
    "This part assumes that you application is a call to `AzureOpenAI`'s chat completion endpoint. Feel free to change this method to call your application with its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source for the simulator\n",
    "In this example we use a wikipedia article as raw text generate Query Response pairs. Alternatively, text from an Azure Search index can be used as a data source for the simulator to generate Query Response pairs. An example of this behavior can be seen in [simulate_input_index sample](..\\simulate_input_index\\simulate_input_index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wiki_search_term = \"Leonardo da vinci\"\n",
    "wiki_title = wikipedia.search(wiki_search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to simulator\n",
    "This call to the simulator generates 4 query response pairs in its first pass.\n",
    "In the second pass, it picks up one task, pairs it with a query (generated in previous pass) and sends it to the configured llm to build the first user turn. This user turn is then passed to the `callback` method. The conversation continutes till the `max_conversation_turns` turns.\n",
    "\n",
    "The output of the simulator will have the original task, original query, the original query and the response generated from the first turn as expected response. You can find them in the `context` key of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=text,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=3,\n",
    "    tasks=[\n",
    "        f\"I am a student and I want to learn more about {wiki_search_term}\",\n",
    "        f\"I am a teacher and I want to teach my students about {wiki_search_term}\",\n",
    "        f\"I am a researcher and I want to do a detailed research on {wiki_search_term}\",\n",
    "        f\"I am a statistician and I want to do a detailed table of factual data concerning {wiki_search_term}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the user simulating behavior\n",
    "Internally, the SDK has a `prompty` file that defines how the LLM which simulates the user should behave. Our SDK also offers an option for users to override the file, to support your own prompty files. Here's a brief overview of how to accomplish overriding the user behavior.\n",
    "\n",
    "Make sure you have `user_override.prompty` file in the same directory. The file in this repo takes an additional argument called mood. This is to show how you can add any additional keyword arguments to your prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = Path.cwd()\n",
    "user_override_prompty = Path(current_directory) / \"user_override.prompty\"\n",
    "user_prompty_kwargs = {\"mood\": \"happy\"}\n",
    "\n",
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=text,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=1,\n",
    "    tasks=[\n",
    "        f\"I am a student and I want to learn more about {wiki_search_term}\",\n",
    "        f\"I am a teacher and I want to teach my students about {wiki_search_term}\",\n",
    "        f\"I am a researcher and I want to do a detailed research on {wiki_search_term}\",\n",
    "        f\"I am a statistician and I want to do a detailed table of factual data concerning {wiki_search_term}\",\n",
    "    ],\n",
    "    user_simulator_prompty=user_override_prompty,\n",
    "    user_simulator_prompty_options=user_prompty_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_file = Path(\"output.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluations on the simulated data\n",
    "Here we will try to run GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator on the output data from the simulator.\n",
    "\n",
    "From the documentation we know that running those evaluators need the following data: `query`, `response`, `context`, `ground_truth`\n",
    "\n",
    "For simplicity's sake, we can use our source document `text` as both `context` and `ground_truth`. This step only evaluates the first user message and first response from your AI Application for each of the simulated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_json_lines = \"\"\n",
    "for output in outputs:\n",
    "    query = None\n",
    "    response = None\n",
    "    context = text\n",
    "    ground_truth = text\n",
    "    for message in output[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            query = message[\"content\"]\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            response = message[\"content\"]\n",
    "    if query and response:\n",
    "        eval_input_data_json_lines += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"response\": response,\n",
    "                    \"context\": context,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_file = Path(\"eval_input_data.jsonl\")\n",
    "with eval_input_data_file.open(\"w\") as f:\n",
    "    f.write(eval_input_data_json_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run evaluation\n",
    "`QAEvaluator` is a composite evaluator which runs GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "Optionally set the azure_ai_project to upload the evaluation results to Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=model_config)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(eval_input_data_file),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # optional to store the evaluation results in Azure AI Studio\n",
    "    output_path=\"./myevalresults.json\",  # optional to store the evaluation results in a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
