{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AI agents (Azure AI Agent Service) in Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "\n",
    "This sample demonstrates how to evaluate an AI agent (Azure AI Agent Service) on these important aspects of your agentic workflow:\n",
    "\n",
    "- Intent Resolution: Measures how well the agent identifies the user’s request, including how well it scopes the user’s intent, asks clarifying questions, and reminds end users of its scope of capabilities.\n",
    "- Tool Call Accuracy: Evaluates the agent's ability to select the appropriate tools, and process correct parameters from previous steps.\n",
    "- Task Adherence: Measures how well the agent’s response adheres to its assigned tasks, according to its system message and prior steps.\n",
    "\n",
    "For AI agents outside of Azure AI Agent Service, you can still provide th agent data in the two formats (either simple data or agent messages) specified in the individual evaluator samples:\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample)\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample)\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample)\n",
    "- [Response Completeness](https://aka.ms/rescompleteness-sample)\n",
    "\n",
    "\n",
    "\n",
    "## Time \n",
    "\n",
    "You should expect to spend about 20 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "Creating an agent using Azure AI agent service requires an Azure AI Foundry project and a deployed, supported model. See more details in [Create a new agent](https://learn.microsoft.com/azure/ai-services/agents/quickstart?pivots=ai-foundry-portal).\n",
    "\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "Important: Make sure to authenticate to Azure using `az login` in your terminal before running this notebook.\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "This notebook will evaluate an agent created in a legacy Azure AI Foundry project. If you have a Foundry Development Platform project, you can use a notebook for [FDP agents](https://aka.ms/e2e-agent-eval-sample).\n",
    "\n",
    "Before running the sample, you must install a compatible version of the azure-ai-projects SDK:\n",
    "```bash\n",
    "pip install azure-ai-projects<=1.0.0b10 azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for AI-assisted evaluators, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n",
    "9) **AGENT_MODEL_DEPLOYMENT_NAME** - The deployment name of the model for your Azure AI agent, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "\n",
    "# Import your custom functions to be used as Tools for the Agent\n",
    "from user_functions import user_functions\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "AGENT_NAME = \"Seattle Tourist Assistant\"\n",
    "\n",
    "# Add Tools to be used by Agent\n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "# To enable tool calls executed automatically\n",
    "project_client.agents.enable_auto_function_calls(toolset=toolset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an AI agent (Azure AI Agent Service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_uxQS769fwqimVkoV3Ae5wuQt\n"
     ]
    }
   ],
   "source": [
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"AGENT_MODEL_DEPLOYMENT_NAME\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created thread, ID: thread_68j9mUaZSWsdNcDit24XCI6W\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation with Agent\n",
    "Use below cells to have conversation with the agent\n",
    "- `Create Message[1]`\n",
    "- `Execute[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Message[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created message, ID: msg_hbz8dCWnBmLgvFYJSPl0eFig\n"
     ]
    }
   ],
   "source": [
    "# Create message to thread\n",
    "\n",
    "MESSAGE = \"Can you email me weather info for Seattle ?\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending email to you@example.com...\n",
      "Subject: Weather Info for Seattle\n",
      "Body:\n",
      "The current weather in Seattle is rainy with a temperature of 14°C.\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Run ID: run_pwXvPMuuaktwMKqGP8nIjwCN\n"
     ]
    }
   ],
   "source": [
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: MessageRole.USER\n",
      "Content: Can you email me weather info for Seattle ?\n",
      "----------------------------------------\n",
      "Role: MessageRole.AGENT\n",
      "Content: I have sent you an email with the current weather information for Seattle. The weather is rainy with a temperature of 14°C.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in project_client.agents.list_messages(thread.id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.ai.evaluation._common._experimental:Class AIAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class LegacyAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class AIAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = converter.convert(thread_id=thread_id, run_id=run_id)\n",
    "\n",
    "# Run this to save thread data to a JSONL file for evaluation\n",
    "# Save the agent thread data to a JSONL file\n",
    "# evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=<>)\n",
    "# print(json.dumps(evaluation_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "We will select the following evaluators to assess the different aspects relevant for agent quality: \n",
    "\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample): measures the extent of which an agent identifies the correct intent from a user query. Scale: integer 1-5. Higher is better.\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample): evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps. Scale: float 0-1. Higher is better.\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample): measures the extent of which an agent’s final response adheres to the task based on its system message and a user query. Scale: integer 1-5. Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.ai.evaluation._common._experimental:Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "    \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "}\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-14 09:45:44 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-14 09:45:44 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-14 09:45:44 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-14 09:45:44 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_task_adherence_20250514_094543_478144, log path: /Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250514_094543_478144/logs.txt\n",
      "[2025-05-14 09:45:44 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_intent_resolution_20250514_094543_477868, log path: /Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_intent_resolution_20250514_094543_477868/logs.txt\n",
      "[2025-05-14 09:45:44 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182, log path: /Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182/logs.txt\n",
      "[2025-05-14 09:45:46 -0700][promptflow._sdk._orchestrator.run_submitter][WARNING] - 1 out of 5 runs failed in batch run.\n",
      " Please check out /Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182 for more details.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/promptflow/_sdk/operations/_local_storage_operations.py:516: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '(Failed)' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  outputs.fillna(value=\"(Failed)\", inplace=True)  # replace nan with explicit prompt\n",
      "/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_batch_run/proxy_client.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.replace(\"(Failed)\", math.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:45:45 -0700   40510 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-14 09:45:45 -0700   40510 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-14 09:45:45 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 0.02 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-14 09:45:45 -0700   40510 execution          ERROR    1/5 flow run failed, indexes: [2], exception of index 2: (UserError) response does not have tool calls. Either provide tool_calls or response with tool calls.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-14 09:45:43.505344-07:00\"\n",
      "Duration: \"0:00:02.673605\"\n",
      "Output path: \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182\"\n",
      "\n",
      "2025-05-14 09:45:48 -0700   40510 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-14 09:45:48 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 3.69 seconds. Estimated time for incomplete lines: 14.76 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 3.92 seconds. Estimated time for incomplete lines: 15.68 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 2.05 seconds. Estimated time for incomplete lines: 6.15 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.36 seconds. Estimated time for incomplete lines: 2.72 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 1.03 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.39 seconds. Estimated time for incomplete lines: 2.78 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 1.06 seconds.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-14 09:45:45 -0700   40510 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 3.92 seconds. Estimated time for incomplete lines: 15.68 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 2.05 seconds. Estimated time for incomplete lines: 6.15 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.39 seconds. Estimated time for incomplete lines: 2.78 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 1.06 seconds.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_task_adherence_20250514_094543_478144\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-14 09:45:43.505134-07:00\"\n",
      "Duration: \"0:00:07.661312\"\n",
      "Output path: \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250514_094543_478144\"\n",
      "\n",
      "2025-05-14 09:45:45 -0700   40510 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-14 09:45:48 -0700   40510 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-14 09:45:48 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 3.69 seconds. Estimated time for incomplete lines: 14.76 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.36 seconds. Estimated time for incomplete lines: 2.72 seconds.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-14 09:45:49 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 1.03 seconds.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-14 09:45:50 -0700   40510 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_intent_resolution_20250514_094543_477868\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-14 09:45:43.505519-07:00\"\n",
      "Duration: \"0:00:07.671026\"\n",
      "Output path: \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_intent_resolution_20250514_094543_477868\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"tool_call_accuracy\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:02.673605\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 1,\n",
      "        \"log_path\": \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_tool_call_accuracy_20250514_094543_477182\"\n",
      "    },\n",
      "    \"intent_resolution\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.671026\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_intent_resolution_20250514_094543_477868\"\n",
      "    },\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.661312\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/Users/steven/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250514_094543_478144\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Failed to upload evaluation run to the cloud due to insufficient permission to access the storage. Please ensure that the necessary access rights are granted.\nVisit https://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot to troubleshoot this issue.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHttpResponseError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_eval_run.py:434\u001B[0m, in \u001B[0;36mEvalRun.log_artifact\u001B[0;34m(self, artifact_folder, artifact_name)\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[0;32m--> 434\u001B[0m             blob_client\u001B[38;5;241m.\u001B[39mupload_blob(fp, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HttpResponseError \u001B[38;5;28;01mas\u001B[39;00m ex:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/core/tracing/decorator.py:116\u001B[0m, in \u001B[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m     span\u001B[38;5;241m.\u001B[39madd_attribute(key, value)\n\u001B[0;32m--> 116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/storage/blob/_blob_client.py:596\u001B[0m, in \u001B[0;36mBlobClient.upload_blob\u001B[0;34m(self, data, blob_type, length, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m blob_type \u001B[38;5;241m==\u001B[39m BlobType\u001B[38;5;241m.\u001B[39mBlockBlob:\n\u001B[0;32m--> 596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m upload_block_blob(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    597\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m blob_type \u001B[38;5;241m==\u001B[39m BlobType\u001B[38;5;241m.\u001B[39mPageBlob:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/storage/blob/_upload_helpers.py:197\u001B[0m, in \u001B[0;36mupload_block_blob\u001B[0;34m(client, stream, overwrite, encryption_options, blob_settings, headers, validate_content, max_concurrency, length, **kwargs)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m     process_storage_error(error)\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ResourceModifiedError \u001B[38;5;28;01mas\u001B[39;00m mod_error:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/storage/blob/_shared/response_handlers.py:186\u001B[0m, in \u001B[0;36mprocess_storage_error\u001B[0;34m(storage_error)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001B[39;00m\n\u001B[0;32m--> 186\u001B[0m     exec(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise error from None\u001B[39m\u001B[38;5;124m\"\u001B[39m)   \u001B[38;5;66;03m# pylint: disable=exec-used # nosec\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mSyntaxError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m<string>:1\u001B[0m\n",
      "\u001B[0;31mHttpResponseError\u001B[0m: Key based authentication is not permitted on this storage account.\nRequestId:97b71a60-001e-0034-6def-c48a79000000\nTime:2025-05-14T16:45:56.9675583Z\nErrorCode:KeyBasedAuthenticationNotPermitted\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>KeyBasedAuthenticationNotPermitted</Code><Message>Key based authentication is not permitted on this storage account.\nRequestId:97b71a60-001e-0034-6def-c48a79000000\nTime:2025-05-14T16:45:56.9675583Z</Message></Error>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mEvaluationException\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluate\n\u001B[0;32m----> 3\u001B[0m response \u001B[38;5;241m=\u001B[39m evaluate(\n\u001B[1;32m      4\u001B[0m     data\u001B[38;5;241m=\u001B[39mfile_name,\n\u001B[1;32m      5\u001B[0m     evaluators\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_call_accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_call_accuracy,\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mintent_resolution\u001B[39m\u001B[38;5;124m\"\u001B[39m: intent_resolution,\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask_adherence\u001B[39m\u001B[38;5;124m\"\u001B[39m: task_adherence,\n\u001B[1;32m      9\u001B[0m     },\n\u001B[1;32m     10\u001B[0m     azure_ai_project\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubscription_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAZURE_SUBSCRIPTION_ID\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproject_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPROJECT_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresource_group_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRESOURCE_GROUP_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     14\u001B[0m     },\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m pprint(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAI Foundary URL: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstudio_url\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:797\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001B[0m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, EvaluationException):\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m EvaluationException(\n\u001B[1;32m    791\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    792\u001B[0m         target\u001B[38;5;241m=\u001B[39mErrorTarget\u001B[38;5;241m.\u001B[39mEVALUATE,\n\u001B[1;32m    793\u001B[0m         category\u001B[38;5;241m=\u001B[39mErrorCategory\u001B[38;5;241m.\u001B[39mFAILED_EXECUTION,\n\u001B[1;32m    794\u001B[0m         blame\u001B[38;5;241m=\u001B[39mErrorBlame\u001B[38;5;241m.\u001B[39mSYSTEM_ERROR,\n\u001B[1;32m    795\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m--> 797\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:755\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001B[39;00m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;124;03m    data will be run through target function and then results will be evaluated.\u001B[39;00m\n\u001B[1;32m    705\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    752\u001B[0m \u001B[38;5;124;03m            https://{resource_name}.services.ai.azure.com/api/projects/{project_name}\u001B[39;00m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _evaluate(\n\u001B[1;32m    756\u001B[0m         evaluation_name\u001B[38;5;241m=\u001B[39mevaluation_name,\n\u001B[1;32m    757\u001B[0m         target\u001B[38;5;241m=\u001B[39mtarget,\n\u001B[1;32m    758\u001B[0m         data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m    759\u001B[0m         evaluators_and_graders\u001B[38;5;241m=\u001B[39mevaluators,\n\u001B[1;32m    760\u001B[0m         evaluator_config\u001B[38;5;241m=\u001B[39mevaluator_config,\n\u001B[1;32m    761\u001B[0m         azure_ai_project\u001B[38;5;241m=\u001B[39mazure_ai_project,\n\u001B[1;32m    762\u001B[0m         output_path\u001B[38;5;241m=\u001B[39moutput_path,\n\u001B[1;32m    763\u001B[0m         fail_on_evaluator_errors\u001B[38;5;241m=\u001B[39mfail_on_evaluator_errors,\n\u001B[1;32m    764\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    765\u001B[0m     )\n\u001B[1;32m    766\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    767\u001B[0m     \u001B[38;5;66;03m# Handle multiprocess bootstrap error\u001B[39;00m\n\u001B[1;32m    768\u001B[0m     bootstrap_error \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    769\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn attempt has been made to start a new process before the\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    770\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent process has finished its bootstrapping phase.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    771\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:937\u001B[0m, in \u001B[0;36m_evaluate\u001B[0;34m(evaluators_and_graders, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001B[0m\n\u001B[1;32m    935\u001B[0m     studio_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trace_destination:\n\u001B[0;32m--> 937\u001B[0m         studio_url \u001B[38;5;241m=\u001B[39m _log_metrics_and_instance_results(\n\u001B[1;32m    938\u001B[0m             metrics, results_df, trace_destination, \u001B[38;5;28;01mNone\u001B[39;00m, evaluation_name, name_map, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    939\u001B[0m         )\n\u001B[1;32m    941\u001B[0m result_df_dict \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39mto_dict(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    942\u001B[0m result: EvaluationResult \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrows\u001B[39m\u001B[38;5;124m\"\u001B[39m: result_df_dict, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m\"\u001B[39m: metrics, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstudio_url\u001B[39m\u001B[38;5;124m\"\u001B[39m: studio_url}  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_utils.py:261\u001B[0m, in \u001B[0;36m_log_metrics_and_instance_results\u001B[0;34m(metrics, instance_results, trace_destination, run, evaluation_name, name_map, **kwargs)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(tmp_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39mDefaultOpenEncoding\u001B[38;5;241m.\u001B[39mWRITE) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    259\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(instance_results\u001B[38;5;241m.\u001B[39mto_json(orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m--> 261\u001B[0m ev_run\u001B[38;5;241m.\u001B[39mlog_artifact(tmpdir, artifact_name)\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# Using mlflow to create a dummy run since once created via PF show traces of dummy run in UI.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# Those traces can be confusing.\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# adding these properties to avoid showing traces if a dummy run is created.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# We are doing that only for the pure evaluation runs.\u001B[39;00m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/azure/ai/evaluation/_evaluate/_eval_run.py:441\u001B[0m, in \u001B[0;36mEvalRun.log_artifact\u001B[0;34m(self, artifact_folder, artifact_name)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ex\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[1;32m    437\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    438\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to upload evaluation run to the cloud due to insufficient permission to access the storage.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    439\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please ensure that the necessary access rights are granted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    440\u001B[0m         )\n\u001B[0;32m--> 441\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m EvaluationException(\n\u001B[1;32m    442\u001B[0m             message\u001B[38;5;241m=\u001B[39mmsg,\n\u001B[1;32m    443\u001B[0m             target\u001B[38;5;241m=\u001B[39mErrorTarget\u001B[38;5;241m.\u001B[39mEVAL_RUN,\n\u001B[1;32m    444\u001B[0m             category\u001B[38;5;241m=\u001B[39mErrorCategory\u001B[38;5;241m.\u001B[39mFAILED_REMOTE_TRACKING,\n\u001B[1;32m    445\u001B[0m             blame\u001B[38;5;241m=\u001B[39mErrorBlame\u001B[38;5;241m.\u001B[39mUSER_ERROR,\n\u001B[1;32m    446\u001B[0m             tsg_link\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    447\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mex\u001B[39;00m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ex\n\u001B[1;32m    451\u001B[0m \u001B[38;5;66;03m# To show artifact in UI we will need to register it. If it is a promptflow run,\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;66;03m# we are rewriting already registered artifact and need to skip this step.\u001B[39;00m\n",
      "\u001B[0;31mEvaluationException\u001B[0m: (UserError) Failed to upload evaluation run to the cloud due to insufficient permission to access the storage. Please ensure that the necessary access rights are granted.\nVisit https://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot to troubleshoot this issue."
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect results on Azure AI Foundry\n",
    "\n",
    "Go to AI Foundry URL for rich Azure AI Foundry data visualization to inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can use the following to get the evaluation results in memory\n",
    "\n",
    "# average scores across all runs\n",
    "pprint(response[\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
