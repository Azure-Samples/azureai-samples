{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Adherence Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates to how to use task adherence evaluator on agent data. The supported input formats include:\n",
    "- simple data such as strings;\n",
    "- user-agent conversations in the form of list of agent messages. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 10 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend using `gpt-4o` or `gpt-4.1`.  \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity openai\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **AZURE_AI_PROJECT_ENDPOINT** - Your Azure AI project endpoint in format: `https://<account_name>.services.ai.azure.com/api/projects/<project_name>`\n",
    "2) **AZURE_AI_MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator (e.g., gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task Adherence evaluator measures how well the agent adheres to their assigned tasks or predefined goal.\n",
    "\n",
    "The scoring is on a 1-5 integer scale and is as follows:\n",
    "\n",
    "  - Score 1: Fully Inadherent\n",
    "  - Score 2: Barely Adherent\n",
    "  - Score 3: Moderately Adherent\n",
    "  - Score 4: Mostly Adherent\n",
    "  - Score 5: Fully Adherent\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "  - Query    : The user query. Either a string with a user request or a list of messages with previous requests from the user and responses from the assistant, potentially including a system message.\n",
    "  - Response : The response to be evaluated. Either a string or a message with the response from the agent to the last user query.\n",
    "\n",
    "There is a third optional parameter:\n",
    "  - ToolDefinitions : The list of tool definitions the agent can call. This may be useful for the evaluator to better assess if the right tool was called to adhere to user intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Task Adherence Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai.types.evals.create_eval_jsonl_run_data_source_param import SourceFileContentContent\n",
    "from pprint import pprint\n",
    "from agent_utils import run_evaluator\n",
    "\n",
    "# Get environment variables\n",
    "deployment_name = os.environ[\"AZURE_AI_MODEL_DEPLOYMENT_NAME\"]\n",
    "\n",
    "# Data source configuration (defines the schema for evaluation inputs)\n",
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"anyOf\": [\n",
    "                    {\"type\": \"string\"},\n",
    "                    {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"anyOf\": [\n",
    "                    {\"type\": \"string\"},\n",
    "                    {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"tool_definitions\": {\n",
    "                \"anyOf\": [\n",
    "                    {\"type\": \"object\"},\n",
    "                    {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\", \"response\"]\n",
    "    },\n",
    "    \"include_sample_schema\": True\n",
    "}\n",
    "\n",
    "# Data mapping (maps evaluation inputs to evaluator parameters)\n",
    "data_mapping = {\n",
    "    \"query\": \"{{item.query}}\",\n",
    "    \"response\": \"{{item.response}}\",\n",
    "    \"tool_definitions\": \"{{item.tool_definitions}}\"\n",
    "}\n",
    "\n",
    "# Initialization parameters for the evaluator\n",
    "initialization_parameters = {\n",
    "    \"deployment_name\": deployment_name\n",
    "}\n",
    "\n",
    "# Initialize the evaluation_contents list - we'll append all test cases here\n",
    "evaluation_contents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: Vague adherence example (should score low)\n",
    "query1 = \"What are the best practices for maintaining a healthy rose garden during the summer?\"\n",
    "response1 = \"Make sure to water your roses regularly and trim them occasionally.\"\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query1,\n",
    "            \"response\": response1,\n",
    "            \"tool_definitions\": None,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Full adherence example (should score high)\n",
    "query2 = \"What are the best practices for maintaining a healthy rose garden during the summer?\"\n",
    "response2 = \"For optimal summer care of your rose garden, start by watering deeply early in the morning to ensure the roots are well-hydrated without encouraging fungal growth. Apply a 2-3 inch layer of organic mulch around the base of the plants to conserve moisture and regulate soil temperature. Fertilize with a balanced rose fertilizer every 4 to 6 weeks to support healthy growth. Prune away any dead or diseased wood to promote good air circulation, and inspect regularly for pests such as aphids or spider mites, treating them promptly with an appropriate organic insecticidal soap. Finally, ensure that your roses receive at least 6 hours of direct sunlight daily for robust flowering.\"\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query2,\n",
    "            \"response\": response2,\n",
    "            \"tool_definitions\": None,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query and Response as String with Tool Definition as Single Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3: Book recommendation with tool definition as single dict\n",
    "query3 = \"Can you recommend a science fiction book for me?\"\n",
    "response3 = \"I found 'Dune' by Frank Herbert for you. It's a classic science fiction novel set in a distant future.\"\n",
    "\n",
    "tool_definition_dict = [\n",
    "    {\n",
    "        \"name\": \"get_book\",\n",
    "        \"description\": \"Retrieve a book recommendation for a specified genre.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"genre\": {\"type\": \"string\", \"description\": \"The genre for which a book recommendation is requested.\"}\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query3,\n",
    "            \"response\": response3,\n",
    "            \"tool_definitions\": tool_definition_dict\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 4: Historical fiction recommendation with conversation messages and tool usage\n",
    "query4 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in literature and at provid can provide book recommendations.\"},\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:00Z\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"I love historical fiction. Can you recommend a good book from that genre?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response4 = [\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:05Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Let me fetch a recommendation for historical fiction.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:10Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "                \"name\": \"get_book\",\n",
    "                \"arguments\": {\"genre\": \"historical fiction\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:15Z\",\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_result\": '{ \"book\": { \"title\": \"The Pillars of the Earth\", \"author\": \"Ken Follett\", \"summary\": \"A captivating tale set in medieval England that weaves historical events with personal drama.\" } }',\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:20Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Based on our records, I recommend 'The Pillars of the Earth' by Ken Follett. This novel is an excellent example of historical fiction with a rich narrative and well-developed characters. Would you like more details or another suggestion?\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_definitions4 = [\n",
    "    {\n",
    "        \"name\": \"get_book\",\n",
    "        \"description\": \"Retrieve a book recommendation for a specified genre.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"genre\": {\"type\": \"string\", \"description\": \"The genre for which a book recommendation is requested.\"}\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query4,\n",
    "            \"response\": response4,\n",
    "            \"tool_definitions\": tool_definitions4\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation on All Test Cases\n",
    "\n",
    "Now that we've defined all test cases, let's run the evaluation once on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_evaluator(\n",
    "    evaluator_name=\"task_adherence\",\n",
    "    evaluation_contents=evaluation_contents,\n",
    "    data_source_config=data_source_config,\n",
    "    initialization_parameters=initialization_parameters,\n",
    "    data_mapping=data_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "View the evaluation results for each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
