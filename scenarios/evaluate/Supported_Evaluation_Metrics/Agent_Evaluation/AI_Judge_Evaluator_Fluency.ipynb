{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21599c07",
   "metadata": {},
   "source": [
    "# Fluency Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates how to use the Fluency evaluator to assess the linguistic quality of AI-generated responses. The evaluator measures how well generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage.\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 20 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode.  We recommend using `gpt-4o` or `gpt-4.1`.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity openai\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **AZURE_AI_PROJECT_ENDPOINT** - Your Azure AI project endpoint in format: `https://<account_name>.services.ai.azure.com/api/projects/<project_name>`\n",
    "2) **AZURE_AI_MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator (e.g., gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add5ec1",
   "metadata": {},
   "source": [
    "The Fluency evaluator assesses the extent to which generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage, resulting in linguistically correct responses.\n",
    "\n",
    "Fluency scores range from 1 to 5:\n",
    "\n",
    "<pre>\n",
    "Score 1: Very Poor - The response is incomprehensible with severe grammatical errors and improper vocabulary.\n",
    "Score 2: Poor - The response has frequent grammatical errors and awkward phrasing that hinder understanding.\n",
    "Score 3: Fair - The response is understandable but contains noticeable grammatical errors or awkward expressions.\n",
    "Score 4: Good - The response is mostly fluent with minor grammatical issues that don't significantly impact readability.\n",
    "Score 5: Excellent - The response is perfectly fluent with proper grammar, syntax, and vocabulary usage.\n",
    "</pre>\n",
    "\n",
    "The evaluation requires the following input format:\n",
    "\n",
    "**Query-Response Evaluation**\n",
    "- Response: The AI's response to be evaluated for fluency (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd4d05",
   "metadata": {},
   "source": [
    "### Initialize Fluency Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd097f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai.types.evals.create_eval_jsonl_run_data_source_param import SourceFileContentContent\n",
    "from pprint import pprint\n",
    "from agent_utils import run_evaluator\n",
    "\n",
    "# Get environment variables\n",
    "deployment_name = os.environ[\"AZURE_AI_MODEL_DEPLOYMENT_NAME\"]\n",
    "\n",
    "# Data source configuration (defines the schema for evaluation inputs)\n",
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"query\": {\"type\": \"string\"}, \"response\": {\"type\": \"string\"}},\n",
    "        \"required\": [],\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "}\n",
    "\n",
    "# Data mapping (maps evaluation inputs to evaluator parameters)\n",
    "data_mapping = {\n",
    "    \"query\": \"{{item.query}}\",\n",
    "    \"response\": \"{{item.response}}\"\n",
    "}\n",
    "\n",
    "# Initialization parameters for the evaluator\n",
    "initialization_parameters = {\n",
    "    \"deployment_name\": deployment_name\n",
    "}\n",
    "\n",
    "# Initialize the evaluation_contents list - we'll append all test cases here\n",
    "evaluation_contents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793759a",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc8414",
   "metadata": {},
   "source": [
    "#### Response as String (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2df39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"The weather in Seattle is currently partly cloudy with a temperature of 15Â°C. The forecast indicates that conditions will remain stable throughout the day, with a gentle breeze from the northwest.\"\n",
    "\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": None,\n",
    "            \"response\": response\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59709c65",
   "metadata": {},
   "source": [
    "#### Example of Poor Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c624ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poor fluency example\n",
    "response = \"Email draft attach is. You review and giving feedback must. Important very for project success it being.\"\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": None,\n",
    "            \"response\": response\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b113ad",
   "metadata": {},
   "source": [
    "### Run Evaluation on All Test Cases\n",
    "\n",
    "Now that we've defined all test cases, let's run the evaluation once on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d14a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_evaluator(\n",
    "    evaluator_name=\"fluency\",\n",
    "    evaluation_contents=evaluation_contents,\n",
    "    data_source_config=data_source_config,\n",
    "    initialization_parameters=initialization_parameters,\n",
    "    data_mapping=data_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80270783",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "View the evaluation results for each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc32e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
