{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0675a7",
   "metadata": {},
   "source": [
    "# Relevance Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates how to use the Relevance evaluator to assess the relevance of AI-generated responses. The evaluator supports the following input format:\n",
    "- Simple query and response pairs\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 15 minutes running this notebook.\n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend using `gpt-4.1`. It is not recommended to use `gpt-4.1-nano`.\n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity openai\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **AZURE_AI_PROJECT_ENDPOINT** - Your Azure AI project endpoint in format: `https://<account_name>.services.ai.azure.com/api/projects/<project_name>`\n",
    "2) **AZURE_AI_MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator (e.g., gpt-4o-mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21aa7e",
   "metadata": {},
   "source": [
    "## What is Relevance?\n",
    "\n",
    "The Relevance evaluator assesses the ability of AI responses to capture the key points and address the user's query appropriately. It measures how well the response relates to and answers the specific question or request.\n",
    "\n",
    "**Scoring:** Relevance scores range from 1 to 5, with:\n",
    "- **1**: Completely irrelevant to the query\n",
    "- **2**: Partially relevant but misses key points\n",
    "- **3**: Moderately relevant with some key points addressed\n",
    "- **4**: Highly relevant with most key points covered\n",
    "- **5**: Perfectly relevant, fully addressing the query\n",
    "\n",
    "High relevance scores indicate the AI system's understanding of the input and capability to produce contextually appropriate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd0ebf",
   "metadata": {},
   "source": [
    "## Relevance Evaluator Input Requirements\n",
    "\n",
    "The Relevance evaluator supports the following input format:\n",
    "\n",
    "1. **Simple Query-Response Evaluation:**\n",
    "   - `query`: The user's question or request (str)\n",
    "   - `response`: The AI's response to evaluate (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519631da",
   "metadata": {},
   "source": [
    "### Initialize Relevance Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai.types.evals.create_eval_jsonl_run_data_source_param import SourceFileContentContent\n",
    "from pprint import pprint\n",
    "from agent_utils import run_evaluator\n",
    "\n",
    "# Get environment variables\n",
    "deployment_name = os.environ[\"AZURE_AI_MODEL_DEPLOYMENT_NAME\"]\n",
    "\n",
    "# Data source configuration (defines the schema for evaluation inputs)\n",
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"type\": \"object\"}}]},\n",
    "            \"response\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"type\": \"object\"}}]},\n",
    "        },\n",
    "        \"required\": [\"query\", \"response\"],\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "}\n",
    "\n",
    "# Data mapping (maps evaluation inputs to evaluator parameters)\n",
    "data_mapping = {\n",
    "    \"query\": \"{{item.query}}\",\n",
    "    \"response\": \"{{item.response}}\"\n",
    "}\n",
    "\n",
    "# Initialization parameters for the evaluator\n",
    "initialization_parameters = {\n",
    "    \"deployment_name\": deployment_name\n",
    "}\n",
    "\n",
    "# Initialize the evaluation_contents list - we'll append all test cases here\n",
    "evaluation_contents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f739700",
   "metadata": {},
   "source": [
    "## Sample Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8d0ec",
   "metadata": {},
   "source": [
    "### Query and Response as Strings (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eaf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: Highly relevant weather response (should score high)\n",
    "query1 = \"How is the weather in Seattle?\"\n",
    "response1 = \"The current weather in Seattle is rainy with a temperature of 14Â°C. It's typical Pacific Northwest weather for this time of year with overcast skies and light precipitation.\"\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query1,\n",
    "            \"response\": response1\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89266048",
   "metadata": {},
   "source": [
    "### Example of Irrelevant Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69761825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Completely irrelevant response (should score low)\n",
    "query2 = \"What are the symptoms of flu?\"\n",
    "response2 = \"The stock market had significant fluctuations today, with technology stocks leading the decline.\"\n",
    "\n",
    "# Append to evaluation_contents\n",
    "evaluation_contents.append(\n",
    "    SourceFileContentContent(\n",
    "        item={\n",
    "            \"query\": query2,\n",
    "            \"response\": response2\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56752df0",
   "metadata": {},
   "source": [
    "### Run Evaluation on All Test Cases\n",
    "\n",
    "Now that we've defined all test cases, let's run the evaluation once on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7519cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_evaluator(\n",
    "    evaluator_name=\"relevance\",\n",
    "    evaluation_contents=evaluation_contents,\n",
    "    data_source_config=data_source_config,\n",
    "    initialization_parameters=initialization_parameters,\n",
    "    data_mapping=data_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97b1e7",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "View the evaluation results for each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7343ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
