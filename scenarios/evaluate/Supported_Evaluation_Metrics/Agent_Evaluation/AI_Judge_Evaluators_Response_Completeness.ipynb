{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Completeness Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Response Completeness Evaluator on agent's response when ground truth is provided. This evaluator is helpful when you have ground truth to assess the quality of the agent's final response. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 20 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **AZURE_AI_PROJECT** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Response Completeness evaluator assesses the quality of an agent response by examining how well it aligns with the provided ground truth. The evaluation is based on the following scoring system:\n",
    "\n",
    "<pre>\n",
    "Score 1: Fully incomplete: The response misses all necessary and relevant information compared to the ground truth.\n",
    "Score 2: Barely complete: The response contains only a small percentage of the necessary information.\n",
    "Score 3: Moderately complete: The response includes about half of the necessary information.\n",
    "Score 4: Mostly complete: The response contains most of the necessary information, with only minor omissions.\n",
    "Score 5: Fully complete: The response perfectly matches all necessary and relevant information from the ground truth.\n",
    "</pre>\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "- Response: The response to be evaluated. (string)\n",
    "- Ground Truth: The correct and complete information against which the response is compared. (string)\n",
    "\n",
    "The evaluator uses these inputs to determine the completeness score, ensuring that the response meaningfully addresses the query while adhering to the provided definitions and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Completeness Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator, AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "response_completeness_evaluator = ResponseCompletenessEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating for a ground_truth and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 4,\n",
       " 'response_completeness_result': 'pass',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The response contains most of the relevant information, but misses the explicit purpose of \"city sightseeing\" for Day 1. Otherwise, it is accurate and complete for Day 2.',\n",
       " 'response_completeness_prompt_tokens': 1395,\n",
       " 'response_completeness_completion_tokens': 176,\n",
       " 'response_completeness_total_tokens': 1571,\n",
       " 'response_completeness_finish_reason': 'stop',\n",
       " 'response_completeness_model': 'gpt-4.1-2025-04-14',\n",
       " 'response_completeness_sample_input': '[{\"role\": \"user\", \"content\": \"{\\\\\"response\\\\\": \\\\\"Itinery: Day 1 check out the downtown district of the city on train; for Day 2, we can rest in hotel.\\\\\", \\\\\"ground_truth\\\\\": \\\\\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\\\\\"}\"}]',\n",
       " 'response_completeness_sample_output': '[{\"role\": \"assistant\", \"content\": \"<S0>Let\\'s think step by step: The ground truth has two statements: (1) Day 1 involves taking a train to visit the downtown area for city sightseeing, and (2) Day 2 involves resting in the hotel. The response covers both days: (1) Day 1 mentions checking out the downtown district on train, which is similar to the ground truth but omits \\\\\"city sightseeing\\\\\" explicitly, and (2) Day 2 mentions resting in the hotel, which matches the ground truth. The main omission is the explicit mention of \\\\\"city sightseeing\\\\\" as the purpose for visiting downtown.</S0>\\\\n<S1>The response contains most of the relevant information, but misses the explicit purpose of \\\\\"city sightseeing\\\\\" for Day 1. Otherwise, it is accurate and complete for Day 2.</S1>\\\\n<S2>4</S2>\"}]'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent response is complete\n",
    "result = response_completeness_evaluator(\n",
    "    response=\"Itinery: Day 1 check out the downtown district of the city on train; for Day 2, we can rest in hotel.\",\n",
    "    ground_truth=\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 5,\n",
       " 'response_completeness_result': 'pass',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The response fully contains the ground truth information without missing any details.',\n",
       " 'response_completeness_prompt_tokens': 1410,\n",
       " 'response_completeness_completion_tokens': 120,\n",
       " 'response_completeness_total_tokens': 1530,\n",
       " 'response_completeness_finish_reason': 'stop',\n",
       " 'response_completeness_model': 'gpt-4.1-2025-04-14',\n",
       " 'response_completeness_sample_input': '[{\"role\": \"user\", \"content\": \"{\\\\\"response\\\\\": \\\\\"The order with ID 123 has been shipped and is expected to be delivered on March 15, 2025. However, the order with ID 124 is delayed and should now arrive by March 20, 2025.\\\\\", \\\\\"ground_truth\\\\\": \\\\\"The order with ID 124 is delayed and should now arrive by March 20, 2025.\\\\\"}\"}]',\n",
       " 'response_completeness_sample_output': '[{\"role\": \"assistant\", \"content\": \"<S0>Let\\'s think step by step: The ground truth contains one statement: \\\\\"The order with ID 124 is delayed and should now arrive by March 20, 2025.\\\\\" The response includes this statement accurately and completely. The response also adds information about another order (ID 123), but this does not detract from the completeness regarding the ground truth statement. All necessary and relevant information from the ground truth is present in the response.</S0>\\\\n<S1>The response fully contains the ground truth information without missing any details.</S1>\\\\n<S2>5</S2>\"}]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent response is incomplete\n",
    "result = response_completeness_evaluator(\n",
    "    response=\"The order with ID 123 has been shipped and is expected to be delivered on March 15, 2025. However, the order with ID 124 is delayed and should now arrive by March 20, 2025.\",\n",
    "    ground_truth=\"The order with ID 124 is delayed and should now arrive by March 20, 2025.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating using Conversation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 1,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'There is no information in the ground truth, so the response cannot be evaluated for completeness and is considered fully incomplete.',\n",
       " 'response_completeness_prompt_tokens': 1362,\n",
       " 'response_completeness_completion_tokens': 109,\n",
       " 'response_completeness_total_tokens': 1471,\n",
       " 'response_completeness_finish_reason': 'stop',\n",
       " 'response_completeness_model': 'gpt-4.1-2025-04-14',\n",
       " 'response_completeness_sample_input': '[{\"role\": \"user\", \"content\": \"{\\\\\"response\\\\\": \\\\\"The weather in Seattle this weekend will be partly cloudy with temperatures around 15\\\\\\\\u00b0C on Saturday.\\\\\", \\\\\"ground_truth\\\\\": \\\\\"\\\\\"}\"}]',\n",
       " 'response_completeness_sample_output': '[{\"role\": \"assistant\", \"content\": \"<S0>Let\\'s think step by step: The ground truth is completely empty, meaning there is no information provided to compare against the response. According to the definitions, completeness is measured by how well the response reflects the ground truth. Since there are no claims or statements in the ground truth, the response cannot be complete in relation to it.</S0>\\\\n<S1>There is no information in the ground truth, so the response cannot be evaluated for completeness and is considered fully incomplete.</S1>\\\\n<S2>1</S2>\"}]'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation format - includes user query, expected ground truth, and agent response\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather forecast for Seattle this weekend?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The weather in Seattle this weekend will be partly cloudy with temperatures around 15°C on Saturday.\",\n",
    "            \"context\": {\n",
    "                \"ground_truth\": \"The weather in Seattle this weekend will be partly cloudy with temperatures around 15°C on Saturday and 17°C on Sunday, with a 20% chance of rain on Sunday afternoon.\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "result = response_completeness_evaluator(conversation=conversation)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
