{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: end-to-end enterprise agent evaluation (Azure AI Agent Service)\n",
    "\n",
    "This evaluation sample evalutes the enterprise mulit-tool agent in https://github.com/Azure-Samples/azure-ai-agent-service-enterprise-demo from end to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to evaluate Azure AI Agent\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation yfinance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime as pydatetime\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = os.environ[\"MODEL_DEPLOYMENT_NAME\"]\n",
    "AGENT_NAME = \"my-enterprise-agent-v1\"\n",
    "INSTRUCTION = (\n",
    "    \"You are a helpful enterprise assistant for Contoso. \"\n",
    "    f\"Today's date is {pydatetime.now().strftime('%A, %b %d, %Y, %I:%M %p')}. \"\n",
    "    \"You have access to HR documents in file_search, product catalog in ai_search, the grounding engine from bing and \"\n",
    "    \"custom python functions like fetch_weather, fetch_stock_price, etc. \"\n",
    "    \"Provide well-structured, concise, and professional answers.\"\n",
    ")\n",
    "\n",
    "# set up enterprise toolset with Bing/FileSearch/AZSSearch/yfinance/custom functions\n",
    "from set_up_enterprise_toolset import set_up_enterprise_toolset\n",
    "\n",
    "enterprise_toolset = set_up_enterprise_toolset(project_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent or find existing agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_agent = None\n",
    "print(\"Listing existing agents...\")\n",
    "agents_list = project_client.agents.list_agents().data\n",
    "print(f\"Found {len(agents_list)} existing agents\")\n",
    "\n",
    "for agent in agents_list:\n",
    "    if agent.name == AGENT_NAME:\n",
    "        found_agent = agent\n",
    "        print(f\"Found existing agent: {agent.name} (id: {agent.id})\")\n",
    "        break\n",
    "\n",
    "if found_agent:\n",
    "    print(\"Updating existing agent...\")\n",
    "    print(\"found_agent.id = \", found_agent.id)\n",
    "    try:\n",
    "        agent = project_client.agents.update_agent(\n",
    "            agent_id=found_agent.id,\n",
    "            model=MODEL_NAME,\n",
    "            instructions=found_agent.instructions,\n",
    "            toolset=enterprise_toolset,\n",
    "        )\n",
    "        print(\"Agent updated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating agent: {e!s}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Creating new agent...\")\n",
    "    try:\n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=MODEL_NAME, name=AGENT_NAME, instructions=INSTRUCTION, toolset=enterprise_toolset\n",
    "        )\n",
    "        print(f\"New agent created with id: {agent.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating agent: {e!s}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = \"\"\n",
    "if not thread_id:\n",
    "    thread = project_client.agents.create_thread()\n",
    "    thread_id = thread.id\n",
    "    print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation with Agent\n",
    "Use below cells to have conversation with the agent\n",
    "- `Create Message[1]`\n",
    "- `Execute[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Message[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create message to thread\n",
    "\n",
    "MESSAGE = \"The best tent available\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread_id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = project_client.agents.create_and_process_run(thread_id=thread_id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Batch Execute on a list of user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # HR Policy & Document Search (RAG) Questions\n",
    "    \"How many vacation days do employees get per year?\",\n",
    "    # Weather-Related Questions (Function Calling)\n",
    "    \"Will it rain in New York this weekend?\",\n",
    "    # Stock Market & Financial Questions\n",
    "    \"How is Microsoft's stock performing today?\",\n",
    "    # Bing Search Integration Questions\n",
    "    \"What are the latest developments in AI technology?\",\n",
    "    # Edge Cases and Error Handling\n",
    "    \"Can you access classified company documents?\",\n",
    "    # Combined Capability Questions\n",
    "    \"Based on the weather forecast and stock market performance, should we have our team meeting outside?\",\n",
    "]\n",
    "\n",
    "for query in questions:\n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=thread_id,\n",
    "        role=\"user\",\n",
    "        content=query,\n",
    "    )\n",
    "    print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread_id, agent_id=agent.id)\n",
    "\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    print(f\"Run ID: {run.id}\")\n",
    "    import time\n",
    "\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Agent Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in project_client.agents.list_messages(thread_id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the agent thread data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "- Intent Resolution: Measures how well the agent identifies the user’s request, including how well it scopes the user’s intent, asks clarifying questions, and reminds end users of its scope of capabilities.\n",
    "- Tool Call Accuracy: Evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps.\n",
    "- Task Adherence: Measures how well the agent’s response adheres to its assigned tasks, according to its system message and prior steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ToolCallAccuracyEvaluator, IntentResolutionEvaluator, TaskAdherenceEvaluator\n",
    "from pprint import pprint\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "\n",
    "default_connection = project_client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_OPEN_AI, include_credentials=True\n",
    ")\n",
    "model_config = default_connection.to_evaluator_model_config(\n",
    "    deployment_name=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    "    api_version=os.environ[\"MODEL_DEPLOYMENT_API_VERSION\"],\n",
    "    include_credentials=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Measures how well the agent identifies the user’s request, including how well it scopes the user’s intent, asks clarifying questions, and reminds end users of its scope of capabilities.\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "# Evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps.\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "# Measures how well the agent’s response adheres to its assigned tasks, according to its system message and prior steps\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Batch Evaluation on agent thread data (multiple agent runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# specify a file path to save agent output (which is evaluation input data)\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename)\n",
    "\n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"agent eval demo - batch run\",\n",
    "    evaluators={\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect batch run results in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall average evaluation metrics\n",
    "response[\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
