{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5280e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Evaluate Semantic Kernel AI (ChatCompletion) Agents in Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330c099",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This sample demonstrates how to evaluate Semantic Kernel AI ChatCompletionAgents in Azure AI Foundry. It provides a step-by-step guide to set up the environment, create an agent, and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364c694",
   "metadata": {},
   "source": [
    "## Time\n",
    "You can expect to complete this sample in approximately 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c6017",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### Packages\n",
    "- `semantic-kernel` installed (`pip install semantic-kernel`)\n",
    "- `azure-ai-evaluation` SDK installed\n",
    "- An Azure OpenAI resource with a deployment configured\n",
    "\n",
    "### Environment Variables\n",
    "- For AzureChatService:\n",
    "  - `api_key`\n",
    "  - `chat_deployment_name`\n",
    "  - `endpoint`\n",
    "- For evaluating agents:\n",
    "  - `AZURE_OPENAI_ENDPOINT`\n",
    "  - `AZURE_OPENAI_API_KEY`\n",
    "  - `AZURE_OPENAI_API_VERSION`\n",
    "  - `MODEL_DEPLOYMENT_NAME`\n",
    "- For Azure AI Foundry (Bonus):\n",
    "  - `AZURE_SUBSCRIPTION_ID`\n",
    "  - `PROJECT_NAME`\n",
    "  - `RESOURCE_GROUP_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d6576",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Create a AzureChatCompletion service - [reference](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/?tabs=csharp-AzureOpenAI%2Cpython-AzureOpenAI%2Cjava-AzureOpenAI&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# You can do the following if you have set the necessary environment variables or created a .env file\n",
    "chat_completion_service = AzureChatCompletion(service_id=\"my-service-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef319288",
   "metadata": {},
   "source": [
    "### Create a ChatCompletionAgent - [reference](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/chat-completion-agent?pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76781359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "from typing import Annotated\n",
    "\n",
    "# This is a sample plugin that provides tools\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6abead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "# Create the agent by directly providing the chat completion service\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service,\n",
    "    name=\"Chef\",\n",
    "    instructions=\"Answer questions about the menu.\",\n",
    "    plugins=[MenuPlugin()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b9ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## User: Hello\n",
      "## Chef: Hello! How can I assist you today? If you have any questions about the menu, feel free to ask!\n",
      "\n",
      "## User: What is the special drink today?\n",
      "## Chef: The special drink today is Chai Tea. Would you like to know more about other specials or the menu?\n",
      "\n",
      "## User: What does that cost?\n",
      "## Chef: The Chai Tea costs $9.99. Is there anything else you would like to know?\n",
      "\n",
      "## User: Thank you\n",
      "## Chef: You're welcome! If you have any more questions or need assistance in the future, feel free to ask. Enjoy your day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread = None\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello\",\n",
    "    \"What is the special drink today?\",\n",
    "    \"What does that cost?\",\n",
    "    \"Thank you\",\n",
    "]\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"## User: {user_input}\")\n",
    "    print(f\"## {response.name}: {response}\\n\")\n",
    "    thread = response.thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586d3e5",
   "metadata": {},
   "source": [
    "### Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available turn indices: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import SKAgentConverter\n",
    "\n",
    "# Get the avaiable turn indices for the thread, \n",
    "# useful for selecting a specific turn for evaluation\n",
    "turn_indices = await SKAgentConverter._get_thread_turn_indices(thread=thread)\n",
    "print(f\"Available turn indices: {turn_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d4ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SKAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "converter = SKAgentConverter()\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = await converter.convert(\n",
    "    thread=thread,\n",
    "    turn_index=2, # Specify the turn index you want to evaluate\n",
    "    agent=agent # Pass it to include the instructions and plugins in the evaluation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7813b5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "# Save the agent thread data to a JSONL file (all turns)\n",
    "evaluation_data = await converter.prepare_evaluation_data(threads=[thread], filename=file_name, agent=agent)\n",
    "# print(json.dumps(evaluation_data, indent=4))\n",
    "len(evaluation_data) # number of turns in the thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf87cab",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "We will select the following evaluators to assess the different aspects relevant for agent quality: \n",
    "\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample): measures the extent of which an agent identifies the correct intent from a user query. Scale: integer 1-5. Higher is better.\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample): evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps. Scale: float 0-1. Higher is better.\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample): measures the extent of which an agent’s final response adheres to the task based on its system message and a user query. Scale: integer 1-5. Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee09df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bd50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tool_call_accuracy\": 1.0,\n",
      "    \"tool_call_accuracy_result\": \"pass\",\n",
      "    \"tool_call_accuracy_threshold\": 0.8,\n",
      "    \"per_tool_call_details\": [\n",
      "        {\n",
      "            \"tool_call_accurate\": true,\n",
      "            \"tool_call_accurate_reason\": \"The tool call is relevant to the user's request for the price of the special drink, has appropriate parameters that match the tool definition, and uses a parameter value that is present in the conversation. Therefore, it is likely to help resolve the user's need.\",\n",
      "            \"tool_call_id\": \"call_4aGdX7zYtMV28tjngPTTvyXx\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test a single evaluation run\n",
    "evaluator = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "# evaluation_data_single_run.keys() # query, response, tool_definitions\n",
    "res = evaluator(**evaluation_data_single_run)\n",
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bab561",
   "metadata": {},
   "source": [
    "#### Bonus - run on perviously saved file for all turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0530c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38d924",
   "metadata": {},
   "source": [
    "## Inspect results on Azure AI Foundry\n",
    "\n",
    "Go to AI Foundry URL for rich Azure AI Foundry data visualization to inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ae69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can use the following to get the evaluation results in memory\n",
    "\n",
    "# average scores across all runs\n",
    "pprint(response[\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
