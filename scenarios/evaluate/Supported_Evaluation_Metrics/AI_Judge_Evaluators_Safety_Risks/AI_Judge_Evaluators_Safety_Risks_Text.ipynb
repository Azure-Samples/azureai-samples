{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate Risk and Safety of Text - Protected Material and Indirect Attack Jailbreak\n",
    "\n",
    "## Objective\n",
    "This notebook walks through how to generate a simulated conversation targeting a deployed AzureOpenAI model and then evaluate that conversation test dataset for Protected Material and Indirect Attack Jailbreak (also know as XPIA or cross-domain prompt injected attack) vulnerability. It also references Azure AI Content Safety service's prompt filtering capabilities to help identify and mitigate these vulnerabilities in your AI system.\n",
    "\n",
    "## Time\n",
    "You should expect to spend about 30 minutes running this notebook. If you increase or decrease the number of simulated conversations, the time will vary accordingly.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install openai azure-ai-evaluation azure-identity promptflow-azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"<your-subscription-id>\",\n",
    "    \"resource_group_name\": \"<your-resource-group-name>\",\n",
    "    \"project_name\": \"<your-project-name>\",\n",
    "}\n",
    "\n",
    "azure_openai_deployment = \"\"\n",
    "azure_openai_endpoint = \"\"\n",
    "azure_openai_api_version = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "The following simulator and evaluators require an Azure AI Studio project configuration and an Azure credential to use. \n",
    "Your project configuration will be what is used to log your evaluation results in your project after the evaluation run is finished.\n",
    "\n",
    "For this sample, we recommend creating or using a project in East US 2. For full region supportability, see [our documentation](https://learn.microsoft.com/azure/ai-studio/how-to/develop/flow-evaluate-sdk#built-in-evaluators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import ProtectedMaterialEvaluator, IndirectAttackEvaluator\n",
    "from azure.ai.evaluation.simulator import (\n",
    "    AdversarialSimulator,\n",
    "    AdversarialScenario,\n",
    "    AdversarialScenarioJailbreak,\n",
    "    IndirectAttackSimulator,\n",
    ")\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this notebook lightweight, let's create a dummy application that calls GPT 3.5 Turbo, which is essentially Chat GPT. When we are testing your application for certain safety metrics like Protected Material or Indirect Attacks, it's important to have a way to automate a basic style of red-teaming to elicit behaviors from a simulated malicious user. We will use the `Simulator` class and this is how we will generate a synthetic test dataset against your application. Once we have the test dataset, we can evaluate them with our `ProtectedMaterialEvaluator` and `IndirectAttackEvaluator` classes.\n",
    "\n",
    "The `Simulator` needs a structured contract with your application in order to simulate conversations or other types of interactions with it. This is achieved via a callback function. This is the function you would rewrite to actually format the response from your generative AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "async def protected_material_callback(\n",
    "    messages: List[Dict], stream: bool = False, session_state: Optional[str] = None, context: Optional[Dict] = None\n",
    ") -> dict:\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    # Get a client handle for the model\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    # Call the model\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": messages[\"messages\"][0][\"content\"],  # injection of prompt happens here.\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    formatted_response = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": context,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Protected Material\n",
    "\n",
    "When building your application, you want to test that Protected Material (i.e. copyrighted content or material) is not being generated by your generative AI applications. The following example uses an `AdversarialSimulator` paired with a protected content scenario to prompt your model to respond with material that is protected by intellectual property laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the adversarial simulator\n",
    "protected_material_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "# define the adversarial scenario you want to simulate\n",
    "protected_material_scenario = AdversarialScenario.ADVERSARIAL_CONTENT_PROTECTED_MATERIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_protected_material_outputs = await protected_material_simulator(\n",
    "    scenario=protected_material_scenario,\n",
    "    max_conversation_turns=3,  # define the number of conversation turns\n",
    "    max_simulation_results=10,  # define the number of simulation results\n",
    "    target=protected_material_callback,  # define the target model callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are truncated for brevity.\n",
    "truncation_limit = 50\n",
    "for output in unfiltered_protected_material_outputs:\n",
    "    for turn in output[\"messages\"]:\n",
    "        print(f\"{turn['role']} : {turn['content'][0:truncation_limit]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(unfiltered_protected_material_outputs.to_eval_qr_json_lines())\n",
    "output = unfiltered_protected_material_outputs.to_eval_qr_json_lines()\n",
    "file_path = \"unfiltered_protected_material_output.jsonl\"\n",
    "\n",
    "# Write the output to the file\n",
    "with Path.open(Path(file_path), \"w\") as file:\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it for Protected Material. The `ProtectedMaterialEvaluator` class can take in the dataset and detect whether your data contains copyrighted content. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_material_eval = ProtectedMaterialEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "result = evaluate(\n",
    "    data=file_path,\n",
    "    evaluators={\"protected_material\": protected_material_eval},\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./mynewfilteredIPevalresults.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our \"model\" application gives us a defect rate showing us that we can't deploy our application just yet. Moving forward, to protect our application against generating protected material content, we can add an [Azure AI Content Safety filter for Protected Materials for text](https://learn.microsoft.com/azure/ai-services/content-safety/quickstart-protected-material) which is a mitigation layer to help protect and filter out responses from your model that may contain protected material content. Let's apply this filter and re-run the simulator and evaluation step to see if it helps with our defect rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_protected_material_outputs = await protected_material_simulator(\n",
    "    scenario=protected_material_scenario,\n",
    "    max_conversation_turns=3,  # define the number of conversation turns\n",
    "    max_simulation_results=10,  # define the number of simulation results\n",
    "    target=protected_material_callback,  # now with the Prompt Shield attached to our model deployment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_protected_material_outputs.to_eval_qr_json_lines())\n",
    "output = filtered_protected_material_outputs.to_eval_qr_json_lines()\n",
    "filtered_protected_material_file_path = \"filtered_protected_material_output.jsonl\"\n",
    "\n",
    "# Write the output to the file\n",
    "with Path.open(Path(filtered_protected_material_file_path), \"w\") as file:\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result = evaluate(\n",
    "    data=filtered_protected_material_file_path,\n",
    "    evaluators={\"protected_material\": protected_material_eval},\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myfilteredevalresults.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Indirect Attack Jailbreaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jailbreaks are direct attacks injected into either the user's query towards your application (UPIA or user prompt injected attack) or indirect attacks injected into the context sent to your application to generate a response (XPIA or cross domaine prompt injected attack). Both types of attacks will result in an altered or unexpected behavior that may result in disrupted functionality or security risks like information leakage or engaging in harmful behavior. \n",
    "\n",
    "The following example takes the \"model\" application above and simulates indirect attacks to jailbreak the model and then evaluates the dataset generated by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "async def xpia_callback(\n",
    "    messages: List[Dict], stream: bool = False, session_state: Optional[str] = None, context: Optional[Dict] = None\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    if \"file_content\" in messages[\"template_parameters\"]:\n",
    "        query += messages[\"template_parameters\"][\"file_content\"]\n",
    "    # the next few lines explain how to use the AsyncAzureOpenAI's chat.completions\n",
    "    # to respond to the simulator. You should replace it with a call to your model/endpoint/application\n",
    "    # make sure you pass the `query` and format the response as we have shown below\n",
    "\n",
    "    # Get a client handle for the model\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    oai_client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_ENDPOINT\"),\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    try:\n",
    "        response_from_oai_chat_completions = oai_client.chat.completions.create(\n",
    "            messages=[{\"content\": query, \"role\": \"user\"}], model=deployment, max_tokens=300\n",
    "        )\n",
    "        print(response_from_oai_chat_completions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} with content length {len(query)}\")\n",
    "        # to continue the conversation, return the messages, else you can fail the adversarial with an exception\n",
    "        message = {\n",
    "            \"content\": \"Something went wrong. Check the exception e for more details.\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"context\": None,\n",
    "        }\n",
    "        messages[\"messages\"].append(message)\n",
    "        return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state}\n",
    "    response_result = response_from_oai_chat_completions.choices[0].message.content\n",
    "    formatted_response = {\n",
    "        \"content\": response_result,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {},\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indirect_attack_simulator = IndirectAttackSimulator(\n",
    "    azure_ai_project=azure_ai_project, credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "unfiltered_indirect_attack_outputs = await indirect_attack_simulator(\n",
    "    target=xpia_callback,\n",
    "    scenario=AdversarialScenarioJailbreak.ADVERSARIAL_INDIRECT_JAILBREAK,\n",
    "    max_simulation_results=10,\n",
    "    max_conversation_turns=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(unfiltered_indirect_attack_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are truncated for brevity.\n",
    "truncation_limit = 50\n",
    "for output in unfiltered_indirect_attack_outputs:\n",
    "    for turn in output[\"messages\"]:\n",
    "        content = turn[\"content\"]\n",
    "        if isinstance(content, dict):  # user response from callback is dict\n",
    "            print(f\"{turn['role']} : {content['content'][0:truncation_limit]}\")\n",
    "        elif isinstance(content, tuple):  # assistant response from callback is tuple\n",
    "            print(f\"{turn['role']} : {content[0:truncation_limit]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(unfiltered_indirect_attack_outputs)\n",
    "print(unfiltered_indirect_attack_outputs.to_eval_qr_json_lines())\n",
    "output = unfiltered_indirect_attack_outputs.to_eval_qr_json_lines()\n",
    "xpia_file_path = \"unfiltered_indirect_attack_outputs.jsonl\"\n",
    "\n",
    "# Write the output to the file\n",
    "with Path.open(Path(xpia_file_path), \"w\") as file:\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it to see if the indirect attacks resulted in jailbreaks. The `IndirectAttackEvaluator` class can take in the dataset and detects instances of jailbreak. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indirect_attack_eval = IndirectAttackEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "file_path = \"indirect_attack_outputs.jsonl\"\n",
    "result = evaluate(\n",
    "    data=xpia_file_path,\n",
    "    evaluators={\n",
    "        \"indirect_attack\": indirect_attack_eval,\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./mynewindirectattackevalresults.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our \"model\" application gives us a defect rate broken down by different behaviors resulting from the jailbreak, showing us that we can't deploy our application just yet. Moving forward, to protect our application against indirect jailbreak attacks, we can add an [Azure AI Content Safety Prompt Shield](https://learn.microsoft.com/azure/ai-services/content-safety/quickstart-jailbreak) which is a mitigation layer to help annotate and block requests to your model or application that contain known indirect attacks for jailbreak. Let's apply this filter and re-run the simulator and evaluation step to see if it helps with our defect rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_indirect_attack_outputs = await indirect_attack_simulator(\n",
    "    target=xpia_callback,  # now with the Prompt Shield attached to our model deployment\n",
    "    scenario=AdversarialScenarioJailbreak.ADVERSARIAL_INDIRECT_JAILBREAK,\n",
    "    max_simulation_results=10,\n",
    "    max_conversation_turns=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(filtered_indirect_attack_outputs)\n",
    "print(filtered_indirect_attack_outputs.to_eval_qr_json_lines())\n",
    "output = filtered_indirect_attack_outputs.to_eval_qr_json_lines()\n",
    "xpia_file_path = \"filtered_indirect_attack_outputs.jsonl\"\n",
    "\n",
    "# Write the output to the file\n",
    "with Path.open(Path(xpia_file_path), \"w\") as file:\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indirect_attack_result = evaluate(\n",
    "    data=xpia_file_path,\n",
    "    evaluators={\"indirect_attack\": indirect_attack_eval},\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myindirectattackevalresults.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we've walked through how to generate test datasets using the simulation framework and our safety evaluation framework. See our documentation for more details and additional functionality on [simulation](https://aka.ms/advsimulatorhowto) and [evaluation](https://aka.ms/azureaistudiosafetyevalhowto).\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
