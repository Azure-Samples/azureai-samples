{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Safety Evaluations of Audio Models\n",
    "This following demo notebook demonstrates the evaluation of safety evaluations for audio scenarios.\n",
    "\n",
    "Azure AI evaluations provides a comprehensive Python SDK and studio UI experience for running evaluations for your generative AI applications. The notebook is broken up into the following sections:\n",
    "\n",
    "1. Setup and Configuration\n",
    "2. Helper Functions for [Speech SDK](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#text-to-speech-models-preview) and [Real-time Audio Models](https://learn.microsoft.com/en-us/azure/ai-services/openai/realtime-audio-quickstart?tabs=keyless%2Cwindows&pivots=ai-foundry-portal)\n",
    "3. Simulating Adversarial Conversations with Audio \n",
    "4. Using Content Safety Evaluator to Evaluate Conversations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "First ensure you install the necessary requirements. In addition to what is listed in `requirements.txt`, you will need to download [ffpmg](https://ffmpeg.org/download.html) for handling of audio files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following multi-modal evaluators in this sample require an Azure AI Studio project configuration and an Azure credential to use. \n",
    "\n",
    "- ContentSafetyEvaluator (This is composite version of following evaluators)\n",
    "\t\n",
    "    - ViolenceEvaluator\t\n",
    "    - SexualEvaluator\t\n",
    "    - SelfHarmEvaluator\t\n",
    "    - HateUnfairnessEvaluator\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the assignments below with the required values to run the rest of this sample. \n",
    "Ensure that you have downloaded and installed the Azure CLI and logged in with your Azure credentials using `az login` in your CLI prior to these steps. \n",
    "\n",
    "*Important*: We recommend using East US 2 or Sweden Central as your AI Hub/AI project region to support all built-in safety evaluators. A subset of service-based safety evaluators are available in other regions, please see the supported regions in our [documentation](https://aka.ms/azureaistudiosafetyevalhowto). Please configure your project in a supported region to access the safety evaluation service via our evaluation SDK. Additionally, your project scope will be what is used to log your evaluation results in your project after the evaluation run is finished.\n",
    "\n",
    "Set the following environment variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Azure OpenAI variables\n",
    "os.environ[\"AZURE_SUBSCRIPTION_ID\"] = \"\"\n",
    "os.environ[\"AZURE_RESOURCE_GROUP\"] = \"\"\n",
    "os.environ[\"AZURE_PROJECT_NAME\"] = \"\"\n",
    "\n",
    "# Azure OpenAI Realtime Audio deployment variables\n",
    "os.environ[\"AZURE_OPENAI_AUDIO_DEPLOYMENT\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_AUDIO_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_AUDIO_ENDPOINT\"] = \"\"\n",
    "\n",
    "# Azure Speech Service variables\n",
    "os.environ[\"AZURE_SPEECH_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_SPEECH_REGION\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation.simulator import AdversarialSimulator, AdversarialScenario\n",
    "\n",
    "\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Speech SDK and Real-time Audio Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Speech SDK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "def text_to_speech(text: str, output_file: str) -> None:\n",
    "    # Set up the subscription info for the Speech Service:\n",
    "    speech_key = os.environ.get(\"AZURE_SPEECH_KEY\")\n",
    "    service_region = os.environ.get(\"AZURE_SPEECH_REGION\")\n",
    "\n",
    "    # Create an instance of a speech config with specified subscription key and service region.\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "    # Create an audio configuration that points to an audio file.\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(filename=output_file)\n",
    "\n",
    "    # Create a synthesizer with the given settings\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Synthesize the text to speech\n",
    "    result = synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    # Check result\n",
    "    # if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    #     print(f\"Speech synthesized for text [{text}] and saved to [{output_file}]\")\n",
    "    if result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def add_silence(input_file: str, output_file: str, silence_duration_ms: int = 500) -> None:\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Create silence audio segments\n",
    "    silence = AudioSegment.silent(duration=silence_duration_ms)\n",
    "\n",
    "    # Add silence at the beginning and end\n",
    "    audio_with_silence = silence + audio + silence\n",
    "\n",
    "    # Export the modified audio\n",
    "    audio_with_silence.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Real-time Audio Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing_extensions import Any\n",
    "\n",
    "\n",
    "def log(start_time: float, *args: Any) -> None:  # noqa: ANN401\n",
    "    elapsed_time_ms = int((time.time() - start_time) * 1000)\n",
    "    print(f\"{elapsed_time_ms} [ms]: \", *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtclient import RTClient\n",
    "\n",
    "\n",
    "async def receive_control(start_time: float, client: RTClient) -> None:\n",
    "    async for control in client.control_messages():\n",
    "        if control is not None:\n",
    "            log(start_time, f\"Received a control message: {control.type}\")\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resample_audio(audio_data, original_sample_rate, target_sample_rate):  # noqa: ANN201, ANN001\n",
    "    number_of_samples = round(len(audio_data) * float(target_sample_rate) / original_sample_rate)\n",
    "    resampled_audio = resample(audio_data, number_of_samples)\n",
    "    return resampled_audio.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "async def send_audio(client: RTClient, audio_file_path: Path) -> None:\n",
    "    sample_rate = 24000\n",
    "    duration_ms = 100\n",
    "    samples_per_chunk = sample_rate * (duration_ms / 1000)\n",
    "    bytes_per_sample = 2\n",
    "    bytes_per_chunk = int(samples_per_chunk * bytes_per_sample)\n",
    "\n",
    "    audio_data, original_sample_rate = sf.read(audio_file_path, dtype=\"int16\")\n",
    "    if original_sample_rate != sample_rate:\n",
    "        audio_data = resample_audio(audio_data, original_sample_rate, sample_rate)\n",
    "\n",
    "    audio_bytes = audio_data.tobytes()\n",
    "    for i in range(0, len(audio_bytes), bytes_per_chunk):\n",
    "        chunk = audio_bytes[i : i + bytes_per_chunk]\n",
    "        await client.send_audio(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtclient import (\n",
    "    RTAudioContent,\n",
    "    RTClient,\n",
    "    RTFunctionCallItem,\n",
    "    RTMessageItem,\n",
    "    RTResponse,\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def receive_message_item(start_time: float, item: RTMessageItem, out_dir: str) -> None:\n",
    "    prefix = f\"[response={item.response_id}][item={item.id}]\"\n",
    "    async for contentPart in item:\n",
    "        if contentPart.type == \"audio\":\n",
    "\n",
    "            async def collect_audio(audioContentPart: RTAudioContent) -> bytearray:\n",
    "                audio_data = bytearray()\n",
    "                async for chunk in audioContentPart.audio_chunks():\n",
    "                    audio_data.extend(chunk)\n",
    "                return audio_data\n",
    "\n",
    "            async def collect_transcript(audioContentPart: RTAudioContent) -> str:\n",
    "                audio_transcript: str = \"\"\n",
    "                async for chunk in audioContentPart.transcript_chunks():\n",
    "                    audio_transcript += chunk\n",
    "                return audio_transcript\n",
    "\n",
    "            audio_task = asyncio.create_task(collect_audio(contentPart))\n",
    "            transcript_task = asyncio.create_task(collect_transcript(contentPart))\n",
    "            audio_data, audio_transcript = await asyncio.gather(audio_task, transcript_task)\n",
    "            log(start_time, prefix, f\"Audio received with length: {len(audio_data)}\")\n",
    "            log(start_time, prefix, f\"Audio Transcript: {audio_transcript}\")\n",
    "            audio_path = Path(out_dir) / f\"{item.id}.wav\"\n",
    "            with Path(audio_path).resolve().open(\"wb\") as out:\n",
    "                audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "                sf.write(out, audio_array, samplerate=24000)\n",
    "            audio_transcript_path = Path(out_dir) / f\"{item.id}.audio_transcript.txt\"\n",
    "            with Path(audio_transcript_path).resolve().open(\n",
    "                \"w\",\n",
    "                encoding=\"utf-8\",\n",
    "            ) as out:\n",
    "                out.write(audio_transcript)\n",
    "        elif contentPart.type == \"text\":\n",
    "            text_data = \"\"\n",
    "            async for chunk in contentPart.text_chunks():\n",
    "                text_data += chunk\n",
    "            log(start_time, prefix, f\"Text: {text_data}\")\n",
    "            text_path = Path(out_dir) / f\"{item.id}.text.txt\"\n",
    "            with Path(text_path).resolve().open(\"w\", encoding=\"utf-8\") as out:\n",
    "                out.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def receive_function_call_item(start_time: float, item: RTFunctionCallItem, out_dir: str) -> None:\n",
    "    prefix = f\"[function_call_item={item.id}]\"\n",
    "    await item\n",
    "    log(start_time, prefix, f\"Function call arguments: {item.arguments}\")\n",
    "    function_call_path = Path(out_dir) / f\"{item.id}.function_call.json\"\n",
    "    with Path(function_call_path).resolve().open(\"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(item.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def receive_response(start_time: float, response: RTResponse, out_dir: str) -> list:\n",
    "    prefix = f\"[response={response.id}]\"\n",
    "    item_ids = set()\n",
    "    background_tasks = set()\n",
    "    async for item in response:\n",
    "        log(start_time, prefix, f\"Received item {item.id}\")\n",
    "        item_ids.add(item.id)\n",
    "        if item.type == \"message\":\n",
    "            task = asyncio.create_task(receive_message_item(start_time=start_time, item=item, out_dir=out_dir))\n",
    "        elif item.type == \"function_call\":\n",
    "            task = asyncio.create_task(receive_function_call_item(start_time=start_time, item=item, out_dir=out_dir))\n",
    "        background_tasks.add(task)\n",
    "        task.add_done_callback(background_tasks.discard)\n",
    "    return list(item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating Adversarial Conversations with Audio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio-based Callback Function\n",
    "\n",
    "The Azure AI Evaluation SDK's Adversarial Simulator provides text to prompt your model to produce harmful content. In this callback function, we use your Speech service connection to convert this text to audio, and then prompt your audio model to respond to the converted audio. These responses will form the dataset of conversations which are converted back to text using the Speech service to be used by the Content Safety evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from rtclient import NoTurnDetection\n",
    "\n",
    "\n",
    "async def audio_callback(\n",
    "    messages: List[Dict], stream: bool = False, session_state: Optional[str] = None, context: Optional[Dict] = None\n",
    ") -> dict:\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_AUDIO_ENDPOINT\")\n",
    "    audio_key = os.environ.get(\"AZURE_OPENAI_AUDIO_API_KEY\")\n",
    "    audio_deployment = os.environ.get(\"AZURE_OPENAI_AUDIO_DEPLOYMENT\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    async with RTClient(\n",
    "        url=endpoint, key_credential=AzureKeyCredential(audio_key), azure_deployment=audio_deployment\n",
    "    ) as rt_client:\n",
    "        log(start_time, \"Connected to RTClient\")\n",
    "        text_to_speech(messages[\"messages\"][0][\"content\"], f\"./generated-audio/conv_{0}_{1}_tmp.wav\")\n",
    "\n",
    "        add_silence(f\"./generated-audio/conv_{0}_{1}_tmp.wav\", f\"./generated-audio/conv_{0}_{1}.wav\")\n",
    "\n",
    "        asyncio.create_task(receive_control(start_time, rt_client))  # noqa: RUF006\n",
    "        with Path(\"./instruction.txt\").open() as instructions_file:\n",
    "            instructions = instructions_file.read()\n",
    "\n",
    "        log(start_time, \"Configuring Session...\")\n",
    "        await rt_client.configure(instructions=instructions, turn_detection=NoTurnDetection())\n",
    "\n",
    "        audio_file_path = Path(\"./generated-audio/conv_0_1.wav\")\n",
    "        out_dir = Path(f\"./generated-audio/conv_{0}_{1}_out\")\n",
    "\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        log(start_time, f\"Sending Audio: {audio_file_path}\")\n",
    "        await send_audio(rt_client, Path.resolve(audio_file_path))\n",
    "        await rt_client.commit_audio()\n",
    "        response = await rt_client.generate_response()\n",
    "        last_transcript = \"\"\n",
    "        item_ids = await receive_response(start_time, response, out_dir)\n",
    "        log(start_time, item_ids)\n",
    "        await asyncio.sleep(3)  # Adding sleep to ensure all audio transcripts are written\n",
    "        formatted_response = {}\n",
    "        for item_id in item_ids:\n",
    "            file_path = Path(out_dir) / f\"{item_id}.audio_transcript.txt\"\n",
    "            if item_id is not None and Path(file_path).resolve().exists():\n",
    "                with Path(file_path).resolve().open(\"r\", encoding=\"utf-8\") as out:\n",
    "                    last_transcript = out.read()\n",
    "                    last_transcript = last_transcript.replace(\"\\n\", \" \").strip()\n",
    "                formatted_response = {\n",
    "                    \"content\": last_transcript,\n",
    "                    \"role\": \"assistant\",\n",
    "                }\n",
    "        messages[\"messages\"].append(formatted_response)\n",
    "        return {\n",
    "            \"messages\": messages[\"messages\"],\n",
    "            \"stream\": stream,\n",
    "            \"session_state\": session_state,\n",
    "            \"context\": context,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "adv_scenario = AdversarialScenario.ADVERSARIAL_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await adv_simulator(\n",
    "    scenario=adv_scenario,\n",
    "    max_simulation_results=3,  # define the number of simulation results\n",
    "    target=audio_callback,  # define the target model callback\n",
    "    concurrent_async_task=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to the file\n",
    "with Path(\"audio-harm.jsonl\").open(\"w\") as file:\n",
    "    file.writelines(outputs.to_eval_qr_json_lines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Content Safety Evaluator to Evaluate Conversations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ContentSafetyEvaluator\n",
    "\n",
    "cs_eval = ContentSafetyEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "result = evaluate(\n",
    "    name=\"content-safety-audio-conversations\",\n",
    "    data=\"audio-harm.jsonl\",\n",
    "    evaluators={\"content_safety\": cs_eval},\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./content-safety-audio-conversations-results.json\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-test-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
