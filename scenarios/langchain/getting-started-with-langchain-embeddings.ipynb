{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and Azure AI Foundry\n",
    "\n",
    "This notebook explain how to use `langchain-azure-ai` package with the capabilities in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "To run this tutorial you need either:\n",
    "\n",
    "1. Using GitHub Models:\n",
    "\n",
    "    1. You can use [GitHub models](https://github.com/marketplace/models) endpoint including the free tier experience.\n",
    "    2. Use the endpoint `https://models.inference.ai.azure.com` along with your GitHub Token.\n",
    "\n",
    "1. Using Azure AI Foundry:\n",
    "\n",
    "    1. Create an [Azure subscription](https://azure.microsoft.com).\n",
    "    2. Create an Azure AI hub resource as explained at [How to create and manage an Azure AI Studio hub](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource).\n",
    "    3. Deploy one model supporting the [Azure AI model inference API](https://aka.ms/azureai/modelinference). In this example we use a `text-embedding-3-large` deployment. \n",
    "\n",
    "        * You can follow the instructions at [Add and configure models to Azure AI model inference service](https://learn.microsoft.com/azure/ai-studio/ai-services/how-to/create-model-deployments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages:\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-core langchain-azure-ai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use embeddings models\n",
    "\n",
    "Create a client to connect to the endpoint. In this case, we are working with an embeddings model hence we import the class `AzureAIEmbeddingsModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "create_embed_model_client"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n",
    "\n",
    "embed_model = AzureAIEmbeddingsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"text-embedding-3-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the previous example, we are indicating the parameter `model_name` since our endpoint has multiple models deployed on it. If your endpoint has only 1 model deployed, like with Serverless API Endpoints, you don't need to indicate the parameter `model_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an in memory vector store to demonstrate how to use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "create_vector_store"
   },
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some documents to the store. Adding documents will call the embeddings model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "add_documents"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(id=\"1\", page_content=\"foo\", metadata={\"baz\": \"bar\"})\n",
    "document_2 = Document(id=\"2\", page_content=\"thud\", metadata={\"bar\": \"baz\"})\n",
    "\n",
    "documents = [document_1, document_2]\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then search by similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "search_similarity"
   },
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(query=\"thud\", k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using asynchronous calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await vector_store.asimilarity_search(query=\"thud\", k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
