{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing LangChain with Azure AI Foundry\n",
    "\n",
    "This notebook explain how to use `langchain-azure-ai` package with the capabilities in Azure AI Foundry for tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "To run this tutorial you need either:\n",
    "\n",
    "1. Create an [Azure subscription](https://azure.microsoft.com).\n",
    "2. Create an Azure AI hub resource as explained at [How to create and manage an Azure AI Studio hub](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource).\n",
    "3. Deploy one model supporting the [Azure AI model inference API](https://aka.ms/azureai/modelinference). In this example we use a `Mistral-Large-2407` and a `Mistral-Small` deployment. \n",
    "\n",
    "    * You can follow the instructions at [Add and configure models to Azure AI model inference service](https://learn.microsoft.com/azure/ai-studio/ai-services/how-to/create-model-deployments).\n",
    "\n",
    "4. Go to the section **Tracing** in Azure AI Foundry portal and copy the instrumentation connection string.\n",
    "5. Configure the following environment variables (or update the code in the notebook directly):\n",
    "\n",
    "    * `AZURE_APPINSIGHT_CONNECTION`: The instrumentation connection string, pointing to the Azure Application Insights used by the hub or project.\n",
    "    * `AZURE_INFERENCE_ENDPOINT`: The endpoint where the models are deployed.\n",
    "    * `AZURE_INFERENCE_CREDENTIAL`: The credentials to connect to the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages:\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-core langchain-azure-ai[opentelemetry]\n",
    "```\n",
    "\n",
    "> Notice that we are installing the extra `langchain-azure-ai[opentelemetry]` which allows instrumentation via opentelemetry with LangChain in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the connection string to Application Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the tracing capabilities in Azure AI Foundry by creating a tracer. Logs are stored in Azure Application Insights and can be queried at any time and hence you need a connection string to it. Each AI Hub has an Azure Application Insights created for you. You can get the connection string by **either**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the connection string directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "application_insights_connection_string = os.environ[\"AZURE_APPINSIGHT_CONNECTION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Azure AI Foundry SDK\n",
    "\n",
    "You can also get the connection string to Application Insights by using the Azure AI Foundry SDK along with the connection string to the project, as follows:\n",
    "\n",
    "Install the Azure AI Foundry SDK:\n",
    "\n",
    "```bash\n",
    "pip install azure-ai-projects\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=\"<your-project-connection-string>\",\n",
    ")\n",
    "\n",
    "application_insights_connection_string = project_client.telemetry.get_connection_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can find the project connection string in the landing page of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure tracing for Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a tracer connected to a project in Azure AI Foundry. Notice that the parameter `enable_content_recording` is set to `True`. This enables the capture of the inputs and outputs of the entire application as well as the intermediate steps. Such is helpful when debugging and building applications, but you may want to disable it on production environments. By default, the environment variable `AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_azure_ai.callbacks.tracers import AzureAIInferenceTracer\n",
    "\n",
    "tracer = AzureAIInferenceTracer(\n",
    "    connection_string=application_insights_connection_string,\n",
    "    enable_content_recording=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how tracers work, let's create a chain that uses multiple models and multiple steps. The following example generates a poem written by an urban poet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "producer_template = PromptTemplate(\n",
    "    template=\"You are an urban poet, your job is to come up \\\n",
    "             verses based on a given topic.\\n\\\n",
    "             Here is the topic you have been asked to generate a verse on:\\n\\\n",
    "             {topic}\",\n",
    "    input_variables=[\"topic\"],\n",
    ")\n",
    "\n",
    "verifier_template = PromptTemplate(\n",
    "    template=\"You are a verifier of poems, you are tasked\\\n",
    "              to inspect the verses of poem. If they consist of violence and abusive language\\\n",
    "              report it. Your response should be only one word either True or False.\\n \\\n",
    "              Here is the lyrics submitted to you:\\n\\\n",
    "              {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two different models, one for the producer and one for the verifier. The producer has a more complicated task so we use a more powerful model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "producer = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-large-2407\",\n",
    ")\n",
    "\n",
    "verifier = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the template, model, and the output parser from above using the pipe (`|`) operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "generate_and_verify_chain = producer_template | producer | parser | verifier_template | verifier | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure tracing with your chain, indicate the value `config` in the `invoke` operation as a callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_verify_chain.invoke({\"topic\": \"living in a foreign country\"}, config={\"callbacks\": [tracer]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: the `generate_and_verify_chain` doesn't return the actual poem. This is done to facilitate the reading in this notebook as learning chains is not the main objective. In LangChain, to return also intermediate outputs from the chain you need to use `RunnableParallel` with `RunnablePassthrough()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure the chain itself for tracing, use the `.with_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_verify_chain_tracer = generate_and_verify_chain.with_config({\"callbacks\": [tracer]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the `invoke()` method as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_verify_chain_tracer.invoke({\"topic\": \"living in a foreign country\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, traces can be seen in Azure AI Foundry as follows:\n",
    "\n",
    "![](docs/langchain-azure-ai/inference/tracing/portal-tracing-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../docs/langchain-azure-ai/inference/tracing/portal-tracing-example.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
