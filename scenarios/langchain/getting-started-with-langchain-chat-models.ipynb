{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and Azure AI Foundry\n",
    "\n",
    "This notebook explain how to use `langchain-azure-ai` package with the capabilities in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "To run this tutorial you need either:\n",
    "\n",
    "1. Using GitHub Models:\n",
    "\n",
    "    1. You can use [GitHub models](https://github.com/marketplace/models) endpoint including the free tier experience.\n",
    "    2. Use the endpoint `https://models.inference.ai.azure.com` along with your GitHub Token.\n",
    "\n",
    "1. Using Azure AI Foundry:\n",
    "\n",
    "    1. Create an [Azure subscription](https://azure.microsoft.com).\n",
    "    2. Create an Azure AI hub resource as explained at [How to create and manage an Azure AI Studio hub](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource).\n",
    "    3. Deploy one model supporting the [Azure AI model inference API](https://aka.ms/azureai/modelinference). In this example we use a `Mistral-Large-2407` and a `Mistral-Small` deployment. \n",
    "\n",
    "        * You can follow the instructions at [Add and configure models to Azure AI model inference service](https://learn.microsoft.com/azure/ai-studio/ai-services/how-to/create-model-deployments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages:\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-core langchain-azure-ai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use chat completions models\n",
    "\n",
    "Create a client to connect to the endpoint. In this case, we are working with a chat completions model hence we import the class `AzureAIChatCompletionsModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "model = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-large-2407\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the previous example, we are indicating the parameter `model_name` since our endpoint has multiple models deployed on it. If your endpoint has only 1 model deployed, like with Serverless API Endpoints, you don't need to indicate the parameter `model_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use the model directly. ChatModels are instances of LangChain Runnable, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the invoke method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tools\n",
    "\n",
    "Certain models support the use of tools, either built-in tools or defined. LangChain allows indicating tools in different ways. In the following example, we use Python functions to define the schemas. Here the use of docstring is required in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually bind those schemas to a chat model, we'll use the .bind_tools() method. This handles converting the add and multiply schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"What is 3 * 12?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple models in a chain\n",
    "\n",
    "Models deployed to Azure AI Foundry support the Azure AI model inference API, which is standard across all the models. Chain multiple LLM operations based on the capabilities of each model so you can optimize for the right model based on capabilities.\n",
    "\n",
    "In the following example, we create 2 model clients, one is a producer and another one is a verifier. To make the distinction clear, we are using a multi-model endpoint like the Azure AI model inference service and hence we are passing the parameter `model_name` to use a Mistral-Large and a Mistral-Small model, quoting the fact that producing content is more complex than verifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "producer = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-large-2407\",\n",
    ")\n",
    "\n",
    "verifier = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example generates a poem written by an urban poet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "producer_template = PromptTemplate(\n",
    "    template=\"You are an urban poet, your job is to come up \\\n",
    "             verses based on a given topic.\\n\\\n",
    "             Here is the topic you have been asked to generate a verse on:\\n\\\n",
    "             {topic}\",\n",
    "    input_variables=[\"topic\"],\n",
    ")\n",
    "\n",
    "verifier_template = PromptTemplate(\n",
    "    template=\"You are a verifier of poems, you are tasked\\\n",
    "              to inspect the verses of poem. If they consist of violence and abusive language\\\n",
    "              report it. Your response should be only one word either True or False.\\n \\\n",
    "              Here is the lyrics submitted to you:\\n\\\n",
    "              {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an output parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the template, model, and the output parser from above using the pipe (`|`) operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = producer_template | producer | parser | verifier_template | verifier | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous chain returns the output of the step `verifier` only. Since we want to access the intermediate result generated by the `producer`, in LangChain you need to use a `RunnablePassthrough` object to also output that intermediate step. The following code shows how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem = producer_template | producer | parser\n",
    "verify_poem = verifier_template | verifier | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "chain = generate_poem | RunnableParallel(poem=RunnablePassthrough(), abuse=RunnablePassthrough() | verify_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invoke the chain, identify the inputs required and provide values using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"living in a foreign country\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debugging and troubleshooting\n",
    "\n",
    "If you need to debug your application and understand which parameters are being sent to the models in Azure AI Foundry, you can use the debug capabilities of the integration as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, configure logging to the level you are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "# Acquire the logger for this client library. Use 'azure' to affect both\n",
    "# 'azure.core` and `azure.ai.inference' libraries.\n",
    "logger = logging.getLogger(\"azure\")\n",
    "\n",
    "# Set the desired logging level. logging.INFO or logging.DEBUG are good options.\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Direct logging output to stdout:\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "# Or direct logging output to a file:\n",
    "# handler = logging.FileHandler(filename=\"sample.log\")\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Optional: change the default logging format. Here we add a timestamp.\n",
    "formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(name)s:%(message)s\")\n",
    "handler.setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the payloads of the requests, when instantiating the client, pass the argument `logging_enable=True` to the `client_kwargs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "model = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    model=\"mistral-large-2407\",\n",
    "    client_kwargs={\"logging_enable\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the client as usual in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
