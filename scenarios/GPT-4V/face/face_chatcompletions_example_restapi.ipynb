{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Face Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61534b8b",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Utilize face attributes obtained from the face API to assess face image quality in GPT-4 Turbo with Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdbfe5c",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7f3b6",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23ddd2",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ed1ea",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION, FACE_API_ENDPOINT.\n",
    "\n",
    "Add \"OPENAI_API_KEY\" and \"FACE_API_KEY\" as variable name and \\<Your API Key Value\\> and \\<Your FACE Key Value\\> as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx FACE_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export FACE_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"<your GPT-4 Turbo with Vision deployment name>\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"<your resource base URL>\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"<your OpenAI API version>\"\n",
    "\n",
    "# The base URL for your face resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "face_api_endpoint: str = \"<your face resource endpoint>\"\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7880d79",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "    \"FACE_API_ENDPOINT\": face_api_endpoint,\n",
    "}\n",
    "\n",
    "p = Path(\"../config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada5e70",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f02d7",
   "metadata": {},
   "source": [
    "### Call Face API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821406a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_face_API\n",
    "\n",
    "# Setting up the face resource key\n",
    "face_api_key = os.getenv(\"FACE_API_KEY\")\n",
    "\n",
    "image_file_path = \"portrait_example.jpg\"\n",
    "attributes_from_01, attributes_from_03 = call_face_API(image_file_path, face_api_endpoint, face_api_key)\n",
    "description = \"\"\n",
    "if attributes_from_01 and attributes_from_03 and len(attributes_from_01) > 0 and len(attributes_from_03) > 0:\n",
    "    with Image.open(image_file_path) as img:\n",
    "        width, height = img.size\n",
    "    width, height = img.size\n",
    "    face_data = attributes_from_03[0][\"faceRectangle\"]\n",
    "    face_width, face_height = face_data[\"width\"], face_data[\"height\"]\n",
    "    face_left, face_top = face_data[\"left\"], face_data[\"top\"]\n",
    "\n",
    "    # Calculate the center points\n",
    "    face_center_x = face_left + face_width / 2\n",
    "    face_center_y = face_top + face_height / 2\n",
    "    image_center_x, image_center_y = width / 2, height / 2\n",
    "\n",
    "    face_size_ratio = face_height / height\n",
    "    center_x_ratio = abs(face_center_x - image_center_x) / width\n",
    "    center_y_ratio = abs(face_center_y - image_center_y) / height\n",
    "\n",
    "    if face_size_ratio < 0.45:\n",
    "        description += \"The patient's face takes up less than half (50%) the height of the photograph\\n\"\n",
    "    else:\n",
    "        description += \"The patient's face takes up more than half (50%) the height of the photograph\\n\"\n",
    "    if center_x_ratio < 0.2 and center_y_ratio < 0.2:\n",
    "        description += \"The patient's face is vertically and horizontally centered in the photograph\\n\"\n",
    "    else:\n",
    "        description += \"The patient's face is not centered in the photograph\\n\"\n",
    "\n",
    "    faceAttributes = {**attributes_from_01[0][\"faceAttributes\"], **attributes_from_03[0][\"faceAttributes\"]}\n",
    "    for attr, value in faceAttributes.items():\n",
    "        if isinstance(value, dict):\n",
    "            description += f\"{attr.capitalize()}:\\n\"\n",
    "            for subkey, subvalue in value.items():\n",
    "                description += f\"  {subkey.capitalize()}: {subvalue}\\n\"\n",
    "        elif isinstance(value, list):\n",
    "            description += f\"{attr.capitalize()}:\\n\"\n",
    "            for item in value:\n",
    "                description += f\"  {item}\\n\"\n",
    "        else:\n",
    "            description += f\"{attr.capitalize()}: {value}\\n\"\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102b54f",
   "metadata": {},
   "source": [
    "### Call GPT-4 Turbo with Vision API with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef62557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_image\n",
    "\n",
    "# Image Tagging Assistant\n",
    "sys_message = \"\"\"\n",
    "    Judge the portrait quality by the following 8 criteria. Specifically, please answer yes or no to each criterion and structure the answers of all 8 criteria in a JSON.\n",
    "    At the end, please provide an overall judgment, either 'Meets Criteria' (should meet all 8 criteria) or 'Does Not Meet Criteria' (if not meet at least one), along with a detailed explanation, \n",
    "    based on the responses to the eight criteria.\n",
    "    1. The camera is level with the patient's face (head pose value < +/-20), resulting in a photograph that is not angled up at the chin or down at the top of the head.\n",
    "    2. The patient's face is vertically and horizontally centered in the photograph.\n",
    "    3. The patient's face takes up more than half (50%) the height of the photograph. \n",
    "    4. The patient is facing the camera directly with full face in view (head pose value < +/-20) \n",
    "    5. The patient is not wearing sunglasses, but reading glasses are okay. A head covering may be worn for cultural, religious or medical purposes. \n",
    "    6. The face, including the forehead, eyes, mouth, and nose, is not occluded. The photograph is not grainy, blurry, dark, or exceedingly bright. DO NOT check blur level, only check blur value. The blur value < 0.5 is within the acceptable range for a clear view of the face. \n",
    "    7. The photograph appears to have a reasonable uniform or solid background.\n",
    "    8. To avoid HIPAA violations, there are no other people recognizable in the photograph\n",
    "    \"\"\"\n",
    "\n",
    "# Encode the image in base64\n",
    "with Path(image_file_path).open(\"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": description}],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    response_content = call_GPT4V_image(messages, face=True)\n",
    "    display(Image(image_file_path))\n",
    "    print(response_content[\"choices\"][0][\"message\"][\"content\"])  # Print the content of the response\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4 Turbo with Vision API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa690d8",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
