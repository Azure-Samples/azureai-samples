{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Video Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cfcba",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Conducting Q&A with video inputs in GPT-4 Turbo with Vision.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbcb1f",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13636fdc",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbb7da",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52a354",
   "metadata": {},
   "source": [
    "- An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services/)\n",
    "- An Azure AI resource. For guidance on setting up an Azure AI resource, see: [How to create and manage an Azure AI resource](https://review.learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource?branch=main)\n",
    "- Once you have your Azure subscription, [create a Vision resource](https://portal.azure.com/#create/Microsoft.CognitiveServicesComputerVision) in the Azure portal to get your key and endpoint. After it deploys, select Go to resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232525c",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ab502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d4a0f",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION, VISION_API_ENDPOINT.\n",
    "\n",
    "Add \"OPENAI_API_KEY\" and \"VISION_API_KEY\" as variable name and \\<Your API Key Value\\> and \\<Your VISION Key Value\\> as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         setx VISION_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "         export VISION_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57086a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"<your GPT-4 Turbo with Vision deployment name>\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"<your resource base URL>\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"<your OpenAI API version>\"\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "vision_api_endpoint: str = \"<your vision resource endpoint>\"\n",
    "\n",
    "# Insert your video SAS URL, e.g. https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-video-name>?<SAS-token>\n",
    "video_SAS_url: str = \"https://gpt4vsamples.blob.core.windows.net/videos/Microsoft%20Copilot%20Short.mp4\"\n",
    "# This index name must be unique and contain no white spaces.\n",
    "# It must start with alphanumeric, can contain hyphens but they must be followed by alphanumeric (no consecutive hyphens or trailing hyphen).\n",
    "# It must be 24 characters or less.\n",
    "video_index_name: str = \"copilot-video-demo-index\"\n",
    "# This video ID must be unique\n",
    "video_id: str = \"copilot-video-1\"\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0f8ef",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "    \"VISION_API_ENDPOINT\": vision_api_endpoint,\n",
    "}\n",
    "\n",
    "p = Path(\"../config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b60b2a",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_video, process_video_indexing\n",
    "\n",
    "# Setting up the vision resource key\n",
    "vision_api_key = os.getenv(\"VISION_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907668a",
   "metadata": {},
   "source": [
    "### Create Video Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ffbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this cell once to create the index\n",
    "process_video_indexing(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabcd3f",
   "metadata": {},
   "source": [
    "### Call GPT-4 Turbo with Vision API with Video Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6165c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System messages and user prompt\n",
    "sys_message = \"\"\"\n",
    "Your task is to assist in analyzing and optimizing creative assets. \n",
    "You will be presented with advertisement videos for products. \n",
    "First describe the video in detail paying close attention to Product characteristics highlighted, \n",
    "Background images, Lighting, Color Palette and Human characteristics for persons in the video. \n",
    "Finally provide a summary of the video and talk about the main message the advertisement video tries to convey to the viewer. \n",
    "\"\"\"\n",
    "user_prompt = \"Summarize the ad video\"\n",
    "\n",
    "# Make sure that the content of type acv_document_id is first in the use content list like in this example.\n",
    "# Otherwise unexpected behavior can happen.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"acv_document_id\", \"acv_document_id\": video_id}, {\"type\": \"text\", \"text\": user_prompt}],\n",
    "    },  # Prompt for the user\n",
    "]\n",
    "\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "video_config = {\n",
    "    \"video_SAS_url\": video_SAS_url,\n",
    "    \"video_index_name\": video_index_name,\n",
    "}\n",
    "\n",
    "# Call GPT-4 Turbo with Vision API and print the response\n",
    "try:\n",
    "    response = call_GPT4V_video(messages, vision_api=vision_api_config, video_index=video_config)\n",
    "    text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    for sentence in sentences:  # Print the content of the response\n",
    "        print(sentence)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4 Turbo with Vision API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e4343",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb64f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    # {{TODO: Add resource cleanup}}\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
