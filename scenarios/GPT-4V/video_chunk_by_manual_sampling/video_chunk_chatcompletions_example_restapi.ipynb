{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759f9ec0",
   "metadata": {},
   "source": [
    "# REST API Video Chunk Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9cb96",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Sequential processing of video chunks in GPT-4 Turbo with Vision.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d46d8",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "You should expect to spend 5-10 minutes running this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a09083",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f153d",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d4a0f",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "You need to set a series of configurations such as GPT-4V_DEPLOYMENT_NAME, OPENAI_API_BASE, OPENAI_API_VERSION.\n",
    "\n",
    "Add \"OPENAI_API_KEY\" as variable name and \\<Your API Key Value\\>as variable value in the environment variables.\n",
    " <br>\n",
    "      \n",
    "      WINDOWS Users: \n",
    "         setx OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n",
    "\n",
    "      MACOS/LINUX Users: \n",
    "         export OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"<your GPT-4 Turbo with Vision deployment name>\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"<your resource base URL>\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"<your OpenAI API version>\"\n",
    "\n",
    "# Insert your video SAS URL, e.g. https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-video-name>?<SAS-token>\n",
    "video_SAS_url = (\n",
    "    \"https://gpt4vsamples.blob.core.windows.net/videos/Redwire%20Field%20Trip%20-%203D%20Printing%20a%20Zune.mkv\"\n",
    ")\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6f99",
   "metadata": {},
   "source": [
    "## Connect to your project\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "}\n",
    "\n",
    "p = Path(\"../config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a644ef",
   "metadata": {},
   "source": [
    "## Run this Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V, sample_frames, download_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1338c",
   "metadata": {},
   "source": [
    "### Call GPT-4 Turbo with Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4343a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "local_file_path = \"downloaded_video.mp4\"\n",
    "assert download_video(video_SAS_url, local_file_path), \"Failed to download video.\"\n",
    "video = VideoFileClip(local_file_path)\n",
    "video_length = video.duration\n",
    "\n",
    "#  Call GPT-4 Turbo with Vision API on Each Video Chunk Sequentially\n",
    "# Define the number of seconds for each segment\n",
    "chunk_size = 120  # seconds\n",
    "\n",
    "num_frames_to_sample = 20\n",
    "\n",
    "if video_length is not None:\n",
    "    print(f\"Video Length: {video_length} seconds\")\n",
    "    sys_message = f\"\"\"\n",
    "    The total length of the video is {video_length}s. Your job is to analyze a {chunk_size}-\n",
    "    sec segment of the video and {num_frames_to_sample} frames from that segment. You will then provide a Current Scene Breakdown of the \n",
    "    video so far. Scenes must cover the entire video and non-overlapping. This breakdown should be a JSON object, with \n",
    "    each scenes being a key, and the value being an array of information about the scene, including topic, visual description,\n",
    "    start and end times formatted MM:SS.\n",
    "    \"\"\"\n",
    "    number_of_segments = int(video_length // chunk_size)\n",
    "    updated_response = \"\"\n",
    "    for i in range(number_of_segments + 1):  # Include the last segment\n",
    "        start_time = i * chunk_size\n",
    "        end_time = min((i + 1) * chunk_size, video_length)\n",
    "        video_clip = video.subclip(start_time, end_time)\n",
    "\n",
    "        user_prompt = f\"How many scenes from {start_time}s to {end_time}s?\"\n",
    "        print(f\"Segment {i+1}: {user_prompt}\")\n",
    "        if i > 0:\n",
    "            user_prompt += f\"\"\"And here are scenes in the previous segments: {updated_response}. \n",
    "                            You need to combine the scenes in the previous segments with the scenes in this segment and provide a summary.\n",
    "                            \"\"\"\n",
    "        base64_frames = sample_frames(video_clip, num_frames_to_sample)\n",
    "        content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "        for frame in base64_frames:\n",
    "            content.append({\"type\": \"text\", \"text\": f\"Frame at time {start_time + frame[0]} seconds:\"})\n",
    "            content.append(\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpg;base64,{frame[1]}\", \"detail\": \"low\"}}\n",
    "            )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ]\n",
    "\n",
    "        response = call_GPT4V(messages)\n",
    "        updated_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"Response for segment {i+1}: {updated_response}\")\n",
    "        time.sleep(2)  # Avoid throttling\n",
    "\n",
    "    # Print the final response\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", updated_response)\n",
    "    for sentence in sentences:  # Print the content of the response\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"Failed to process video length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66c9b7",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Azure ML resources used in this example, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "If you made a resource group specifically to run this example, you could instead [delete the resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/delete-resource-group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_cleanup:\n",
    "    Path(local_file_path).unlink()  # Delete the downloaded video\n",
    "    # {{TODO: Add resource cleanup}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
