{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protected Material and Indirect Attack Jailbreak Simulation and Evaluation\n",
    "This following demo notebook demonstrates the simulation and evaluation of the Protected Material and Indirect Attack Jailbreak. \n",
    "\n",
    "## Setup\n",
    "Here are the imports needed to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import ProtectedMaterialsEvaluator, IndirectAttackEvaluator\n",
    "from promptflow.evals.synthetic import AdversarialSimulator, AdversarialScenario, IndirectAttackSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following simulator and evaluators require an Azure AI Studio project configuration and an Azure credential to use. Please fill in the assignments below with the required values to run the rest of this sample.\n",
    "\n",
    "Protected Materials simulator and evaluator are only supported in East US 2, so please configure your project in that region to access this functionality. Additionally, your project scope will be what is used to log your evaluation results in your project after the evaluation run is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_scope = {\n",
    "#     \"subscription_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", # Azure subscription ID\n",
    "#     \"resource_group_name\": \"resource-group\", # Azure resource group name\n",
    "#     \"project_name\": \"project-name\", # Azure Machine Learning project name\n",
    "# }\n",
    "\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this notebook lightweight, let's create a dummy application that calls GPT 3.5 Turbo, which is essentially Chat GPT. When we are testing your application for certain safety metrics like Protected Material or Indirect Attacks, it's important to have a way to automate a basic style of red-teaming to elicit behaviors from a simulated malicious user. We will use the `Simulator` class and this is how we will generate a synthetic test dataset against your application. Once we have the test dataset, we can evaluate them with our `ProtectedMaterialEvaluator` and `IndirectAttackEvaluator` classes.\n",
    "\n",
    "The `Simulator` needs a structured contract with your application in order to simulate conversations or other types of interactions with it. This is achieved via a callback function. This is the function you would rewrite to actually format the response from your generative AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "async def azure_model_callback(\n",
    "        messages, stream = False, session_state = None, context = None\n",
    "    ) -> dict:\n",
    "    # Fill these values in with own model data\n",
    "    endpoint = \"https://ai-railpmhub406357280652.openai.azure.com/\"\n",
    "    deployment = \"gpt-35-turbo\"\n",
    "    # In theory, the api_key input can be replaced with a azure_ad_token_provider input, although I seem to lack the permissions for that.\n",
    "    # Get a client handle for the model\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=\"2024-05-01-preview\",\n",
    "        api_key=\"6eb3ccd0351c44b2ae865ac055c07828\"\n",
    "    )\n",
    "    # Call the model\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages= [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": messages[\"messages\"][0][\"content\"] # injection of prompt happens here.\n",
    "        }],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    formatted_response = completion.to_dict()['choices'][0]['message']\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": context,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Protected Material\n",
    "\n",
    "When building your application, you want to test that Protected Material (i.e. copyrighted content or material) is not being generated by your generative AI applications. The following example uses an `AdversarialSimulator` paired with a content scenario to prompt your model to respond with material that is protected by intellectual property laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the adversarial simulator\n",
    "protected_material_simulator = AdversarialSimulator(azure_ai_project=project_scope, credential=credential)\n",
    "\n",
    "# define the adversarial scenario you want to simulate\n",
    "protected_material_scenario = AdversarialScenario.ADVERSARIAL_CONTENT_PROTECTED_MATERIAL\n",
    "\n",
    "protected_material_outputs = await protected_material_simulator(\n",
    "    scenario=protected_material_scenario,\n",
    "    max_conversation_turns=1, # define the number of conversation turns\n",
    "    max_simulation_results=10, # define the number of simulation results\n",
    "    target=azure_model_callback, # define the target model callback\n",
    "    # api_call_retry_limit=3,\n",
    "    # api_call_retry_sleep_sec=1,\n",
    "    # api_call_delay_sec=30,\n",
    "    # concurrent_async_task=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are truncated for brevity.\n",
    "truncation_limit = 50\n",
    "for output in protected_material_outputs:\n",
    "    for turn in output['messages']:\n",
    "        print(f\"{turn['role']} : {turn['content'][0:truncation_limit]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it for Protected Material. The `ProtectedMaterialEvaluator` class can take in the dataset and detect whether your data contains copyrighted content. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize your protected material evaluator\n",
    "protected_material_eval = ProtectedMaterialsEvaluator(azure_ai_project=project_scope, credential=credential)\n",
    "result=evaluate(\n",
    "    data= protected_material_outputs,# TO DO ADD DATA\n",
    "    evaluators={\n",
    "        \"protected_material\": protected_material_eval\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project = project_scope,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myevalresults.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our \"model\" application gives us a defect rate showing us that we can't deploy our application just yet. Moving forward, to protect our application against generating protected material content, we can add an Azure AI Content Safety filter for Protected Materials for text which is a mitigation layer to help protect and filter out responses from your model that may contain protected material content. Let's apply this filter and re-run the simulator and evaluation step to see if it helps with our defect rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_protected_material_outputs = await protected_material_simulator(\n",
    "    scenario=protected_material_scenario,\n",
    "    max_conversation_turns=1, # define the number of conversation turns\n",
    "    max_simulation_results=10, # define the number of simulation results\n",
    "    target=azure_model_callback, # define the target model callback once we add the filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result=evaluate(\n",
    "    data= filtered_protected_material_outputs,# TO DO ADD DATA\n",
    "    evaluators={\n",
    "        \"protected_material\": protected_material_eval\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project = project_scope,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myfilteredevalresults.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your application for Indirect Attack Jailbreaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jailbreaks are direct attacks injected into either the user's query towards your application (UPIA or user prompt injected attack) or indirect attacks injected into the context sent to your application to generate a response (XPIA or cross domaine prompt injected attack). Both types of attacks will result in an altered or unexpected behavior that may result in disrupted functionality or security risks like information leakage or engaging in harmful behavior. \n",
    "\n",
    "The following example takes the \"model\" application above and simulates indirect attacks to jailbreak the model and then evaluates the dataset generated by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_attack_simulator = IndirectAttackSimulator(azure_ai_project=project_scope, credential=DefaultAzureCredential())\n",
    "\n",
    "indirect_attack_outputs = await indirect_attack_simulator(\n",
    "        target=azure_model_callback,\n",
    "        scenario=AdversarialScenario.ADVERSARIAL_INDIRECT_JAILBREAK,\n",
    "        max_simulation_results=10,\n",
    "        max_conversation_turns=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"indirect_attack_outputs.jsonl\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    for output in indirect_attack_outputs:\n",
    "        f.write(json.dumps(output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are truncated for brevity.\n",
    "truncation_limit = 50\n",
    "for output in indirect_attack_outputs:\n",
    "    for turn in output['messages']:\n",
    "        content = turn[\"content\"]\n",
    "        if isinstance(content, dict): # user response from callback is dict\n",
    "            print(f\"{turn['role']} : {content['content'][0:truncation_limit]}\")\n",
    "        elif isinstance(content, tuple): # assistant response from callback is tuple\n",
    "            print(f\"{turn['role']} : {content[0:truncation_limit]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can evaluate it to see if the indirect attacks resulted in jailbreaks. The `IndirectAttackEvaluator` class can take in the dataset and detects instances of jailbreak. Let's use the `evaluate()` API to run the evaluation and log it to our Azure AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_attack_eval = IndirectAttackEvaluator(project_scope=project_scope, credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    data=path,\n",
    "    evaluators={\n",
    "        \"indirect_attack\": indirect_attack_eval\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project = project_scope,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myindirectattackevalresults.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our \"model\" application gives us a defect rate broken down by different behaviors resulting from the jailbreak, showing us that we can't deploy our application just yet. Moving forward, to protect our application against indirect jailbreak attacks, we can add an Azure AI Content Safety Prompt Shield which is a mitigation layer to help annotate and block requests to your model or application that contain known indirect attacks for jailbreak. Let's apply this filter and re-run the simulator and evaluation step to see if it helps with our defect rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indirect_attack_outputs = await indirect_attack_simulator(\n",
    "        target=azure_model_callback, # now with the Prompt Shield attached to our model deployment\n",
    "        scenario=AdversarialScenario.ADVERSARIAL_INDIRECT_JAILBREAK,\n",
    "        max_simulation_results=10,\n",
    "        max_conversation_turns=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indirect_attack_result=evaluate(\n",
    "    data=filtered_indirect_attack_outputs,# TO DO ADD DATA\n",
    "    evaluators={\n",
    "        \"indirect_attack\": indirect_attack_eval\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project = project_scope,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=\"./myindirectattackevalresults.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptflow-dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
