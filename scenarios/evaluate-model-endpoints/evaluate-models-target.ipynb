{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model endpoints using Prompt Flow Eval APIs\n",
    "\n",
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to evaluate prompts against variety of model endpoints deployed on Azure AI Platform or non Azure AI platforms. \n",
    "\n",
    "This guide uses Python Class as a application target to evaluate results generated by LLM models against provided prompts. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [promptflow-evals](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-evals/promptflow.html)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 30 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates evaluating model endpoints responses against provided prompts using promptflow-evals\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-evals\n",
    "%pip install promptflow-azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target function\n",
    "We will use a `ModelEndpoints` application target to get answers from multip model endpoints against provided prompts (questions). We will use `evaluate` API to evaluate `ModelEndpoints` applicaton\n",
    "\n",
    "`ModelEndpoints` class needs following list of model endpoints and their authentication keys.\n",
    "\n",
    "For simplicity, we have provided endpoints and keys in the `env_var` variable and passed into Application Target class `ModelEndpoints` in init() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_var = {\n",
    "    \"tiny_llama\": {\n",
    "        \"endpoint\": \"https://api-inference.huggingface.co/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/v1/chat/completions\",\n",
    "        \"key\": \"\",\n",
    "    },\n",
    "    \"phi3_mini_serverless\": {\n",
    "        \"endpoint\": \"https://Phi-3-mini-4k-instruct-rqvel.eastus2.models.ai.azure.com/v1/chat/completions\",\n",
    "        \"key\": \"\",\n",
    "    },\n",
    "    \"gpt2\": {\n",
    "        \"endpoint\": \"https://api-inference.huggingface.co/models/openai-community/gpt2\",\n",
    "        \"key\": \"\",\n",
    "    },\n",
    "    \"mistral7b\": {\n",
    "        \"endpoint\": \"https://mistral-7b-east1092381.eastus2.inference.ml.azure.com/chat/completions\",\n",
    "        \"key\": \"\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please provide Azure AI Project details so that traces and eval results are pushing in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\"subscription_id\": \"\", \"resource_group_name\": \"\", \"project_name\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Following code reads Json file \"data.jsonl\" which contains inputs to the Application Target __call__ function. It provides question, context and grouth truth for evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "To use Relevance and Cohenrence Evaluator, we will Azure Open AI model details as a Judge that can be passed as model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=\"https://ai-***.openai.azure.com\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"\",\n",
    "    azure_deployment=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation\n",
    "\n",
    "Following code runs Evaluate API and uses Content Safety, Relevance and Coherence Evaluator to evaluate results from different models.\n",
    "\n",
    "Test data is provided in json file 'data.jsonl' for App \n",
    "\n",
    "Application Target  uses the questions to call specific endpoints and retrive answer from response to evaluate using Evaluate API from Promoptflow SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app_target import ModelEndpoints\n",
    "import pathlib\n",
    "\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import ContentSafetyEvaluator, RelevanceEvaluator, CoherenceEvaluator\n",
    "\n",
    "\n",
    "content_safety_evaluator = ContentSafetyEvaluator(project_scope=azure_ai_project)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config=configuration)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config=configuration)\n",
    "\n",
    "models = [\"tiny_llama\", \"phi3_mini_serverless\", \"gpt2\", \"mistral7b\"]\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data.jsonl\"\n",
    "\n",
    "for model in models:\n",
    "    randomNum = random.randint(1111, 9999)\n",
    "    results = evaluate(\n",
    "        azure_ai_project=azure_ai_project,\n",
    "        evaluation_name=\"Eval-Run-\" + str(randomNum) + \"-\" + model.title(),\n",
    "        data=path,\n",
    "        target=ModelEndpoints(env_var, model),\n",
    "        evaluators={\n",
    "            \"content_safety\": content_safety_evaluator,\n",
    "            \"coherence\": coherence_evaluator,\n",
    "            \"relevance\": relevance_evaluator,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"content_safety\": {\"question\": \"${data.question}\", \"answer\": \"${target.answer}\"},\n",
    "            \"coherence\": {\"answer\": \"${target.answer}\", \"question\": \"${data.question}\"},\n",
    "            \"relevance\": {\"answer\": \"${target.answer}\", \"context\": \"${data.context}\", \"question\": \"${data.question}\"},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
