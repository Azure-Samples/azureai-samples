{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Investment Portfolio Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **About the scenario**\n",
    "This scenario demonstrates a common project, where notebooks are used to orchestrate complex tasks involving real-time data fetching, computation, and report delivery. The notebook is modular, and secure, employing batch processing, error handling, and proper separation of concerns.\n",
    "\n",
    "In this scenario, we will:\n",
    "\n",
    "1. Upload a CSV file containing the user’s investment portfolio to the Azure OpenAI Service.\n",
    "2. Fetch real-time stock prices via the Yahoo! Finanace API utilizing *Function Calling*.\n",
    "3. Perform calculations on the portfolio using *Code Interpreter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Time**\n",
    "You should expect to spend 10-15 minutes building and running this scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Before you begin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install required libraries\n",
    "Install dependencies directly within a Jupyter notebook is a good practice because it ensures that all required packages are installed in the correct versions, making the notebook self-contained and reproducible. This approach helps other users or collaborators to set up the environment quickly and avoid potential issues related to missing or incompatible packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Setting up the environment\n",
    "\n",
    "Before we begin, we'll load all necessary environment variables from a `.env` file. These variables contain sensitive information such as API keys and endpoint URLs. Ensure your `.env` file is properly configured in the `.venv/.env` format. In order to run the following code, the `.env` file must contain the follow secrets:\n",
    "\n",
    "- AZURE_OPENAI_API_KEY\n",
    "- AZURE_OPENAI_ENDPOINT\n",
    "- AZURE_OPENAI_DEPLOYMENT\n",
    "- AZURE_OPENAI_API_VERSION \n",
    "\n",
    "For more information about leveraging Python Virtual Environments can be found [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\".venv/.env\")\n",
    "\n",
    "# Retrieve the secrets\n",
    "__AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "__AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "__AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "__AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "# Verify environment variables\n",
    "if not all([ __AZURE_OPENAI_API_KEY,\n",
    "             __AZURE_OPENAI_ENDPOINT, \n",
    "             __AZURE_OPENAI_DEPLOYMENT, \n",
    "             __AZURE_OPENAI_API_VERSION]):\n",
    "    raise EnvironmentError(\"One or more environment variables are missing. Please check the .env file.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Azure OpenAI setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initializing the Azure OpenAI Client\n",
    "\n",
    "Next, we’ll initialize the Azure OpenAI client. This client allows us to interact with Azure OpenAI's service and file management APIs. You’ll need your API key, endpoint, and API version, which we just loaded from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureOpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI, OpenAIError\n",
    "\n",
    "# Initialize the AzureOpenAI client\n",
    "try:\n",
    "    openai_client = AzureOpenAI(\n",
    "        api_key=__AZURE_OPENAI_API_KEY,\n",
    "        api_version=__AZURE_OPENAI_API_VERSION,\n",
    "        azure_endpoint=__AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "    print(\"AzureOpenAI client initialized.\")\n",
    "except OpenAIError as e:\n",
    "    raise ConnectionError(f\"Failed to initialize AzureOpenAI client: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Upload supporting file to Azure OpenAI deployment\n",
    "\n",
    "Now, we'll upload the `investment_portfolio.csv` file from the `\\data` directory to Azure OpenAI, ensuring any existing `investment_portfolio.csv` removed beforehand to ensure the latest version of the file is used and no duplicates exist. This process will handle the entire upload. The file is necessary for this scenario, but its contents can be modified as long as the file structure remains unchanged. The file schema must be as follows:\n",
    "\n",
    "- Symbol\n",
    "- Average_Cost\n",
    "- QTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing file: investment_portfolio.csv\n",
      "Uploaded file: investment_portfolio.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory containing files to upload\n",
    "directory=\"data\"\n",
    "portfolio_file=\"investment_portfolio.csv\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(directory):\n",
    "    print(f\"Directory '{directory}' does not exist.\")\n",
    "    raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n",
    "\n",
    "file_path = os.path.join(directory, portfolio_file)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(file_path):\n",
    "    print(f\"Skipping non-file item: {portfolio_file}\")\n",
    "\n",
    "try:\n",
    "    # Delete existing file in Azure OpenAI Service if it has the same name and purpose\n",
    "    existing_files = openai_client.files.list()\n",
    "    for f in existing_files:\n",
    "        if f.filename == portfolio_file and f.purpose == \"assistants\":\n",
    "            openai_client.files.delete(file_id=f.id)\n",
    "            print(f\"Deleted existing file: {portfolio_file}\")\n",
    "\n",
    "    # Upload new file\n",
    "    with open(file_path, \"rb\") as file_data:\n",
    "       portfolio_file = openai_client.files.create(file=file_data, purpose=\"assistants\")\n",
    "    print(f\"Uploaded file: {portfolio_file.filename}\")\n",
    "\n",
    "except OpenAIError as e:\n",
    "    print(f\"Error processing file '{portfolio_file}': {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error with file '{portfolio_file}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Azure OpenAI Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define function for Assistant\n",
    "The `fetch_stock_price` function retrieves the stock data for a specified `ticker symbol` using the `yfinance` library, specifically pulling the latest data for the last trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def fetch_stock_price(ticker_symbol: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the latest stock price for a given ticker symbol.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker_symbol (str): The ticker symbol of the stock to retrieve data for.\n",
    "\n",
    "    Returns:\n",
    "    - str: The closing price of the stock for the latest trading day, or an error message if data is unavailable.\n",
    "\n",
    "    Example:\n",
    "    >>> fetch_stock_price(\"AAPL\")\n",
    "    \"148.9\"\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the stock's trading history for the last day\n",
    "        stock = yf.Ticker(ticker_symbol)\n",
    "        stock_data = stock.history(period=\"1d\")\n",
    "\n",
    "        # Check if the data is empty, indicating an invalid ticker or no data available\n",
    "        if stock_data.empty:\n",
    "            return f\"Error: No data found for ticker symbol: {ticker_symbol}\"\n",
    "\n",
    "        # Retrieve and return the latest closing price\n",
    "        latest_close_price = stock_data['Close'].iloc[-1]\n",
    "        return str(round(latest_close_price, 3))\n",
    "\n",
    "    except KeyError as e:\n",
    "        return f\"Error: Data missing for key: {e}. Verify the ticker symbol.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Unexpected issue occurred - {type(e).__name__}: {e}\"\n",
    "    \n",
    "print(\"Function defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Function Calling tool for Assistant\n",
    "The tool-calling definition informs the LLM about available tools. For our scenario, it enables code interpretation and function calling. The `fetch_stock_price` function configuration is to be used along side the code interpreter tool. By adding this tool to the Assistant, we enable it to retrieve and interpret stock data when a user prompts it with related queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools list defined successfully.\n"
     ]
    }
   ],
   "source": [
    "tools_list = [\n",
    "    {\"type\": \"code_interpreter\"},\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_stock_price\",\n",
    "            \"description\": \"Retrieve the latest closing price of a stock using its ticker symbol.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"ticker_symbol\": {\"type\": \"string\", \"description\": \"The ticker symbol of the stock\"}},\n",
    "                \"required\": [\"ticker_symbol\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Tools list defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Creating the Investment Management Assistant\n",
    "In this step, we create an assistant equipped with specialized tools, including a code interpreter, to handle investment-related queries and utilize the previously defined function calls. This assistant will analyze the uploaded portfolio file and offer valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant created successfully.\n",
      " Assistant(id='asst_xvxLgxK8MUMutkrrMp5IMR92', created_at=1731700777, description=None, instructions='You are an expert investment analyst. Use your knowledge base to answer questions about personal investment portfolio management.', metadata={}, model='gpt-4o', name='Investment Management Assistant', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter'), FunctionTool(function=FunctionDefinition(name='fetch_stock_price', description='Retrieve the latest closing price of a stock using its ticker symbol.', parameters={'type': 'object', 'properties': {'ticker_symbol': {'type': 'string', 'description': 'The ticker symbol of the stock'}}, 'required': ['ticker_symbol']}, strict=False), type='function')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=[]), file_search=None), top_p=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Create the assistant with code interpreter and function calling tools enabled\n",
    "try:\n",
    "    assistant = openai_client.beta.assistants.create(\n",
    "        name=\"Investment Management Assistant\",\n",
    "        instructions=(\n",
    "            \"You are an expert investment analyst. \"\n",
    "            \"Use your knowledge base to answer questions about personal investment portfolio management.\"\n",
    "        ),\n",
    "        model=__AZURE_OPENAI_DEPLOYMENT,\n",
    "        tools=tools_list\n",
    "    )\n",
    "    print(\"Assistant created successfully.\\n\", assistant)\n",
    "except OpenAIError as e:\n",
    "    print(\"Error creating assistant:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Querying the Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Starting a New Conversation Thread\n",
    "Let's create a new thread to handle user interactions. Each thread allows for a dedicated conversation with the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread created successfully.\n",
      " Thread(id='thread_aDZQEiy8QK3QwMaHk3JiM6TI', created_at=1731700780, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation thread\n",
    "try:\n",
    "    thread = openai_client.beta.threads.create()\n",
    "    print(\"Thread created successfully.\\n\", thread)\n",
    "except OpenAIError as e:\n",
    "    print(\"Error creating thread:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Adding User Message\n",
    "In this step, we add a question to the thread. For this demonstration, we'll what the latest closing price is for specified company (*Microsoft*) that will leverage *function calling* to retrieve this information from the `fetch_stock_price` function. Then, code interpreter will use this information as well as the `QTY` data from the `investment_portfolio.csv` that was uploaded to calculate the **total investment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user question\n",
    "prompt_content = \"What is the latest closing price for Microsoft? What is my total investment for MSFT as of today?\"\n",
    "\n",
    "# Add the question to the thread\n",
    "try:\n",
    "    message = openai_client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=prompt_content,\n",
    "        attachments=[  # Add files by using the attachments parameter\n",
    "            {\"file_id\": portfolio_file.id, \"tools\": [{\"type\": \"code_interpreter\"}]}\n",
    "        ],\n",
    "    )\n",
    "    print(\"User question added:\", message)\n",
    "except OpenAIError as e:\n",
    "    print(\"Error adding user question:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Running the Assistant\n",
    "Now that the assistant and thread are set up, we'll initiate the assistant's response process. This will analyze the user prompt and provide insights based on the investment portfolio data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the assistant's response\n",
    "try:\n",
    "    run = openai_client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=prompt_content,\n",
    "    )\n",
    "    print(\"Run started:\", run)\n",
    "except OpenAIError as e:\n",
    "    print(\"Error starting run:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <code style=\"background:yellow;color:black\">Define the dictionary `available_functions` that maps function names to their corresponding implementations, in this case, fetch_stock_price</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\"fetch_stock_price\": fetch_stock_price}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Monitor run status\n",
    " <code style=\"background:yellow;color:black\">The assistant may take some time to analyze and respond so we must monitor the run status. Each status will perform an action. To understand the status actions more, lets simulate a loops. Click the next 4 cells multiple times to see the progress of the Assitant Run.\n",
    "\n",
    "\n",
    "Generally we use a loop to check the status and perform actions based on the outcome of each check. \n",
    "Retrieves the current status of the OpenAI Assistant run. The status is printed in a formatted JSON string for clarity. Depending on the status of the run, *different actions are taken*. </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Status: Queued*\n",
    "<code style=\"background:yellow;color:black\">What does this mean - probably wont use. but explain</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Run is an instance where an Assistant operates within a Thread. During a Run, the Assistant processes the Thread's Messages and its configuration to perform tasks using models and tools, adding additional Messages to the Thread as part of its operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Retrieve the run status\n",
    "run = openai_client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "#print(run.model_dump_json(indent=4))\n",
    "\n",
    "if run.status in ('queued', 'in_progress'):\n",
    "    print(f\"Run this cell again to monitor the status.\\nCurrent Status: {run.status}\")\n",
    "else:\n",
    "    print(f\"Monitoring the run status...\\nCurrent Status: {run.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4a: Failed status\n",
    "\n",
    "If the status is `failed` we will print the error message with relevant information to aid in troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(run.model_dump_json(indent=4))\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(\"Assistant run failed. Please try again.\")\n",
    "else:\n",
    "    print(f\"Assistant run has not failed...\\nNavigate to and execute the '{run.status}' cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4b: Requires Action status\n",
    "\n",
    "If the status is `requires_action`, first we need to check if the required action is to submit tool outputs and iterate over the tool calls, ensuring that the requested function exists in the available_functions dictionary. If the function exists, it is called with the provided arguments, and the response is stored. After processing all tool calls, we submit the tool outputs back to the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == \"requires_action\":\n",
    "    print(\"Function Calling ...\")\n",
    "    tool_responses = []\n",
    "    if (\n",
    "        run.required_action.type == \"submit_tool_outputs\"\n",
    "        and run.required_action.submit_tool_outputs.tool_calls is not None\n",
    "    ):\n",
    "        tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "\n",
    "        for call in tool_calls:\n",
    "            if call.type == \"function\":\n",
    "                if call.function.name not in available_functions:\n",
    "                    raise Exception(\"Function requested by the model does not exist\")\n",
    "                function_to_call = available_functions[call.function.name]\n",
    "                tool_response = function_to_call(**json.loads(call.function.arguments))\n",
    "                tool_responses.append({\"tool_call_id\": call.id, \"output\": tool_response})\n",
    "                print(f\"Function '{call.function.name}' called successfully. \\nOutput: {tool_response}\\n\")\n",
    "\n",
    "    run = openai_client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread.id, run_id=run.id, tool_outputs=tool_responses\n",
    "    )\n",
    "\n",
    "    print(f\"Results submitted successfully. Go back to the first cell, 'Monitor Run Status' and execute again.\")\n",
    "else:\n",
    "    print(f\"Navigate to and execute the {run.status} cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4c: Completed status\n",
    "\n",
    "If the status is `completed`, we will fetch and print all messages in the thread, displaying the role and content of each message.The messages are printed in reverse order because messages in a thread are in FILO (First-In-Last-Out) order and in order to make the messages more conversational for ease of user reading, we must reverse the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == \"completed\":\n",
    "    messages = openai_client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\", after=message.id) # Ascending order for output\n",
    "\n",
    "    print(f'Run completed!\\n\\nMESSAGES\\n')\n",
    "\n",
    "    # Loop through messages and print content based on role\n",
    "    for msg in messages.data:\n",
    "        role = msg.role\n",
    "        content = msg.content[0].text.value\n",
    "        print(f\"{role.capitalize()}: {content}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Navigate to and execute the {run.status} cell.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Step / Let's refine \n",
    "Wrap in function - explain the \n",
    "\n",
    "We'll use a polling loop to periodically check the assistant's status and retrieve the answer once it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `process_message` created successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "available_functions = {\"fetch_stock_price\": fetch_stock_price}\n",
    "\n",
    "def process_message(thread_id, prompt_message, attachments_list=None):\n",
    "    try:\n",
    "        # Add the prompt to the thread\n",
    "        message = openai_client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content=prompt_message,\n",
    "            attachments=attachments_list,\n",
    "        )\n",
    "        print(\"User message added...\") #:\", message)\n",
    "\n",
    "        # Initiate the assistant's response\n",
    "        run = openai_client.beta.threads.runs.create(\n",
    "            thread_id=thread_id,\n",
    "            assistant_id=assistant.id,\n",
    "            instructions=prompt_message,\n",
    "        )\n",
    "        print(\"Run started...\") #:\", run)\n",
    "    except OpenAIError as e:\n",
    "        print(\"Error starting run:\", e)\n",
    "\n",
    "    while True:  # Polling to monitor Run status\n",
    "        time.sleep(5)  # Wait 5 seconds to give the process time to move past `queued` state\n",
    "\n",
    "        # Retrieves the thread’s response.\n",
    "        run = openai_client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "\n",
    "        run_status = run.status\n",
    "        print(f\"Run Status: {run_status}\\n\")\n",
    "        if run_status == 'completed':\n",
    "            # Get all messages in thread to read\n",
    "            thread_messages = openai_client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\") #, after=message.id)\n",
    "\n",
    "            # Loop through thread messages and print content\n",
    "            for thread_message in thread_messages.data:\n",
    "                role = thread_message.role\n",
    "                content = None\n",
    "                if isinstance(thread_message.content, list) and thread_message.content:\n",
    "                    first_content = thread_message.content[0]\n",
    "                    if hasattr(first_content, 'text') and hasattr(first_content.text, 'value'):\n",
    "                        content = first_content.text.value\n",
    "                    else:\n",
    "                        content = str(first_content)\n",
    "                else:\n",
    "                    content = str(thread_message.content)\n",
    "\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            \n",
    "            break\n",
    "        elif run.status == \"failed\":\n",
    "            messages = openai_client.beta.threads.messages.list(thread_id=thread.id)\n",
    "            answer = messages.data[0].content[0].text.value\n",
    "            print(f\"Failed User:\\n{prompt_message}\\nAssistant:\\n{answer}\\n\")\n",
    "\n",
    "            # Handle failed\n",
    "            break\n",
    "\n",
    "        elif run.status == \"requires_action\" and run.required_action.type == \"submit_tool_outputs\":\n",
    "            print(\"Function calling initiated...\")\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_responses = []\n",
    "            \n",
    "            # Iterate over each function call requested by the assistant\n",
    "            for call in tool_calls:\n",
    "                # Check if the call is a function and if the function exists is our custom function\n",
    "                if call.type == \"function\" and call.function.name in available_functions:\n",
    "                    func = available_functions[call.function.name]  # Retrieve the function reference\n",
    "                    \n",
    "                    # Parse the function arguments from JSON and execute the function\n",
    "                    tool_response = func(**json.loads(call.function.arguments))\n",
    "                    \n",
    "                    # Store the tool call ID and output to later send back to the assistant\n",
    "                    tool_responses.append({\"tool_call_id\": call.id, \"output\": tool_response})\n",
    "                    print(f\"Executed '{call.function.name}'. Output: {tool_response}\")\n",
    "                \n",
    "                else:\n",
    "                    # Raise an error if the function is not in available_functions to handle unexpected requests\n",
    "                    raise ValueError(f\"Requested function '{call.function.name}' is not available.\")\n",
    "            \n",
    "            # Submit all collected tool outputs back to the assistant to satisfy the required action\n",
    "            run = openai_client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread_id, \n",
    "                run_id=run.id, \n",
    "                tool_outputs=tool_responses\n",
    "            )\n",
    "            print(\"Function call(s) completed successfully.\")\n",
    "\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "\n",
    "print(\"Function `process_message` created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message added...\n",
      "Run started...\n",
      "Run Status: completed\n",
      "\n",
      "User: What is today's date?\n",
      "Assistant: Today's date is November 15, 2024.\n"
     ]
    }
   ],
   "source": [
    "process_message(thread_id=thread.id, prompt_message=\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message added...\n",
      "Run started...\n",
      "Run Status: in_progress\n",
      "\n",
      "Run Status: completed\n",
      "\n",
      "User: What is today's date?\n",
      "Assistant: Today's date is November 15, 2024.\n",
      "User: What stock do I have the most investment in?\n",
      "Assistant: The file contains the following columns: `Symbol`, `Average_Cost`, and `QTY`. These columns represent the ticker symbol of the stock, the average cost per share, and the quantity of shares held, respectively.\n",
      "\n",
      "To determine which stock you have the most investment in, I will calculate the total investment for each stock (Average Cost * Quantity) and identify the one with the highest value.\n",
      "Assistant: The stock you have the most investment in is Tesla (TSLA) with a total investment of $90,000.\n"
     ]
    }
   ],
   "source": [
    "attachments=[  # Add files by using the attachments parameter\n",
    "            {\"file_id\": portfolio_file.id, \"tools\": [{\"type\": \"code_interpreter\"}]}\n",
    "        ]\n",
    "\n",
    "process_message(thread_id=thread.id, prompt_message=\"What stock do I have the most investment in?\", attachments_list=attachments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message added...\n",
      "Run started...\n",
      "Run Status: requires_action\n",
      "\n",
      "Function calling initiated...\n",
      "Executed 'fetch_stock_price'. Output: 200.65\n",
      "Function call(s) completed successfully.\n",
      "Run Status: completed\n",
      "\n",
      "User: What is today's date?\n",
      "Assistant: Today's date is November 15, 2024.\n",
      "User: What stock do I have the most investment in?\n",
      "Assistant: The file contains the following columns: `Symbol`, `Average_Cost`, and `QTY`. These columns represent the ticker symbol of the stock, the average cost per share, and the quantity of shares held, respectively.\n",
      "\n",
      "To determine which stock you have the most investment in, I will calculate the total investment for each stock (Average Cost * Quantity) and identify the one with the highest value.\n",
      "Assistant: The stock you have the most investment in is Tesla (TSLA) with a total investment of $90,000.\n",
      "User: What is the current stock price for Amazon?\n",
      "Assistant: The current stock price for Amazon (AMZN) is $200.65.\n"
     ]
    }
   ],
   "source": [
    "process_message(thread_id=thread.id, prompt_message=\"What is the current stock price for Amazon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_message(thread_id=thread.id, prompt_message=\"Show a pie chart of my investments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_message_old(thread_id, prompt_message, attachments_list=None):\n",
    "\n",
    "    try:\n",
    "        # Add the prompt to the thread\n",
    "        message = openai_client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content=prompt_message,\n",
    "            attachments=attachments_list,\n",
    "        )\n",
    "        print(\"User question added:\", message)\n",
    "\n",
    "        # Initiate the assistant's response\n",
    "        run = openai_client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            instructions=prompt_message,\n",
    "        )\n",
    "        print(\"Run started:\", run)\n",
    "    except OpenAIError as e:\n",
    "        print(\"Error starting run:\", e)\n",
    "\n",
    "\n",
    "    while True: # Polling to monitor Run status\n",
    "        \n",
    "        # Wait 5 seconds to give the process time to move past `queued` state\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Retrieves the thread’s response.\n",
    "        run = openai_client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "\n",
    "        run_status = run.status\n",
    "        print(f\"Run Status: {run_status}\\n\")\n",
    "        if run_status =='completed':\n",
    "\n",
    "            # Get all messages in thread to read\n",
    "            thread_messages = openai_client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\") #, after=message.id)\n",
    "\n",
    "            # Loop through thread messages and print content\n",
    "            for thread_message in thread_messages.data:\n",
    "                role = thread_message.role\n",
    "                #content = thread_message.content[0].text.value\n",
    "                \n",
    "                # Safely access content based on its actual structure\n",
    "                if isinstance(thread_message.content, list) and thread_message.content:\n",
    "                    first_content = thread_message.content[0]\n",
    "                    if hasattr(first_content, 'text') and hasattr(first_content.text, 'value'):\n",
    "                        content = first_content.text.value\n",
    "                    else:\n",
    "                        content = str(first_content)  # Fallback to string representation\n",
    "                else:\n",
    "                    content = str(thread_message.content)  # Handle non-list or empty content\n",
    "\n",
    "                # Printing the `role` makes the result conversational for users\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            \n",
    "            break\n",
    "        elif run.status == \"failed\":\n",
    "            messages = openai_client.beta.threads.messages.list(thread_id=thread.id)\n",
    "            answer = messages.data[0].content[0].text.value\n",
    "            print(f\"Failed User:\\n{prompt_message}\\nAssistant:\\n{answer}\\n\")\n",
    "            # Handle failed\n",
    "            break\n",
    "\n",
    "        \n",
    "        else:\n",
    "            time.sleep(5)\n",
    "\n",
    "print(\"Function `process_message` created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time \n",
    "\n",
    "def get_assistant_response(thread_id, run_id, user_prompt):\n",
    "   \n",
    "    Interacts with the assistant by sending a user prompt, handling function calls if required, \n",
    "    and retrieving the conversation messages as a response.\n",
    "\n",
    "    Parameters:\n",
    "        thread_id (str): ID of the conversation thread.\n",
    "        run_id (str): ID of the current assistant run.\n",
    "        user_prompt (str): The user's prompt or question.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Each message as a dictionary with 'role' and 'content' keys.\n",
    "    \n",
    "    # Send initial prompt - Add a message to the thread with the role of \"user\" and the provided content  \n",
    "    openai_client.beta.threads.messages.create(\n",
    "        thread_id=thread_id,    # Use the ID of the thread passed through to the function\n",
    "        content=user_prompt,    # User content message\n",
    "        role=\"user\"             # Role of the message sender\n",
    "    )\n",
    "\n",
    "    # Polling to monitor Run status\n",
    "    while True:\n",
    "        # Retrieve and inspect the assistant's current run status\n",
    "        run = openai_client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
    "\n",
    "        if run.status == \"failed\":\n",
    "            print(\"Assistant run failed. Please try again.\")\n",
    "            break\n",
    "            #return [{\"role\": \"system\", \"content\": \"Assistant run failed. Please try again.\"}]\n",
    "        \n",
    "        elif run.status == \"completed\":\n",
    "            # Collect messages if run is complete\n",
    "            messages = openai_client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\")\n",
    "            #conversation = [{\"role\": msg.role, \"content\": msg.content[0].text.value} for msg in messages.data]\n",
    "\n",
    "            # Loop through messages and print content based on role\n",
    "            for msg in messages.data:\n",
    "                role = msg.role\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            break\n",
    "\n",
    "            #for msg in conversation:  # Display the conversation\n",
    "            #    print(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "            #return conversation\n",
    "        \n",
    "        elif run.status == \"requires_action\" and run.required_action.type == \"submit_tool_outputs\":\n",
    "            print(\"Function calling initiated...\")\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_responses = []\n",
    "            \n",
    "            # Iterate over each function call requested by the assistant\n",
    "            for call in tool_calls:\n",
    "                # Check if the call is a function and if the function exists is our custom function\n",
    "                if call.type == \"function\" and call.function.name == \"fetch_stock_price\":\n",
    "                    func = available_functions[call.function.name]  # Retrieve the function reference\n",
    "                    \n",
    "                    # Parse the function arguments from JSON and execute the function\n",
    "                    tool_response = func(**json.loads(call.function.arguments))\n",
    "                    \n",
    "                    # Store the tool call ID and output to later send back to the assistant\n",
    "                    tool_responses.append({\"tool_call_id\": call.id, \"output\": tool_response})\n",
    "                    print(f\"Executed '{call.function.name}'. Output: {tool_response}\")\n",
    "                \n",
    "                else:\n",
    "                    # Raise an error if the function is not in available_functions to handle unexpected requests\n",
    "                    raise ValueError(f\"Requested function '{call.function.name}' is not available.\")\n",
    "            \n",
    "            # Submit all collected tool outputs back to the assistant to satisfy the required action\n",
    "            run = openai_client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread_id, \n",
    "                run_id=run_id, \n",
    "                tool_outputs=tool_responses\n",
    "            )\n",
    "            print(\"Function call(s) completed successfully.\")\n",
    "        \n",
    "        else:\n",
    "            # If the assistant is still processing, wait before checking the status again\n",
    "            print(\"Waiting for the Assistant to process...\")\n",
    "            time.sleep(5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is the current stock price for Amazon?\"\n",
    "\n",
    "response = get_assistant_response(thread.id, run.id, user_prompt)\n",
    "#for message in response:\n",
    "#    print(f\"{message['role'].capitalize()}: {message['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Examples using Function\n",
    "- one basic with image (uses CI)\n",
    "- one basic function calling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleanup Resources**\n",
    "To avoid creating redundant resources and ensure a clean environment, this cell deletes the assistant, thread, and any other created resources. Run this cell at the end of your session to clean up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = openai_client.beta.assistants.delete(assistant.id)\n",
    "#print(response)\n",
    "\n",
    "# Optionally delete any other temporary files or datas\n",
    "# Note: Any uploaded files to OpenAI could also be cleaned up if needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
